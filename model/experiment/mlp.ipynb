{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'detailed'\n",
    "chosen_feature = ['log_range_value', 'log_variance', 'loglog_sum_diff', 'loglog_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500000], Loss: 0.70972949\n",
      "Epoch [1000/500000], Loss: 0.18624765\n",
      "Epoch [2000/500000], Loss: 0.17963301\n",
      "Epoch [3000/500000], Loss: 0.17027456\n",
      "Epoch [4000/500000], Loss: 0.16597910\n",
      "Epoch [5000/500000], Loss: 0.16509597\n",
      "Epoch [6000/500000], Loss: 0.16464680\n",
      "Epoch [7000/500000], Loss: 0.16442712\n",
      "Epoch [8000/500000], Loss: 0.16423614\n",
      "Epoch [9000/500000], Loss: 0.16394523\n",
      "Epoch [10000/500000], Loss: 0.16369344\n",
      "Epoch [11000/500000], Loss: 0.16343717\n",
      "Epoch [12000/500000], Loss: 0.16317037\n",
      "Epoch [13000/500000], Loss: 0.16294731\n",
      "Epoch [14000/500000], Loss: 0.16281290\n",
      "Epoch [15000/500000], Loss: 0.16266014\n",
      "Epoch [16000/500000], Loss: 0.16253226\n",
      "Epoch [17000/500000], Loss: 0.16240522\n",
      "Epoch [18000/500000], Loss: 0.16227843\n",
      "Epoch [19000/500000], Loss: 0.16195327\n",
      "Epoch [20000/500000], Loss: 0.15958576\n",
      "Epoch [21000/500000], Loss: 0.15905392\n",
      "Epoch [22000/500000], Loss: 0.15875958\n",
      "Epoch [23000/500000], Loss: 0.15854289\n",
      "Epoch [24000/500000], Loss: 0.15834226\n",
      "Epoch [25000/500000], Loss: 0.15820177\n",
      "Epoch [26000/500000], Loss: 0.15807977\n",
      "Epoch [27000/500000], Loss: 0.15795767\n",
      "Epoch [28000/500000], Loss: 0.15785936\n",
      "Epoch [29000/500000], Loss: 0.15771715\n",
      "Epoch [30000/500000], Loss: 0.15759963\n",
      "Epoch [31000/500000], Loss: 0.15750235\n",
      "Epoch [32000/500000], Loss: 0.15742669\n",
      "Epoch [33000/500000], Loss: 0.15734681\n",
      "Epoch [34000/500000], Loss: 0.15728682\n",
      "Epoch [35000/500000], Loss: 0.15723380\n",
      "Epoch [36000/500000], Loss: 0.15717129\n",
      "Epoch [37000/500000], Loss: 0.15708989\n",
      "Epoch [38000/500000], Loss: 0.15704332\n",
      "Epoch [39000/500000], Loss: 0.15698174\n",
      "Epoch [40000/500000], Loss: 0.15682393\n",
      "Epoch [41000/500000], Loss: 0.15665568\n",
      "Epoch [42000/500000], Loss: 0.15648702\n",
      "Epoch [43000/500000], Loss: 0.15612556\n",
      "Epoch [44000/500000], Loss: 0.15595593\n",
      "Epoch [45000/500000], Loss: 0.15401328\n",
      "Epoch [46000/500000], Loss: 0.15320513\n",
      "Epoch [47000/500000], Loss: 0.15255491\n",
      "Epoch [48000/500000], Loss: 0.15224971\n",
      "Epoch [49000/500000], Loss: 0.15208088\n",
      "Epoch [50000/500000], Loss: 0.15179144\n",
      "Epoch [51000/500000], Loss: 0.14919169\n",
      "Epoch [52000/500000], Loss: 0.14846627\n",
      "Epoch [53000/500000], Loss: 0.14813094\n",
      "Epoch [54000/500000], Loss: 0.14792031\n",
      "Epoch [55000/500000], Loss: 0.14774971\n",
      "Epoch [56000/500000], Loss: 0.14760758\n",
      "Epoch [57000/500000], Loss: 0.14749962\n",
      "Epoch [58000/500000], Loss: 0.14726582\n",
      "Epoch [59000/500000], Loss: 0.14712061\n",
      "Epoch [60000/500000], Loss: 0.14697336\n",
      "Epoch [61000/500000], Loss: 0.14688200\n",
      "Epoch [62000/500000], Loss: 0.14680582\n",
      "Epoch [63000/500000], Loss: 0.14673737\n",
      "Epoch [64000/500000], Loss: 0.14667882\n",
      "Epoch [65000/500000], Loss: 0.14663833\n",
      "Epoch [66000/500000], Loss: 0.14659543\n",
      "Epoch [67000/500000], Loss: 0.14655007\n",
      "Epoch [68000/500000], Loss: 0.14651452\n",
      "Epoch [69000/500000], Loss: 0.14648499\n",
      "Epoch [70000/500000], Loss: 0.14646319\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "features_df = pd.read_csv(f'../../training_data/{dataset}/features.csv')[['sequenceID'] + chosen_feature]\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')\n",
    "\n",
    "\n",
    "# Create X_train, y_train, X_test\n",
    "X_train = features_df[chosen_feature].to_numpy()\n",
    "y_train = target_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = SimpleNN(input_dim)\n",
    "criterion = SquaredHingeLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# Training with early stopping\n",
    "best_train_loss = float('inf')\n",
    "patience = 100\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(100000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Early stopping check\n",
    "    train_loss = loss.item()\n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        best_model = model.state_dict()  # Save the best model state\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch [{epoch}/{500000}], Loss: {train_loss:.8f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
