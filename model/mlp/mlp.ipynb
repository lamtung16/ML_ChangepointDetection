{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'detailed'\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 100\n",
    "max_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for size in layer_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))  # Output layer\n",
    "\n",
    "        self.model = nn.Sequential(*layers)  # Combine layers into a sequential model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fold and target data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')\n",
    "features_df = pd.read_csv(f'../../training_data/{dataset}/features.csv')\n",
    "\n",
    "# Prepare CSV file for logging\n",
    "report_path = f'report_{dataset}.csv'\n",
    "report_header = ['dataset', 'num_layers', 'layer_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n",
    "if not os.path.exists(report_path):\n",
    "    pd.DataFrame(columns=report_header).to_csv(report_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "configurations = {\n",
    "    f'num_layers_{num_layers}_layer_size_{layer_size}_test_fold_{test_fold}': {\n",
    "        'num_layers': num_layers,\n",
    "        'layer_size': layer_size,\n",
    "        'test_fold': test_fold\n",
    "    }\n",
    "    for num_layers in range(1, 4)\n",
    "    for layer_size in [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "    for test_fold in range(1, 7)\n",
    "}\n",
    "\n",
    "config_list = list(configurations.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.48779464 \t Avg Val Loss: 0.50346583 \t Avg Test Loss: 0.46622780\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.47449434 \t Avg Val Loss: 0.35255274 \t Avg Test Loss: 0.54014379\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.47042167 \t Avg Val Loss: 0.57191396 \t Avg Test Loss: 0.43236721\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.44777322 \t Avg Val Loss: 0.46341816 \t Avg Test Loss: 0.42979530\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.47739053 \t Avg Val Loss: 0.43064213 \t Avg Test Loss: 0.49505320\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.68149590 \t Avg Val Loss: 0.51733661 \t Avg Test Loss: 0.61558694\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.46600819 \t Avg Val Loss: 0.48861468 \t Avg Test Loss: 0.46075675\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.46485791 \t Avg Val Loss: 0.35067022 \t Avg Test Loss: 0.52883333\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.43610173 \t Avg Val Loss: 0.52639842 \t Avg Test Loss: 0.38731730\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.45967197 \t Avg Val Loss: 0.44701448 \t Avg Test Loss: 0.42744645\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.73440230 \t Avg Val Loss: 0.75237966 \t Avg Test Loss: 0.74921399\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.47832230 \t Avg Val Loss: 0.40043578 \t Avg Test Loss: 0.42079085\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.67526770 \t Avg Val Loss: 0.67967129 \t Avg Test Loss: 0.61665928\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.67394942 \t Avg Val Loss: 0.49746880 \t Avg Test Loss: 0.74332237\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.62096429 \t Avg Val Loss: 0.71874893 \t Avg Test Loss: 0.59711039\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.61941153 \t Avg Val Loss: 0.55395633 \t Avg Test Loss: 0.56991673\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.49399054 \t Avg Val Loss: 0.46364325 \t Avg Test Loss: 0.51116115\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.49590197 \t Avg Val Loss: 0.40301186 \t Avg Test Loss: 0.43614995\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.55651814 \t Avg Val Loss: 0.56688613 \t Avg Test Loss: 0.52036119\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.49036697 \t Avg Val Loss: 0.35866642 \t Avg Test Loss: 0.55082440\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45219505 \t Avg Val Loss: 0.54246753 \t Avg Test Loss: 0.40943748\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.49998182 \t Avg Val Loss: 0.47015595 \t Avg Test Loss: 0.46032920\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.49408805 \t Avg Val Loss: 0.46442643 \t Avg Test Loss: 0.50648147\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.50486606 \t Avg Val Loss: 0.40646064 \t Avg Test Loss: 0.43923858\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.50363386 \t Avg Val Loss: 0.52098507 \t Avg Test Loss: 0.48537084\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.44034463 \t Avg Val Loss: 0.33065072 \t Avg Test Loss: 0.49356377\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.44979286 \t Avg Val Loss: 0.53619546 \t Avg Test Loss: 0.40788242\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.49089742 \t Avg Val Loss: 0.46140853 \t Avg Test Loss: 0.45375744\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.48765233 \t Avg Val Loss: 0.45284501 \t Avg Test Loss: 0.49926257\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.46334338 \t Avg Val Loss: 0.38519725 \t Avg Test Loss: 0.40203488\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.50732410 \t Avg Val Loss: 0.51035094 \t Avg Test Loss: 0.46676204\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.49652094 \t Avg Val Loss: 0.35591504 \t Avg Test Loss: 0.54706597\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.46742746 \t Avg Val Loss: 0.55643523 \t Avg Test Loss: 0.41785964\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.50936651 \t Avg Val Loss: 0.47112158 \t Avg Test Loss: 0.46127963\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.50247669 \t Avg Val Loss: 0.46452269 \t Avg Test Loss: 0.50604516\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51084411 \t Avg Val Loss: 0.39741346 \t Avg Test Loss: 0.43459317\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.53912222 \t Avg Val Loss: 0.52840936 \t Avg Test Loss: 0.49612805\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.48066247 \t Avg Val Loss: 0.33939901 \t Avg Test Loss: 0.52028352\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.44700617 \t Avg Val Loss: 0.52137017 \t Avg Test Loss: 0.39322892\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.53678471 \t Avg Val Loss: 0.48031303 \t Avg Test Loss: 0.47803947\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.53893942 \t Avg Val Loss: 0.49432933 \t Avg Test Loss: 0.52353907\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.53425324 \t Avg Val Loss: 0.40215239 \t Avg Test Loss: 0.44403628\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.57483113 \t Avg Val Loss: 0.54683131 \t Avg Test Loss: 0.50636804\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.49829793 \t Avg Val Loss: 0.34153625 \t Avg Test Loss: 0.52305245\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.52592784 \t Avg Val Loss: 0.58147222 \t Avg Test Loss: 0.45442513\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52081668 \t Avg Val Loss: 0.45999125 \t Avg Test Loss: 0.45325080\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.47374299 \t Avg Val Loss: 0.40972105 \t Avg Test Loss: 0.46640962\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51874542 \t Avg Val Loss: 0.39520812 \t Avg Test Loss: 0.42686564\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.50256050 \t Avg Val Loss: 0.47418335 \t Avg Test Loss: 0.45665160\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.55374187 \t Avg Val Loss: 0.35885730 \t Avg Test Loss: 0.55178773\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.47371289 \t Avg Val Loss: 0.52816921 \t Avg Test Loss: 0.39488965\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52925575 \t Avg Val Loss: 0.45674011 \t Avg Test Loss: 0.44034776\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.50379014 \t Avg Val Loss: 0.42060336 \t Avg Test Loss: 0.46642527\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.53661430 \t Avg Val Loss: 0.39171788 \t Avg Test Loss: 0.42079771\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.44912958 \t Avg Val Loss: 0.46146998 \t Avg Test Loss: 0.46666774\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.79682362 \t Avg Val Loss: 0.61279452 \t Avg Test Loss: 0.88309520\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.43973610 \t Avg Val Loss: 0.52994233 \t Avg Test Loss: 0.39120674\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.65899122 \t Avg Val Loss: 0.59482056 \t Avg Test Loss: 0.61824507\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.66445780 \t Avg Val Loss: 0.67937076 \t Avg Test Loss: 0.68952584\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.56104130 \t Avg Val Loss: 0.43932226 \t Avg Test Loss: 0.50118500\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.48571360 \t Avg Val Loss: 0.50962758 \t Avg Test Loss: 0.47713763\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.51408267 \t Avg Val Loss: 0.37663138 \t Avg Test Loss: 0.57995576\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.47499233 \t Avg Val Loss: 0.57849407 \t Avg Test Loss: 0.44480872\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.44605169 \t Avg Val Loss: 0.46172163 \t Avg Test Loss: 0.42553303\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.77787691 \t Avg Val Loss: 0.78692251 \t Avg Test Loss: 0.78981352\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.70145172 \t Avg Val Loss: 0.53509206 \t Avg Test Loss: 0.63583958\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.48266444 \t Avg Val Loss: 0.50577176 \t Avg Test Loss: 0.47208530\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.60711253 \t Avg Val Loss: 0.44949958 \t Avg Test Loss: 0.67931491\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45501700 \t Avg Val Loss: 0.55284262 \t Avg Test Loss: 0.41118309\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.50005490 \t Avg Val Loss: 0.47831947 \t Avg Test Loss: 0.47286412\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.62395489 \t Avg Val Loss: 0.63097501 \t Avg Test Loss: 0.64634764\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.49463597 \t Avg Val Loss: 0.40225813 \t Avg Test Loss: 0.43587437\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.46303326 \t Avg Val Loss: 0.48070258 \t Avg Test Loss: 0.45877865\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.50616384 \t Avg Val Loss: 0.36990377 \t Avg Test Loss: 0.57010341\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.46406752 \t Avg Val Loss: 0.56586921 \t Avg Test Loss: 0.42924711\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52598071 \t Avg Val Loss: 0.49258462 \t Avg Test Loss: 0.49459407\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.53762090 \t Avg Val Loss: 0.51919329 \t Avg Test Loss: 0.55235058\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.53439790 \t Avg Val Loss: 0.41915947 \t Avg Test Loss: 0.47017229\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.58805186 \t Avg Val Loss: 0.60614026 \t Avg Test Loss: 0.55089027\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.52772540 \t Avg Val Loss: 0.38367331 \t Avg Test Loss: 0.59008062\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.54968053 \t Avg Val Loss: 0.64536893 \t Avg Test Loss: 0.51417804\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.50918919 \t Avg Val Loss: 0.47922611 \t Avg Test Loss: 0.47595087\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.52156013 \t Avg Val Loss: 0.50143343 \t Avg Test Loss: 0.53397143\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51822656 \t Avg Val Loss: 0.41022867 \t Avg Test Loss: 0.45389330\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.50806224 \t Avg Val Loss: 0.52013856 \t Avg Test Loss: 0.48340541\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.48984662 \t Avg Val Loss: 0.35548368 \t Avg Test Loss: 0.54730654\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.47870618 \t Avg Val Loss: 0.57222217 \t Avg Test Loss: 0.44090256\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52747160 \t Avg Val Loss: 0.48579392 \t Avg Test Loss: 0.48595706\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.50552243 \t Avg Val Loss: 0.47610840 \t Avg Test Loss: 0.51347965\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.49535957 \t Avg Val Loss: 0.39973450 \t Avg Test Loss: 0.43328920\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.48077804 \t Avg Val Loss: 0.48857602 \t Avg Test Loss: 0.46136013\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.51816297 \t Avg Val Loss: 0.36375055 \t Avg Test Loss: 0.56133735\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.49537292 \t Avg Val Loss: 0.58207101 \t Avg Test Loss: 0.45016518\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.49899676 \t Avg Val Loss: 0.46443129 \t Avg Test Loss: 0.45580477\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.52057296 \t Avg Val Loss: 0.48429459 \t Avg Test Loss: 0.51814246\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51942396 \t Avg Val Loss: 0.40413237 \t Avg Test Loss: 0.44508955\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.50476640 \t Avg Val Loss: 0.48590210 \t Avg Test Loss: 0.45705023\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.52355945 \t Avg Val Loss: 0.35489127 \t Avg Test Loss: 0.54621994\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.51081926 \t Avg Val Loss: 0.57525718 \t Avg Test Loss: 0.44503927\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52200115 \t Avg Val Loss: 0.45799780 \t Avg Test Loss: 0.44987580\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.49546430 \t Avg Val Loss: 0.43190023 \t Avg Test Loss: 0.47759518\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.53836161 \t Avg Val Loss: 0.39881608 \t Avg Test Loss: 0.43775535\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.48699504 \t Avg Val Loss: 0.43900782 \t Avg Test Loss: 0.42677322\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.50540602 \t Avg Val Loss: 0.32926077 \t Avg Test Loss: 0.49430704\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.51752359 \t Avg Val Loss: 0.53518885 \t Avg Test Loss: 0.39929754\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52573609 \t Avg Val Loss: 0.43925673 \t Avg Test Loss: 0.41736144\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.48807123 \t Avg Val Loss: 0.38764322 \t Avg Test Loss: 0.45344609\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51858205 \t Avg Val Loss: 0.37795541 \t Avg Test Loss: 0.39024398\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.52707535 \t Avg Val Loss: 0.55141771 \t Avg Test Loss: 0.50555098\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.46570498 \t Avg Val Loss: 0.34892932 \t Avg Test Loss: 0.52992541\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.59569091 \t Avg Val Loss: 0.70632327 \t Avg Test Loss: 0.58156568\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.65828592 \t Avg Val Loss: 0.59376383 \t Avg Test Loss: 0.61692107\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 1.13620102 \t Avg Val Loss: 1.19175661 \t Avg Test Loss: 1.16842055\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.46958908 \t Avg Val Loss: 0.40751290 \t Avg Test Loss: 0.41284004\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.61043102 \t Avg Val Loss: 0.63131928 \t Avg Test Loss: 0.57404536\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.46866730 \t Avg Val Loss: 0.34810039 \t Avg Test Loss: 0.52989656\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45028675 \t Avg Val Loss: 0.55130976 \t Avg Test Loss: 0.41371739\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.45811573 \t Avg Val Loss: 0.46068454 \t Avg Test Loss: 0.43679073\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.65593338 \t Avg Val Loss: 0.66915542 \t Avg Test Loss: 0.68045253\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.47609738 \t Avg Val Loss: 0.40117043 \t Avg Test Loss: 0.41880128\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.44779623 \t Avg Val Loss: 0.46302286 \t Avg Test Loss: 0.46085948\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.59300274 \t Avg Val Loss: 0.43879503 \t Avg Test Loss: 0.66557616\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.49245369 \t Avg Val Loss: 0.59953594 \t Avg Test Loss: 0.46807238\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.46391648 \t Avg Val Loss: 0.46289161 \t Avg Test Loss: 0.44164574\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.57534522 \t Avg Val Loss: 0.57335579 \t Avg Test Loss: 0.59616101\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.48380211 \t Avg Val Loss: 0.40037671 \t Avg Test Loss: 0.42631453\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.47477892 \t Avg Val Loss: 0.49766982 \t Avg Test Loss: 0.46894789\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.58311331 \t Avg Val Loss: 0.42859548 \t Avg Test Loss: 0.65216780\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.60011977 \t Avg Val Loss: 0.70772111 \t Avg Test Loss: 0.58252162\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.49498489 \t Avg Val Loss: 0.47410020 \t Avg Test Loss: 0.46743327\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.49545214 \t Avg Val Loss: 0.47782472 \t Avg Test Loss: 0.51724321\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51061511 \t Avg Val Loss: 0.41026714 \t Avg Test Loss: 0.45110297\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.46352175 \t Avg Val Loss: 0.48425466 \t Avg Test Loss: 0.46230549\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.50739092 \t Avg Val Loss: 0.37169096 \t Avg Test Loss: 0.57301712\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.48541063 \t Avg Val Loss: 0.58804959 \t Avg Test Loss: 0.45590466\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.57367772 \t Avg Val Loss: 0.52543575 \t Avg Test Loss: 0.53550971\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.55088401 \t Avg Val Loss: 0.53737152 \t Avg Test Loss: 0.56453508\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.52806091 \t Avg Val Loss: 0.41811600 \t Avg Test Loss: 0.46646914\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.47273850 \t Avg Val Loss: 0.49213839 \t Avg Test Loss: 0.46468404\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.50308329 \t Avg Val Loss: 0.36479595 \t Avg Test Loss: 0.56276757\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.46788025 \t Avg Val Loss: 0.56557101 \t Avg Test Loss: 0.43090016\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.52972150 \t Avg Val Loss: 0.49070552 \t Avg Test Loss: 0.49225515\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.48410022 \t Avg Val Loss: 0.45722103 \t Avg Test Loss: 0.50148761\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.50083780 \t Avg Val Loss: 0.40454054 \t Avg Test Loss: 0.43940797\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.47971064 \t Avg Val Loss: 0.49430406 \t Avg Test Loss: 0.46607202\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.51588768 \t Avg Val Loss: 0.36989734 \t Avg Test Loss: 0.56985211\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.48064360 \t Avg Val Loss: 0.57327408 \t Avg Test Loss: 0.44057152\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.49554920 \t Avg Val Loss: 0.46712226 \t Avg Test Loss: 0.45633185\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.50388891 \t Avg Val Loss: 0.47692809 \t Avg Test Loss: 0.51552367\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.53189689 \t Avg Val Loss: 0.41267720 \t Avg Test Loss: 0.45710486\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.52019125 \t Avg Val Loss: 0.51663303 \t Avg Test Loss: 0.47983724\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.52987742 \t Avg Val Loss: 0.36701185 \t Avg Test Loss: 0.56525677\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.49035165 \t Avg Val Loss: 0.56098378 \t Avg Test Loss: 0.42970353\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.51372516 \t Avg Val Loss: 0.46438447 \t Avg Test Loss: 0.45623422\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.50635344 \t Avg Val Loss: 0.46406356 \t Avg Test Loss: 0.50517422\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.54169422 \t Avg Val Loss: 0.41305301 \t Avg Test Loss: 0.45845670\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.49963585 \t Avg Val Loss: 0.46469572 \t Avg Test Loss: 0.44420075\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.52308649 \t Avg Val Loss: 0.34577286 \t Avg Test Loss: 0.52955711\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.49582738 \t Avg Val Loss: 0.54094100 \t Avg Test Loss: 0.40812275\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.51592857 \t Avg Val Loss: 0.44933367 \t Avg Test Loss: 0.43152416\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.49105534 \t Avg Val Loss: 0.41467333 \t Avg Test Loss: 0.47063965\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.52402955 \t Avg Val Loss: 0.38935634 \t Avg Test Loss: 0.41356286\n"
     ]
    }
   ],
   "source": [
    "# Make sure directories exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs('predictions_all', exist_ok=True)\n",
    "\n",
    "# Iterate over each configuration in the config_list\n",
    "for config in config_list:\n",
    "    num_layers, layer_size, test_fold = config['num_layers'], config['layer_size'], config['test_fold']\n",
    "\n",
    "    # Record start time\n",
    "    fold_start_time = time.time()\n",
    "\n",
    "    # Split data into training and test sets based on the fold\n",
    "    train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "    test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "    # Prepare train and test sequences as arrays\n",
    "    features_train = features_df[features_df['sequenceID'].isin(train_ids)].iloc[:, 1:].to_numpy()\n",
    "    target_train = target_df[target_df['sequenceID'].isin(train_ids)].iloc[:, 1:].to_numpy()\n",
    "    features_test = features_df[features_df['sequenceID'].isin(test_ids)].iloc[:, 1:].to_numpy()\n",
    "\n",
    "    # Normalize training features\n",
    "    scaler = MinMaxScaler()  # Create scaler instance\n",
    "    features_train = scaler.fit_transform(features_train)  # Fit on training data\n",
    "    features_test = scaler.transform(features_test)  # Transform test data using the same parameters\n",
    "\n",
    "    # Convert target data to tensors\n",
    "    target_test = torch.tensor(target_df[target_df['sequenceID'].isin(test_ids)].iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    # Split training data into subtrain and validation sets\n",
    "    X_subtrain, X_val, y_subtrain, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "    y_subtrain = torch.tensor(y_subtrain, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the MLP model, loss function, and optimizer\n",
    "    layer_sizes = [layer_size] * num_layers\n",
    "    model = MLPModel(X_subtrain.shape[1], layer_sizes).to(device)\n",
    "    criterion = SquaredHingeLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Variables for early stopping\n",
    "    best_val_loss, patience_counter = float('inf'), 0\n",
    "    best_model_state, stop_epoch = None, 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(torch.tensor(X_subtrain, dtype=torch.float32))  # No need to convert to tensor again\n",
    "        loss = criterion(predictions, y_subtrain)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(torch.tensor(X_val, dtype=torch.float32)), y_val)\n",
    "            avg_test_loss = criterion(model(torch.tensor(features_test, dtype=torch.float32)), target_test)\n",
    "\n",
    "        avg_train_loss = loss.item()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {val_loss.item():.8f} \\t Avg Test Loss: {avg_test_loss.item():.8f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, best_model_state = val_loss.item(), model.state_dict()\n",
    "            patience_counter = 0\n",
    "            stop_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at Epoch [{epoch}]\")\n",
    "            break\n",
    "\n",
    "    # Restore best model state for final evaluation\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save model parameters\n",
    "    torch.save(model.state_dict(), f'saved_models/{dataset}_{num_layers}layers_{layer_size}neurons_fold{test_fold}.pth')\n",
    "\n",
    "    # Record end time and calculate elapsed time\n",
    "    elapsed_time = time.time() - fold_start_time\n",
    "\n",
    "    # Log the results\n",
    "    report_entry = {\n",
    "        'dataset': dataset,\n",
    "        'num_layers': num_layers,\n",
    "        'layer_size': layer_size,\n",
    "        'test_fold': test_fold,\n",
    "        'stop_epoch': stop_epoch,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': best_val_loss,\n",
    "        'test_loss': avg_test_loss.item(),\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "    pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)\n",
    "\n",
    "    # Predict on the test set and save to CSV\n",
    "    model.eval()\n",
    "    pred_lldas = model(torch.tensor(features_test, dtype=torch.float32)).detach().numpy().ravel()  # No need to convert to tensor again\n",
    "    lldas_df = pd.DataFrame({'sequenceID': features_df[features_df['sequenceID'].isin(test_ids)]['sequenceID'], 'llda': pred_lldas})\n",
    "    lldas_df.to_csv(f'predictions_all/{dataset}.{num_layers}layers_{layer_size}neurons_{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
