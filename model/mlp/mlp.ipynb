{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../../training_data'\n",
    "datasets = [name for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]\n",
    "datasets.remove('detailed')\n",
    "datasets.remove('systematic')\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 100\n",
    "max_epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for size in layer_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))  # Output layer\n",
    "\n",
    "        self.model = nn.Sequential(*layers)  # Combine layers into a sequential model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "configurations = {\n",
    "    f'num_layers_{num_layers}_layer_size_{layer_size}': {\n",
    "        'num_layers': num_layers,\n",
    "        'layer_size': layer_size\n",
    "    }\n",
    "    for num_layers in range(1, 5)\n",
    "    for layer_size in [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "}\n",
    "\n",
    "config_list = list(configurations.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.23325348 \t Avg Val Loss: 89.51642609 \t Avg Test Loss: 93.64671326\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 4.07841063 \t Avg Val Loss: 2.98305225 \t Avg Test Loss: 5.16313410\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 3.11538649 \t Avg Val Loss: 2.35053039 \t Avg Test Loss: 4.07681131\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 2.53376794 \t Avg Val Loss: 1.86213756 \t Avg Test Loss: 3.37207961\n",
      "Test fold 1 \t Epoch [4000] \t Avg Train Loss: 2.21986485 \t Avg Val Loss: 1.58163416 \t Avg Test Loss: 2.89411688\n",
      "Early stopping at Epoch [4525]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 88.90388489 \t Avg Val Loss: 89.05086517 \t Avg Test Loss: 89.24597931\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 71.62762451 \t Avg Val Loss: 71.68116760 \t Avg Test Loss: 71.89104462\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 56.85747528 \t Avg Val Loss: 56.82114029 \t Avg Test Loss: 57.04516602\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 44.24100494 \t Avg Val Loss: 44.11754990 \t Avg Test Loss: 44.35525894\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 33.54888916 \t Avg Val Loss: 33.34063721 \t Avg Test Loss: 33.59165192\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 24.63194656 \t Avg Val Loss: 24.34111023 \t Avg Test Loss: 24.60506630\n",
      "Test fold 2 \t Epoch [6000] \t Avg Train Loss: 17.38279343 \t Avg Val Loss: 17.01180840 \t Avg Test Loss: 17.28831673\n",
      "Test fold 2 \t Epoch [7000] \t Avg Train Loss: 11.74761677 \t Avg Val Loss: 11.27601910 \t Avg Test Loss: 11.57764816\n",
      "Test fold 2 \t Epoch [8000] \t Avg Train Loss: 7.62963009 \t Avg Val Loss: 7.07595444 \t Avg Test Loss: 7.39918756\n",
      "Test fold 2 \t Epoch [9000] \t Avg Train Loss: 4.91895866 \t Avg Val Loss: 4.27428532 \t Avg Test Loss: 4.61869001\n",
      "Test fold 2 \t Epoch [10000] \t Avg Train Loss: 3.40858197 \t Avg Val Loss: 2.66666412 \t Avg Test Loss: 3.03151321\n",
      "Test fold 2 \t Epoch [11000] \t Avg Train Loss: 2.70831084 \t Avg Val Loss: 1.94404602 \t Avg Test Loss: 2.28466201\n",
      "Test fold 2 \t Epoch [12000] \t Avg Train Loss: 2.49485564 \t Avg Val Loss: 1.71346116 \t Avg Test Loss: 2.03238893\n",
      "Test fold 2 \t Epoch [13000] \t Avg Train Loss: 2.46911240 \t Avg Val Loss: 1.68180311 \t Avg Test Loss: 1.98662555\n",
      "Test fold 2 \t Epoch [14000] \t Avg Train Loss: 2.46847367 \t Avg Val Loss: 1.68019283 \t Avg Test Loss: 1.98357058\n",
      "Early stopping at Epoch [14623]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 96.12278748 \t Avg Val Loss: 90.09941101 \t Avg Test Loss: 94.39450836\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.54664397 \t Avg Val Loss: 2.63948703 \t Avg Test Loss: 4.57236290\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.59397769 \t Avg Val Loss: 1.88744164 \t Avg Test Loss: 3.44457793\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 2.18911076 \t Avg Val Loss: 1.56132352 \t Avg Test Loss: 2.83133054\n",
      "Early stopping at Epoch [3245]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 88.29952240 \t Avg Val Loss: 87.83615875 \t Avg Test Loss: 88.16064453\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 4.61104965 \t Avg Val Loss: 4.24402952 \t Avg Test Loss: 3.50319028\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 3.61651492 \t Avg Val Loss: 3.04373169 \t Avg Test Loss: 2.93406367\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.98272848 \t Avg Val Loss: 2.24933624 \t Avg Test Loss: 2.65951991\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 2.56395912 \t Avg Val Loss: 1.75714755 \t Avg Test Loss: 2.45883632\n",
      "Early stopping at Epoch [4587]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.16045380 \t Avg Val Loss: 86.99334717 \t Avg Test Loss: 91.15151978\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.47721267 \t Avg Val Loss: 2.58262634 \t Avg Test Loss: 4.45197535\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.60422897 \t Avg Val Loss: 1.91957510 \t Avg Test Loss: 3.44533348\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 2.19545507 \t Avg Val Loss: 1.56377590 \t Avg Test Loss: 2.83896518\n",
      "Early stopping at Epoch [3231]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 101.91261292 \t Avg Val Loss: 101.55129242 \t Avg Test Loss: 101.83052826\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 4.22734737 \t Avg Val Loss: 3.77193189 \t Avg Test Loss: 3.29257870\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 3.04337358 \t Avg Val Loss: 2.24967933 \t Avg Test Loss: 2.75773501\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.52300787 \t Avg Val Loss: 1.75013423 \t Avg Test Loss: 2.52640963\n",
      "Early stopping at Epoch [3289]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 91.62282562 \t Avg Val Loss: 85.59052277 \t Avg Test Loss: 89.71334076\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.22037053 \t Avg Val Loss: 2.43287301 \t Avg Test Loss: 4.16984892\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.38723755 \t Avg Val Loss: 1.73233485 \t Avg Test Loss: 3.15250349\n",
      "Early stopping at Epoch [2842]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.74276733 \t Avg Val Loss: 93.49760437 \t Avg Test Loss: 93.63652802\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.72504187 \t Avg Val Loss: 3.12558270 \t Avg Test Loss: 3.02724242\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.66270304 \t Avg Val Loss: 1.80766296 \t Avg Test Loss: 2.57267308\n",
      "Early stopping at Epoch [2633]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 97.07194519 \t Avg Val Loss: 90.60054779 \t Avg Test Loss: 94.74897766\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.59223580 \t Avg Val Loss: 1.90345514 \t Avg Test Loss: 3.43440914\n",
      "Early stopping at Epoch [1829]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 95.90900421 \t Avg Val Loss: 95.51638031 \t Avg Test Loss: 95.62980652\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.37265754 \t Avg Val Loss: 2.67204094 \t Avg Test Loss: 2.91052890\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.50158143 \t Avg Val Loss: 1.74706411 \t Avg Test Loss: 2.53594351\n",
      "Early stopping at Epoch [2253]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 97.16093445 \t Avg Val Loss: 90.11796570 \t Avg Test Loss: 94.39438629\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.36472082 \t Avg Val Loss: 1.70276642 \t Avg Test Loss: 3.11649871\n",
      "Early stopping at Epoch [1477]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 97.81127167 \t Avg Val Loss: 96.69474030 \t Avg Test Loss: 96.91836548\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 2.24588609 \t Avg Val Loss: 1.61258721 \t Avg Test Loss: 2.22633719\n",
      "Early stopping at Epoch [1010]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 92.50946808 \t Avg Val Loss: 84.98204041 \t Avg Test Loss: 89.03709412\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 1.71002913 \t Avg Val Loss: 1.43659937 \t Avg Test Loss: 2.26295948\n",
      "Early stopping at Epoch [1171]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.33135223 \t Avg Val Loss: 90.74618530 \t Avg Test Loss: 90.72675323\n",
      "Early stopping at Epoch [746]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.22686005 \t Avg Val Loss: 84.40235901 \t Avg Test Loss: 88.75193787\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 1.55972409 \t Avg Val Loss: 1.43305361 \t Avg Test Loss: 2.24270892\n",
      "Early stopping at Epoch [1380]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.21914673 \t Avg Val Loss: 91.11227417 \t Avg Test Loss: 91.10141754\n",
      "Early stopping at Epoch [928]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 91.51873016 \t Avg Val Loss: 81.79399109 \t Avg Test Loss: 86.08029175\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 1.34126031 \t Avg Val Loss: 1.25387621 \t Avg Test Loss: 2.15188861\n",
      "Early stopping at Epoch [1153]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.15774536 \t Avg Val Loss: 88.27598572 \t Avg Test Loss: 88.30636597\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 1.57668316 \t Avg Val Loss: 1.33417249 \t Avg Test Loss: 2.04791164\n",
      "Early stopping at Epoch [1474]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.12270355 \t Avg Val Loss: 81.57711792 \t Avg Test Loss: 85.87528992\n",
      "Early stopping at Epoch [727]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 91.54728699 \t Avg Val Loss: 86.22589111 \t Avg Test Loss: 85.89911652\n",
      "Early stopping at Epoch [970]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 92.43900299 \t Avg Val Loss: 77.27327728 \t Avg Test Loss: 81.44834137\n",
      "Early stopping at Epoch [541]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.26696777 \t Avg Val Loss: 84.57395172 \t Avg Test Loss: 84.39826202\n",
      "Early stopping at Epoch [643]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 97.11225128 \t Avg Val Loss: 75.80266571 \t Avg Test Loss: 80.05692291\n",
      "Early stopping at Epoch [401]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.94741821 \t Avg Val Loss: 79.58038330 \t Avg Test Loss: 79.21223450\n",
      "Early stopping at Epoch [508]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.52732849 \t Avg Val Loss: 87.78286743 \t Avg Test Loss: 91.94496918\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.58143258 \t Avg Val Loss: 2.65630364 \t Avg Test Loss: 4.56672096\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.66338205 \t Avg Val Loss: 2.01282048 \t Avg Test Loss: 3.52377009\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 2.21013093 \t Avg Val Loss: 1.58149195 \t Avg Test Loss: 2.86833310\n",
      "Early stopping at Epoch [3455]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 78.72309875 \t Avg Val Loss: 78.66136169 \t Avg Test Loss: 78.77052307\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.74128175 \t Avg Val Loss: 3.11192751 \t Avg Test Loss: 2.87877893\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.87884808 \t Avg Val Loss: 2.17434549 \t Avg Test Loss: 2.53703332\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.41276407 \t Avg Val Loss: 1.68341136 \t Avg Test Loss: 2.36339235\n",
      "Early stopping at Epoch [3612]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.09905243 \t Avg Val Loss: 89.06024170 \t Avg Test Loss: 93.32612610\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.49480391 \t Avg Val Loss: 2.56326771 \t Avg Test Loss: 4.44921923\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.60660172 \t Avg Val Loss: 1.92725241 \t Avg Test Loss: 3.44520426\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 2.18291521 \t Avg Val Loss: 1.55488098 \t Avg Test Loss: 2.83768964\n",
      "Early stopping at Epoch [3454]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 85.88273621 \t Avg Val Loss: 85.95424652 \t Avg Test Loss: 86.14683533\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 68.27011871 \t Avg Val Loss: 68.30408478 \t Avg Test Loss: 68.51732635\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 14.09519482 \t Avg Val Loss: 13.64254665 \t Avg Test Loss: 13.92994499\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.53777528 \t Avg Val Loss: 1.76363158 \t Avg Test Loss: 2.08893251\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 2.46847367 \t Avg Val Loss: 1.68019009 \t Avg Test Loss: 1.98356378\n",
      "Early stopping at Epoch [4443]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 98.06927490 \t Avg Val Loss: 92.23435211 \t Avg Test Loss: 96.44860840\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.15248060 \t Avg Val Loss: 2.39524508 \t Avg Test Loss: 4.10731983\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.28294778 \t Avg Val Loss: 1.62322021 \t Avg Test Loss: 2.98699594\n",
      "Early stopping at Epoch [2557]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 89.85430145 \t Avg Val Loss: 89.66564941 \t Avg Test Loss: 89.87395477\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.64690566 \t Avg Val Loss: 3.01035428 \t Avg Test Loss: 2.93400359\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.57647562 \t Avg Val Loss: 1.74478972 \t Avg Test Loss: 2.52296662\n",
      "Early stopping at Epoch [2419]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.32395935 \t Avg Val Loss: 88.46421814 \t Avg Test Loss: 92.56121826\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.62125778 \t Avg Val Loss: 1.96481550 \t Avg Test Loss: 3.46393895\n",
      "Early stopping at Epoch [1499]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.52862549 \t Avg Val Loss: 93.50309753 \t Avg Test Loss: 93.67115784\n",
      "Early stopping at Epoch [711]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 92.74164581 \t Avg Val Loss: 86.65154266 \t Avg Test Loss: 90.72537994\n",
      "Early stopping at Epoch [904]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.99279022 \t Avg Val Loss: 93.69147491 \t Avg Test Loss: 93.87221527\n",
      "Early stopping at Epoch [527]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.80313873 \t Avg Val Loss: 89.62039948 \t Avg Test Loss: 93.78588104\n",
      "Early stopping at Epoch [717]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.96976471 \t Avg Val Loss: 92.52402496 \t Avg Test Loss: 92.70800781\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 1.50644422 \t Avg Val Loss: 1.41230977 \t Avg Test Loss: 2.03323984\n",
      "Early stopping at Epoch [1491]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.10173798 \t Avg Val Loss: 86.29821777 \t Avg Test Loss: 90.44933319\n",
      "Early stopping at Epoch [551]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 94.86459351 \t Avg Val Loss: 93.47777557 \t Avg Test Loss: 93.65808868\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 1.47797430 \t Avg Val Loss: 1.31209445 \t Avg Test Loss: 1.86872208\n",
      "Early stopping at Epoch [1195]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.41215515 \t Avg Val Loss: 84.77950287 \t Avg Test Loss: 88.89477539\n",
      "Early stopping at Epoch [272]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.70380402 \t Avg Val Loss: 90.25574493 \t Avg Test Loss: 90.40483093\n",
      "Early stopping at Epoch [305]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.04242706 \t Avg Val Loss: 81.48201752 \t Avg Test Loss: 85.67205048\n",
      "Early stopping at Epoch [291]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.55610657 \t Avg Val Loss: 86.07009125 \t Avg Test Loss: 86.06864166\n",
      "Early stopping at Epoch [381]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.73090363 \t Avg Val Loss: 73.84819794 \t Avg Test Loss: 77.93785095\n",
      "Early stopping at Epoch [288]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.18009186 \t Avg Val Loss: 78.76416779 \t Avg Test Loss: 78.44795227\n",
      "Early stopping at Epoch [324]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 92.92905426 \t Avg Val Loss: 48.86162186 \t Avg Test Loss: 52.68967438\n",
      "Early stopping at Epoch [230]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.14977264 \t Avg Val Loss: 54.06630325 \t Avg Test Loss: 52.93043137\n",
      "Early stopping at Epoch [236]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 93.92982483 \t Avg Val Loss: 12.23700333 \t Avg Test Loss: 15.47876930\n",
      "Early stopping at Epoch [188]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.40342712 \t Avg Val Loss: 16.62612534 \t Avg Test Loss: 14.81578636\n",
      "Early stopping at Epoch [206]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 81.47534180 \t Avg Val Loss: 76.15148926 \t Avg Test Loss: 80.05912018\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.12906337 \t Avg Val Loss: 1.40123808 \t Avg Test Loss: 2.31009293\n",
      "Early stopping at Epoch [1053]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 94.14749146 \t Avg Val Loss: 94.12447357 \t Avg Test Loss: 94.29531860\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 73.29413605 \t Avg Val Loss: 73.35636139 \t Avg Test Loss: 73.56473541\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 58.32851410 \t Avg Val Loss: 58.30168915 \t Avg Test Loss: 58.52421951\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 45.51913834 \t Avg Val Loss: 45.40508652 \t Avg Test Loss: 45.64132309\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 34.63779068 \t Avg Val Loss: 34.43878937 \t Avg Test Loss: 34.68834686\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 25.53652954 \t Avg Val Loss: 25.25473976 \t Avg Test Loss: 25.51727676\n",
      "Test fold 2 \t Epoch [6000] \t Avg Train Loss: 18.10955429 \t Avg Val Loss: 17.74735451 \t Avg Test Loss: 18.02248764\n",
      "Test fold 2 \t Epoch [7000] \t Avg Train Loss: 12.29873848 \t Avg Val Loss: 11.83970356 \t Avg Test Loss: 12.13734341\n",
      "Test fold 2 \t Epoch [8000] \t Avg Train Loss: 8.01757908 \t Avg Val Loss: 7.47361231 \t Avg Test Loss: 7.79375744\n",
      "Test fold 2 \t Epoch [9000] \t Avg Train Loss: 5.15570593 \t Avg Val Loss: 4.51995134 \t Avg Test Loss: 4.86323738\n",
      "Test fold 2 \t Epoch [10000] \t Avg Train Loss: 3.53244376 \t Avg Val Loss: 2.79398990 \t Avg Test Loss: 3.16155553\n",
      "Test fold 2 \t Epoch [11000] \t Avg Train Loss: 2.75593495 \t Avg Val Loss: 1.99474239 \t Avg Test Loss: 2.33702564\n",
      "Test fold 2 \t Epoch [12000] \t Avg Train Loss: 2.50364423 \t Avg Val Loss: 1.72281253 \t Avg Test Loss: 2.04513955\n",
      "Test fold 2 \t Epoch [13000] \t Avg Train Loss: 2.46964502 \t Avg Val Loss: 1.68299103 \t Avg Test Loss: 1.98832405\n",
      "Test fold 2 \t Epoch [14000] \t Avg Train Loss: 2.46847391 \t Avg Val Loss: 1.68020809 \t Avg Test Loss: 1.98360813\n",
      "Early stopping at Epoch [14734]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 97.47277069 \t Avg Val Loss: 91.69514465 \t Avg Test Loss: 95.90940094\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.53572512 \t Avg Val Loss: 1.80202162 \t Avg Test Loss: 3.25853920\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.00366592 \t Avg Val Loss: 1.38004112 \t Avg Test Loss: 2.48612261\n",
      "Early stopping at Epoch [2039]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 87.66467285 \t Avg Val Loss: 87.61564636 \t Avg Test Loss: 87.84230042\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.79742050 \t Avg Val Loss: 3.11999249 \t Avg Test Loss: 2.90368819\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.75241232 \t Avg Val Loss: 1.97497964 \t Avg Test Loss: 2.49744391\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.29914904 \t Avg Val Loss: 1.62047863 \t Avg Test Loss: 2.29297113\n",
      "Early stopping at Epoch [3138]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.90589905 \t Avg Val Loss: 89.18182373 \t Avg Test Loss: 93.34597015\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.12416840 \t Avg Val Loss: 1.49270260 \t Avg Test Loss: 2.65436578\n",
      "Early stopping at Epoch [1728]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 98.21962738 \t Avg Val Loss: 98.34999084 \t Avg Test Loss: 98.53394318\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.42690468 \t Avg Val Loss: 2.73814774 \t Avg Test Loss: 2.80139542\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.44387579 \t Avg Val Loss: 1.69013929 \t Avg Test Loss: 2.42861295\n",
      "Early stopping at Epoch [2331]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 90.89762115 \t Avg Val Loss: 85.17198944 \t Avg Test Loss: 89.26992798\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.27637315 \t Avg Val Loss: 1.64499235 \t Avg Test Loss: 2.99109769\n",
      "Early stopping at Epoch [1563]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 97.94051361 \t Avg Val Loss: 97.97989655 \t Avg Test Loss: 98.18109894\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 2.40445375 \t Avg Val Loss: 1.66029584 \t Avg Test Loss: 2.33955479\n",
      "Early stopping at Epoch [1207]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 97.43090057 \t Avg Val Loss: 91.52273560 \t Avg Test Loss: 95.71393585\n",
      "Early stopping at Epoch [886]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.67794800 \t Avg Val Loss: 93.45666504 \t Avg Test Loss: 93.66143799\n",
      "Early stopping at Epoch [429]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 97.19920349 \t Avg Val Loss: 91.02307129 \t Avg Test Loss: 95.23164368\n",
      "Early stopping at Epoch [637]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 91.28157043 \t Avg Val Loss: 91.09037018 \t Avg Test Loss: 91.28032684\n",
      "Early stopping at Epoch [396]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.33435822 \t Avg Val Loss: 87.90330505 \t Avg Test Loss: 92.07654572\n",
      "Early stopping at Epoch [388]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 91.98749542 \t Avg Val Loss: 91.16375732 \t Avg Test Loss: 91.31539917\n",
      "Early stopping at Epoch [525]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.16624451 \t Avg Val Loss: 86.41300964 \t Avg Test Loss: 90.56048584\n",
      "Early stopping at Epoch [436]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.85504150 \t Avg Val Loss: 91.31003571 \t Avg Test Loss: 91.44289398\n",
      "Early stopping at Epoch [427]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.85676575 \t Avg Val Loss: 84.95524597 \t Avg Test Loss: 89.12279510\n",
      "Early stopping at Epoch [281]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.48052979 \t Avg Val Loss: 89.07546234 \t Avg Test Loss: 89.14856720\n",
      "Early stopping at Epoch [316]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.64315796 \t Avg Val Loss: 75.26101685 \t Avg Test Loss: 79.37535858\n",
      "Early stopping at Epoch [257]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.85653687 \t Avg Val Loss: 82.94313812 \t Avg Test Loss: 82.74988556\n",
      "Early stopping at Epoch [231]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.28390503 \t Avg Val Loss: 47.25263596 \t Avg Test Loss: 51.10155106\n",
      "Early stopping at Epoch [212]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.77873993 \t Avg Val Loss: 54.98236847 \t Avg Test Loss: 53.89040756\n",
      "Early stopping at Epoch [235]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.87641144 \t Avg Val Loss: 5.55759287 \t Avg Test Loss: 7.82070971\n",
      "Early stopping at Epoch [164]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.68770599 \t Avg Val Loss: 5.43320322 \t Avg Test Loss: 4.56257248\n",
      "Early stopping at Epoch [172]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 79.79107666 \t Avg Val Loss: 74.49057770 \t Avg Test Loss: 78.36710358\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 14.14723396 \t Avg Val Loss: 11.64838123 \t Avg Test Loss: 13.65386868\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.13558507 \t Avg Val Loss: 1.39649153 \t Avg Test Loss: 2.31071877\n",
      "Early stopping at Epoch [2030]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 99.70601654 \t Avg Val Loss: 99.88658905 \t Avg Test Loss: 100.07366180\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 24.72126579 \t Avg Val Loss: 24.37205124 \t Avg Test Loss: 24.63595772\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.55601811 \t Avg Val Loss: 1.78559470 \t Avg Test Loss: 2.11184216\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.46847343 \t Avg Val Loss: 1.68017006 \t Avg Test Loss: 1.98351443\n",
      "Early stopping at Epoch [3391]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.16316223 \t Avg Val Loss: 88.46468353 \t Avg Test Loss: 92.61807251\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 3.01313305 \t Avg Val Loss: 2.24407220 \t Avg Test Loss: 3.87760830\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.29584861 \t Avg Val Loss: 1.71633780 \t Avg Test Loss: 2.99670982\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 2.00175571 \t Avg Val Loss: 1.48383629 \t Avg Test Loss: 2.59393597\n",
      "Early stopping at Epoch [3217]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 95.58362579 \t Avg Val Loss: 95.71546936 \t Avg Test Loss: 95.90811920\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 3.00209236 \t Avg Val Loss: 2.10403514 \t Avg Test Loss: 2.28568673\n",
      "Early stopping at Epoch [1836]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 99.02106476 \t Avg Val Loss: 93.16311646 \t Avg Test Loss: 97.40383911\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.35041404 \t Avg Val Loss: 1.75414515 \t Avg Test Loss: 3.03866959\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 1.95783031 \t Avg Val Loss: 1.44724727 \t Avg Test Loss: 2.53815436\n",
      "Early stopping at Epoch [2144]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 95.89028931 \t Avg Val Loss: 95.96244812 \t Avg Test Loss: 96.14609528\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 2.86024189 \t Avg Val Loss: 2.05912709 \t Avg Test Loss: 2.38595891\n",
      "Early stopping at Epoch [1824]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.88945007 \t Avg Val Loss: 90.09133911 \t Avg Test Loss: 94.26988220\n",
      "Early stopping at Epoch [837]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 91.61093903 \t Avg Val Loss: 91.65296173 \t Avg Test Loss: 91.83311462\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 2.43560266 \t Avg Val Loss: 1.67707610 \t Avg Test Loss: 2.41881561\n",
      "Early stopping at Epoch [1141]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.08101654 \t Avg Val Loss: 89.19465637 \t Avg Test Loss: 93.36315155\n",
      "Early stopping at Epoch [627]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 94.93316650 \t Avg Val Loss: 94.98551178 \t Avg Test Loss: 95.17469788\n",
      "Early stopping at Epoch [592]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.80927277 \t Avg Val Loss: 89.79503632 \t Avg Test Loss: 93.98445129\n",
      "Early stopping at Epoch [787]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 94.65727234 \t Avg Val Loss: 94.40336609 \t Avg Test Loss: 94.59579468\n",
      "Early stopping at Epoch [814]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 95.30440521 \t Avg Val Loss: 88.94807434 \t Avg Test Loss: 93.11976624\n",
      "Early stopping at Epoch [718]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 95.09592438 \t Avg Val Loss: 94.62493896 \t Avg Test Loss: 94.80220032\n",
      "Early stopping at Epoch [425]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.25073242 \t Avg Val Loss: 87.46157074 \t Avg Test Loss: 91.60054779\n",
      "Early stopping at Epoch [286]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.74289703 \t Avg Val Loss: 91.60077667 \t Avg Test Loss: 91.75523376\n",
      "Early stopping at Epoch [324]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.42749023 \t Avg Val Loss: 85.96044922 \t Avg Test Loss: 90.10643005\n",
      "Early stopping at Epoch [244]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.37895203 \t Avg Val Loss: 89.74133301 \t Avg Test Loss: 89.86753845\n",
      "Early stopping at Epoch [310]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.94043732 \t Avg Val Loss: 80.88919830 \t Avg Test Loss: 84.99637604\n",
      "Early stopping at Epoch [212]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.22889709 \t Avg Val Loss: 86.21317291 \t Avg Test Loss: 86.19947052\n",
      "Early stopping at Epoch [222]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.71806335 \t Avg Val Loss: 49.43079376 \t Avg Test Loss: 53.34619904\n",
      "Early stopping at Epoch [167]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 92.50791931 \t Avg Val Loss: 55.73542404 \t Avg Test Loss: 54.64674759\n",
      "Early stopping at Epoch [164]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 94.41886139 \t Avg Val Loss: 66.00866699 \t Avg Test Loss: 66.36073303\n",
      "Early stopping at Epoch [157]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 93.34401703 \t Avg Val Loss: 30.24999619 \t Avg Test Loss: 30.90123177\n",
      "Early stopping at Epoch [151]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 138.27896118 \t Avg Val Loss: 141.60469055 \t Avg Test Loss: 136.75927734\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 12.78824425 \t Avg Val Loss: 21.24807167 \t Avg Test Loss: 15.17164230\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 7.67949438 \t Avg Val Loss: 15.17125893 \t Avg Test Loss: 9.24258709\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 3.84256887 \t Avg Val Loss: 9.03666973 \t Avg Test Loss: 5.47693777\n",
      "Test fold 1 \t Epoch [4000] \t Avg Train Loss: 1.68262911 \t Avg Val Loss: 4.28213310 \t Avg Test Loss: 1.94775796\n",
      "Test fold 1 \t Epoch [5000] \t Avg Train Loss: 0.88178653 \t Avg Val Loss: 2.60916662 \t Avg Test Loss: 1.83253813\n",
      "Test fold 1 \t Epoch [6000] \t Avg Train Loss: 0.60631579 \t Avg Val Loss: 1.68729329 \t Avg Test Loss: 3.36242890\n",
      "Test fold 1 \t Epoch [7000] \t Avg Train Loss: 0.50008327 \t Avg Val Loss: 1.26704526 \t Avg Test Loss: 4.51833916\n",
      "Test fold 1 \t Epoch [8000] \t Avg Train Loss: 0.45115250 \t Avg Val Loss: 1.16065598 \t Avg Test Loss: 5.84605980\n",
      "Early stopping at Epoch [8856]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 132.51365662 \t Avg Val Loss: 125.39994812 \t Avg Test Loss: 133.12225342\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 110.99754333 \t Avg Val Loss: 104.54270172 \t Avg Test Loss: 111.59251404\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 92.06732178 \t Avg Val Loss: 86.20905304 \t Avg Test Loss: 92.64793396\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 75.34450531 \t Avg Val Loss: 70.06761932 \t Avg Test Loss: 75.91090393\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 60.61405563 \t Avg Val Loss: 55.90684891 \t Avg Test Loss: 61.16638184\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 47.74221420 \t Avg Val Loss: 43.59461594 \t Avg Test Loss: 48.28066254\n",
      "Test fold 2 \t Epoch [6000] \t Avg Train Loss: 36.64251709 \t Avg Val Loss: 33.04454422 \t Avg Test Loss: 37.16728210\n",
      "Test fold 2 \t Epoch [7000] \t Avg Train Loss: 27.25032806 \t Avg Val Loss: 24.19087982 \t Avg Test Loss: 27.76166534\n",
      "Test fold 2 \t Epoch [8000] \t Avg Train Loss: 19.50569153 \t Avg Val Loss: 16.97137642 \t Avg Test Loss: 20.00390625\n",
      "Test fold 2 \t Epoch [9000] \t Avg Train Loss: 13.34132767 \t Avg Val Loss: 11.31513119 \t Avg Test Loss: 13.82461166\n",
      "Test fold 2 \t Epoch [10000] \t Avg Train Loss: 8.67094231 \t Avg Val Loss: 7.10882998 \t Avg Test Loss: 9.12354755\n",
      "Test fold 2 \t Epoch [11000] \t Avg Train Loss: 5.38799477 \t Avg Val Loss: 4.21943045 \t Avg Test Loss: 5.78466034\n",
      "Test fold 2 \t Epoch [12000] \t Avg Train Loss: 3.24679613 \t Avg Val Loss: 2.45172572 \t Avg Test Loss: 3.63160348\n",
      "Test fold 2 \t Epoch [13000] \t Avg Train Loss: 2.03576469 \t Avg Val Loss: 1.50551856 \t Avg Test Loss: 2.41593981\n",
      "Test fold 2 \t Epoch [14000] \t Avg Train Loss: 1.50634587 \t Avg Val Loss: 1.08761883 \t Avg Test Loss: 1.85863101\n",
      "Test fold 2 \t Epoch [15000] \t Avg Train Loss: 1.31898105 \t Avg Val Loss: 0.93535966 \t Avg Test Loss: 1.66117728\n",
      "Test fold 2 \t Epoch [16000] \t Avg Train Loss: 1.27349448 \t Avg Val Loss: 0.90524060 \t Avg Test Loss: 1.61451232\n",
      "Early stopping at Epoch [16251]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 130.16921997 \t Avg Val Loss: 133.21601868 \t Avg Test Loss: 128.58665466\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 12.29594231 \t Avg Val Loss: 20.34976006 \t Avg Test Loss: 14.55683899\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 7.64928198 \t Avg Val Loss: 14.92547321 \t Avg Test Loss: 9.09389591\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 3.92523074 \t Avg Val Loss: 9.27147675 \t Avg Test Loss: 5.40868807\n",
      "Test fold 1 \t Epoch [4000] \t Avg Train Loss: 1.73906517 \t Avg Val Loss: 4.47553396 \t Avg Test Loss: 2.05118990\n",
      "Test fold 1 \t Epoch [5000] \t Avg Train Loss: 0.88835692 \t Avg Val Loss: 2.63252044 \t Avg Test Loss: 1.72595179\n",
      "Test fold 1 \t Epoch [6000] \t Avg Train Loss: 0.61033922 \t Avg Val Loss: 1.71200264 \t Avg Test Loss: 3.16698503\n",
      "Test fold 1 \t Epoch [7000] \t Avg Train Loss: 0.50498295 \t Avg Val Loss: 1.28690267 \t Avg Test Loss: 4.31640577\n",
      "Test fold 1 \t Epoch [8000] \t Avg Train Loss: 0.45483258 \t Avg Val Loss: 1.17121565 \t Avg Test Loss: 5.59713411\n",
      "Test fold 1 \t Epoch [9000] \t Avg Train Loss: 0.41876572 \t Avg Val Loss: 1.10425282 \t Avg Test Loss: 6.80731869\n",
      "Early stopping at Epoch [9042]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 119.93212128 \t Avg Val Loss: 112.85602570 \t Avg Test Loss: 120.14431000\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 8.68329620 \t Avg Val Loss: 7.75490332 \t Avg Test Loss: 8.72268391\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 2.71636558 \t Avg Val Loss: 3.64192414 \t Avg Test Loss: 3.65456128\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 0.72781938 \t Avg Val Loss: 1.58810294 \t Avg Test Loss: 1.72583783\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 0.51256979 \t Avg Val Loss: 1.00762653 \t Avg Test Loss: 1.25486755\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 0.44972742 \t Avg Val Loss: 0.85931587 \t Avg Test Loss: 1.09606302\n",
      "Test fold 2 \t Epoch [6000] \t Avg Train Loss: 0.41406700 \t Avg Val Loss: 0.80425638 \t Avg Test Loss: 1.00021660\n",
      "Early stopping at Epoch [6859]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 129.86752319 \t Avg Val Loss: 132.42028809 \t Avg Test Loss: 127.79487610\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 7.68613768 \t Avg Val Loss: 15.31140804 \t Avg Test Loss: 9.38497353\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 2.09989882 \t Avg Val Loss: 5.20626068 \t Avg Test Loss: 2.46909595\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 0.66740632 \t Avg Val Loss: 1.92791867 \t Avg Test Loss: 1.16238439\n",
      "Test fold 1 \t Epoch [4000] \t Avg Train Loss: 0.40485039 \t Avg Val Loss: 1.07213962 \t Avg Test Loss: 2.16582489\n",
      "Early stopping at Epoch [4499]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 138.87690735 \t Avg Val Loss: 131.20423889 \t Avg Test Loss: 139.16476440\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 12.93055248 \t Avg Val Loss: 11.10789871 \t Avg Test Loss: 12.65973949\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 7.21429014 \t Avg Val Loss: 7.13401175 \t Avg Test Loss: 7.47134924\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 2.53934669 \t Avg Val Loss: 3.75624919 \t Avg Test Loss: 3.56972885\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 0.78291047 \t Avg Val Loss: 1.81315815 \t Avg Test Loss: 1.82929206\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 0.54314822 \t Avg Val Loss: 1.13224602 \t Avg Test Loss: 1.32341468\n",
      "Test fold 2 \t Epoch [6000] \t Avg Train Loss: 0.46807060 \t Avg Val Loss: 0.91701537 \t Avg Test Loss: 1.14097846\n",
      "Test fold 2 \t Epoch [7000] \t Avg Train Loss: 0.42745805 \t Avg Val Loss: 0.83233541 \t Avg Test Loss: 1.03582442\n",
      "Test fold 2 \t Epoch [8000] \t Avg Train Loss: 0.39783323 \t Avg Val Loss: 0.79975593 \t Avg Test Loss: 0.96413469\n",
      "Early stopping at Epoch [8571]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 130.91859436 \t Avg Val Loss: 133.84957886 \t Avg Test Loss: 128.97761536\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 5.87211180 \t Avg Val Loss: 12.79756165 \t Avg Test Loss: 7.77010345\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 1.06267321 \t Avg Val Loss: 2.94930458 \t Avg Test Loss: 1.23971593\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 0.52682775 \t Avg Val Loss: 1.42080820 \t Avg Test Loss: 2.38283777\n",
      "Test fold 1 \t Epoch [4000] \t Avg Train Loss: 0.44392270 \t Avg Val Loss: 1.04065883 \t Avg Test Loss: 3.19194007\n",
      "Early stopping at Epoch [4986]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 128.07896423 \t Avg Val Loss: 120.42496490 \t Avg Test Loss: 127.82729340\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 5.89540911 \t Avg Val Loss: 6.13912010 \t Avg Test Loss: 6.28814173\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 0.77591181 \t Avg Val Loss: 1.77023184 \t Avg Test Loss: 1.80206525\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 0.50114655 \t Avg Val Loss: 1.00316024 \t Avg Test Loss: 1.22058380\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 0.43538374 \t Avg Val Loss: 0.85252655 \t Avg Test Loss: 1.06614065\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 0.39836806 \t Avg Val Loss: 0.80740917 \t Avg Test Loss: 0.97528231\n",
      "Early stopping at Epoch [5431]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 127.12493896 \t Avg Val Loss: 128.76541138 \t Avg Test Loss: 123.97735596\n",
      "Test fold 1 \t Epoch [1000] \t Avg Train Loss: 2.50816202 \t Avg Val Loss: 6.34092855 \t Avg Test Loss: 3.44322610\n",
      "Test fold 1 \t Epoch [2000] \t Avg Train Loss: 0.53734028 \t Avg Val Loss: 1.52035320 \t Avg Test Loss: 1.59720612\n",
      "Test fold 1 \t Epoch [3000] \t Avg Train Loss: 0.40396622 \t Avg Val Loss: 1.10917437 \t Avg Test Loss: 2.44898486\n",
      "Test fold 1 \t Epoch [4000] \t Avg Train Loss: 0.36154106 \t Avg Val Loss: 1.07109821 \t Avg Test Loss: 3.70483470\n",
      "Early stopping at Epoch [4136]\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 130.58316040 \t Avg Val Loss: 122.76284027 \t Avg Test Loss: 130.08731079\n",
      "Test fold 2 \t Epoch [1000] \t Avg Train Loss: 5.02367687 \t Avg Val Loss: 5.52293110 \t Avg Test Loss: 5.58942509\n",
      "Test fold 2 \t Epoch [2000] \t Avg Train Loss: 0.67421722 \t Avg Val Loss: 1.49089158 \t Avg Test Loss: 1.60030520\n",
      "Test fold 2 \t Epoch [3000] \t Avg Train Loss: 0.47971320 \t Avg Val Loss: 0.94259107 \t Avg Test Loss: 1.15799069\n",
      "Test fold 2 \t Epoch [4000] \t Avg Train Loss: 0.42408058 \t Avg Val Loss: 0.83540934 \t Avg Test Loss: 1.02707124\n",
      "Test fold 2 \t Epoch [5000] \t Avg Train Loss: 0.38924393 \t Avg Val Loss: 0.80750072 \t Avg Test Loss: 0.95163959\n",
      "Early stopping at Epoch [5271]\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 126.33751678 \t Avg Val Loss: 127.78432465 \t Avg Test Loss: 123.35956573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 73\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(model(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m), y_val)\n\u001b[0;32m     74\u001b[0m     avg_test_loss \u001b[38;5;241m=\u001b[39m criterion(model(torch\u001b[38;5;241m.\u001b[39mtensor(features_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)), target_test\u001b[38;5;241m.\u001b[39mto(device))  \u001b[38;5;66;03m# Ensure target_test is on the same device\u001b[39;00m\n\u001b[0;32m     76\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    # Load fold and target data\n",
    "    folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "    target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')\n",
    "    features_df = pd.read_csv(f'../../training_data/{dataset}/features.csv')\n",
    "\n",
    "    # Prepare CSV file for logging\n",
    "    report_path = f'report_{dataset}.csv'\n",
    "    report_header = ['dataset', 'num_layers', 'layer_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n",
    "    if not os.path.exists(report_path):\n",
    "        pd.DataFrame(columns=report_header).to_csv(report_path, index=False)\n",
    "\n",
    "    # Make sure directories exist\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    os.makedirs('predictions_all', exist_ok=True)\n",
    "\n",
    "    # Iterate over each configuration in the config_list\n",
    "    for config in config_list:\n",
    "        num_layers, layer_size = config['num_layers'], config['layer_size']\n",
    "        for test_fold in sorted(folds_df['fold'].unique()):\n",
    "            # Record start time\n",
    "            fold_start_time = time.time()\n",
    "\n",
    "            # Split data into training and test sets based on the fold\n",
    "            train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "            test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "            # Prepare train and test sequences as arrays\n",
    "            features_train = features_df[features_df['sequenceID'].isin(train_ids)].iloc[:, 1:].to_numpy()\n",
    "            target_train = target_df[target_df['sequenceID'].isin(train_ids)].iloc[:, 1:].to_numpy()\n",
    "            features_test = features_df[features_df['sequenceID'].isin(test_ids)].iloc[:, 1:].to_numpy()\n",
    "\n",
    "            # Normalize training features\n",
    "            scaler = MinMaxScaler()  # Create scaler instance\n",
    "            features_train = scaler.fit_transform(features_train)  # Fit on training data\n",
    "            features_test = scaler.transform(features_test)  # Transform test data using the same parameters\n",
    "\n",
    "            # Convert target data to tensors\n",
    "            target_test = torch.tensor(target_df[target_df['sequenceID'].isin(test_ids)].iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "            # Split training data into subtrain and validation sets\n",
    "            X_subtrain, X_val, y_subtrain, y_val = train_test_split(features_train, target_train, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Move target tensors to the correct device\n",
    "            y_subtrain = torch.tensor(y_subtrain, dtype=torch.float32).to(device)\n",
    "            y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Initialize the MLP model, loss function, and optimizer\n",
    "            layer_sizes = [layer_size] * num_layers\n",
    "            model = MLPModel(X_subtrain.shape[1], layer_sizes).to(device)\n",
    "            criterion = SquaredHingeLoss().to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "            \n",
    "            # Variables for early stopping\n",
    "            best_val_loss, patience_counter = float('inf'), 0\n",
    "            best_model_state, stop_epoch = None, 0\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(max_epochs):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                # Convert training input to tensor and move to device\n",
    "                predictions = model(torch.tensor(X_subtrain, dtype=torch.float32).to(device))\n",
    "                loss = criterion(predictions, y_subtrain)\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                # Evaluation phase\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = criterion(model(torch.tensor(X_val, dtype=torch.float32).to(device)), y_val)\n",
    "                    avg_test_loss = criterion(model(torch.tensor(features_test, dtype=torch.float32).to(device)), target_test.to(device))  # Ensure target_test is on the same device\n",
    "            \n",
    "                avg_train_loss = loss.item()\n",
    "            \n",
    "                if epoch % 1000 == 0:\n",
    "                    print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {val_loss.item():.8f} \\t Avg Test Loss: {avg_test_loss.item():.8f}')\n",
    "            \n",
    "                # Early stopping check\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss, best_model_state = val_loss.item(), model.state_dict()\n",
    "                    patience_counter = 0\n",
    "                    stop_epoch = epoch\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at Epoch [{epoch}]\")\n",
    "                    break\n",
    "            \n",
    "            # Restore best model state for final evaluation\n",
    "            if best_model_state:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            # Record end time and calculate elapsed time\n",
    "            elapsed_time = time.time() - fold_start_time\n",
    "            \n",
    "            # Log the results\n",
    "            report_entry = {\n",
    "                'dataset': dataset,\n",
    "                'num_layers': num_layers,\n",
    "                'layer_size': layer_size,\n",
    "                'test_fold': test_fold,\n",
    "                'stop_epoch': stop_epoch,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': best_val_loss,\n",
    "                'test_loss': avg_test_loss.item(),\n",
    "                'time': elapsed_time\n",
    "            }\n",
    "            pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)\n",
    "            \n",
    "            # Predict on the test set and save to CSV\n",
    "            model.eval()\n",
    "            pred_lldas = model(torch.tensor(features_test, dtype=torch.float32).to(device)).detach().cpu().numpy().ravel()  # Move predictions to CPU for saving\n",
    "            lldas_df = pd.DataFrame({'sequenceID': features_df[features_df['sequenceID'].isin(test_ids)]['sequenceID'], 'llda': pred_lldas})\n",
    "            lldas_df.to_csv(f'predictions_all/{dataset}.{num_layers}layers_{layer_size}neurons_{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
