{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'detailed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):               # x shape: (batch_size, seq_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)      # lstm_out shape: (batch_size, seq_length, hidden_size)\n",
    "        last_out = lstm_out[:, -1, :]   # last_out shape: (batch_size, hidden_size)\n",
    "        x = self.fc(last_out)           # x shape: (batch_size, 1)\n",
    "        x = 10 * torch.tanh(x)          # Apply 10 * tanh() to the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1)\n",
    "            output_seq = model(seq_input)\n",
    "            predictions.append(output_seq.item())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_value(model, seqs, y, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0  # Total validation loss\n",
    "    with torch.no_grad():\n",
    "        for i, seq_input in enumerate(seqs):\n",
    "            target = y[i].unsqueeze(0)\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_length, input_size)\n",
    "            output_seq = model(seq_input)\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(seqs)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Train Loss: 0.45903316 \t Val Loss: 0.45906579 \t Test Loss: 0.47410812\n",
      "Test fold 1 \t Epoch [  1] \t Train Loss: 0.45891933 \t Val Loss: 0.48059026 \t Test Loss: 0.45942527\n",
      "Test fold 1 \t Epoch [  2] \t Train Loss: 0.45651334 \t Val Loss: 0.45475705 \t Test Loss: 0.45804399\n",
      "Test fold 1 \t Epoch [  3] \t Train Loss: 0.44910014 \t Val Loss: 0.49542897 \t Test Loss: 0.46555530\n",
      "Test fold 1 \t Epoch [  4] \t Train Loss: 0.45006981 \t Val Loss: 0.49493023 \t Test Loss: 0.46453555\n",
      "Test fold 1 \t Epoch [  5] \t Train Loss: 0.45177402 \t Val Loss: 0.45334843 \t Test Loss: 0.45568750\n",
      "Test fold 1 \t Epoch [  6] \t Train Loss: 0.45184121 \t Val Loss: 0.46068227 \t Test Loss: 0.47855875\n",
      "Test fold 1 \t Epoch [  7] \t Train Loss: 0.44865381 \t Val Loss: 0.46212906 \t Test Loss: 0.48003811\n",
      "Test fold 1 \t Epoch [  8] \t Train Loss: 0.44837776 \t Val Loss: 0.47006837 \t Test Loss: 0.44836301\n",
      "Test fold 1 \t Epoch [  9] \t Train Loss: 0.44947866 \t Val Loss: 0.45628433 \t Test Loss: 0.46515468\n",
      "Test fold 1 \t Epoch [ 10] \t Train Loss: 0.44207826 \t Val Loss: 0.45193660 \t Test Loss: 0.46526814\n",
      "Test fold 1 \t Epoch [ 11] \t Train Loss: 0.44054324 \t Val Loss: 0.45764123 \t Test Loss: 0.47274371\n",
      "Test fold 1 \t Epoch [ 12] \t Train Loss: 0.44076209 \t Val Loss: 0.44635555 \t Test Loss: 0.44521044\n",
      "Test fold 1 \t Epoch [ 13] \t Train Loss: 0.43214325 \t Val Loss: 0.44841152 \t Test Loss: 0.47399125\n",
      "Test fold 1 \t Epoch [ 14] \t Train Loss: 0.42562536 \t Val Loss: 0.42690411 \t Test Loss: 0.42958179\n",
      "Test fold 1 \t Epoch [ 15] \t Train Loss: 0.38599609 \t Val Loss: 0.48261610 \t Test Loss: 0.41230933\n",
      "Test fold 1 \t Epoch [ 16] \t Train Loss: 0.35952751 \t Val Loss: 0.35326113 \t Test Loss: 0.33579932\n",
      "Test fold 1 \t Epoch [ 17] \t Train Loss: 0.33825775 \t Val Loss: 0.36748773 \t Test Loss: 0.32261628\n",
      "Test fold 1 \t Epoch [ 18] \t Train Loss: 0.32475204 \t Val Loss: 0.43142470 \t Test Loss: 0.34012558\n",
      "Test fold 1 \t Epoch [ 19] \t Train Loss: 0.34740213 \t Val Loss: 0.37367855 \t Test Loss: 0.36266861\n",
      "Test fold 1 \t Epoch [ 20] \t Train Loss: 0.34245176 \t Val Loss: 0.36361325 \t Test Loss: 0.32443303\n",
      "Test fold 1 \t Epoch [ 21] \t Train Loss: 0.33601987 \t Val Loss: 0.36994150 \t Test Loss: 0.32630560\n",
      "Test fold 1 \t Epoch [ 22] \t Train Loss: 0.32944271 \t Val Loss: 0.35531042 \t Test Loss: 0.32284570\n",
      "Test fold 1 \t Epoch [ 23] \t Train Loss: 0.32796309 \t Val Loss: 0.38270454 \t Test Loss: 0.31468218\n",
      "Test fold 1 \t Epoch [ 24] \t Train Loss: 0.32290765 \t Val Loss: 0.35095626 \t Test Loss: 0.28291017\n",
      "Test fold 1 \t Epoch [ 25] \t Train Loss: 0.32982196 \t Val Loss: 0.38064079 \t Test Loss: 0.28929721\n",
      "Test fold 1 \t Epoch [ 26] \t Train Loss: 0.32510420 \t Val Loss: 0.37343016 \t Test Loss: 0.32969961\n",
      "Test fold 1 \t Epoch [ 27] \t Train Loss: 0.32963815 \t Val Loss: 0.34843796 \t Test Loss: 0.27955715\n",
      "Test fold 1 \t Epoch [ 28] \t Train Loss: 0.31600971 \t Val Loss: 0.36104188 \t Test Loss: 0.34349840\n",
      "Test fold 1 \t Epoch [ 29] \t Train Loss: 0.32553383 \t Val Loss: 0.39744396 \t Test Loss: 0.31707376\n",
      "Test fold 1 \t Epoch [ 30] \t Train Loss: 0.31728979 \t Val Loss: 0.35230006 \t Test Loss: 0.29680348\n",
      "Test fold 1 \t Epoch [ 31] \t Train Loss: 0.32868320 \t Val Loss: 0.37960582 \t Test Loss: 0.30357341\n",
      "Test fold 1 \t Epoch [ 32] \t Train Loss: 0.31141138 \t Val Loss: 0.40566631 \t Test Loss: 0.32069703\n",
      "Test fold 1 \t Epoch [ 33] \t Train Loss: 0.32537840 \t Val Loss: 0.33770098 \t Test Loss: 0.30356089\n",
      "Test fold 1 \t Epoch [ 34] \t Train Loss: 0.31463556 \t Val Loss: 0.33736852 \t Test Loss: 0.30725730\n",
      "Test fold 1 \t Epoch [ 35] \t Train Loss: 0.31136865 \t Val Loss: 0.35826916 \t Test Loss: 0.31742054\n",
      "Test fold 1 \t Epoch [ 36] \t Train Loss: 0.31070402 \t Val Loss: 0.34084716 \t Test Loss: 0.30991761\n",
      "Test fold 1 \t Epoch [ 37] \t Train Loss: 0.31306677 \t Val Loss: 0.35570591 \t Test Loss: 0.30926033\n",
      "Test fold 1 \t Epoch [ 38] \t Train Loss: 0.31669582 \t Val Loss: 0.33590876 \t Test Loss: 0.29867923\n",
      "Test fold 1 \t Epoch [ 39] \t Train Loss: 0.31495998 \t Val Loss: 0.35151991 \t Test Loss: 0.30593762\n",
      "Test fold 1 \t Epoch [ 40] \t Train Loss: 0.31556303 \t Val Loss: 0.35118245 \t Test Loss: 0.28432640\n",
      "Test fold 1 \t Epoch [ 41] \t Train Loss: 0.31365598 \t Val Loss: 0.33947981 \t Test Loss: 0.31610588\n",
      "Test fold 1 \t Epoch [ 42] \t Train Loss: 0.31192437 \t Val Loss: 0.33218604 \t Test Loss: 0.28728044\n",
      "Test fold 1 \t Epoch [ 43] \t Train Loss: 0.30651848 \t Val Loss: 0.31232164 \t Test Loss: 0.27713937\n",
      "Test fold 1 \t Epoch [ 44] \t Train Loss: 0.30287030 \t Val Loss: 0.33654745 \t Test Loss: 0.30200770\n",
      "Test fold 1 \t Epoch [ 45] \t Train Loss: 0.31078432 \t Val Loss: 0.40958605 \t Test Loss: 0.32714057\n",
      "Test fold 1 \t Epoch [ 46] \t Train Loss: 0.31133751 \t Val Loss: 0.36501369 \t Test Loss: 0.30777801\n",
      "Test fold 1 \t Epoch [ 47] \t Train Loss: 0.30511373 \t Val Loss: 0.35358535 \t Test Loss: 0.33591529\n",
      "Test fold 1 \t Epoch [ 48] \t Train Loss: 0.30871742 \t Val Loss: 0.31908343 \t Test Loss: 0.29311395\n",
      "Test fold 1 \t Epoch [ 49] \t Train Loss: 0.31090445 \t Val Loss: 0.33422923 \t Test Loss: 0.28257672\n",
      "Test fold 1 \t Epoch [ 50] \t Train Loss: 0.30497981 \t Val Loss: 0.31061108 \t Test Loss: 0.27480340\n",
      "Test fold 1 \t Epoch [ 51] \t Train Loss: 0.30971471 \t Val Loss: 0.33130342 \t Test Loss: 0.30055696\n",
      "Test fold 1 \t Epoch [ 52] \t Train Loss: 0.30765565 \t Val Loss: 0.34705285 \t Test Loss: 0.30154600\n",
      "Test fold 1 \t Epoch [ 53] \t Train Loss: 0.30459830 \t Val Loss: 0.35142513 \t Test Loss: 0.30117300\n",
      "Test fold 1 \t Epoch [ 54] \t Train Loss: 0.30479462 \t Val Loss: 0.34239119 \t Test Loss: 0.28052817\n",
      "Test fold 1 \t Epoch [ 55] \t Train Loss: 0.31030362 \t Val Loss: 0.33822554 \t Test Loss: 0.29752465\n",
      "Test fold 1 \t Epoch [ 56] \t Train Loss: 0.30382349 \t Val Loss: 0.34232432 \t Test Loss: 0.33482773\n",
      "Test fold 1 \t Epoch [ 57] \t Train Loss: 0.30263952 \t Val Loss: 0.30883671 \t Test Loss: 0.28108987\n",
      "Test fold 1 \t Epoch [ 58] \t Train Loss: 0.30094037 \t Val Loss: 0.34290192 \t Test Loss: 0.29093128\n",
      "Test fold 1 \t Epoch [ 59] \t Train Loss: 0.30682386 \t Val Loss: 0.31664384 \t Test Loss: 0.29115712\n",
      "Test fold 1 \t Epoch [ 60] \t Train Loss: 0.30418676 \t Val Loss: 0.31812131 \t Test Loss: 0.28008676\n",
      "Test fold 1 \t Epoch [ 61] \t Train Loss: 0.30008248 \t Val Loss: 0.32094676 \t Test Loss: 0.27537527\n",
      "Test fold 1 \t Epoch [ 62] \t Train Loss: 0.30585302 \t Val Loss: 0.32358240 \t Test Loss: 0.29182970\n",
      "Test fold 1 \t Epoch [ 63] \t Train Loss: 0.30007387 \t Val Loss: 0.33863206 \t Test Loss: 0.29709977\n",
      "Test fold 1 \t Epoch [ 64] \t Train Loss: 0.29735998 \t Val Loss: 0.36580614 \t Test Loss: 0.28812641\n",
      "Test fold 1 \t Epoch [ 65] \t Train Loss: 0.29547697 \t Val Loss: 0.34567705 \t Test Loss: 0.31439005\n",
      "Test fold 1 \t Epoch [ 66] \t Train Loss: 0.30039753 \t Val Loss: 0.32602559 \t Test Loss: 0.28420679\n",
      "Test fold 1 \t Epoch [ 67] \t Train Loss: 0.30311225 \t Val Loss: 0.31764840 \t Test Loss: 0.27833048\n",
      "Test fold 1 \t Epoch [ 68] \t Train Loss: 0.29998451 \t Val Loss: 0.35501878 \t Test Loss: 0.29302161\n",
      "Test fold 1 \t Epoch [ 69] \t Train Loss: 0.30306852 \t Val Loss: 0.32174275 \t Test Loss: 0.29936844\n",
      "Test fold 1 \t Epoch [ 70] \t Train Loss: 0.29647524 \t Val Loss: 0.32308721 \t Test Loss: 0.28997050\n",
      "Test fold 1 \t Epoch [ 71] \t Train Loss: 0.30020646 \t Val Loss: 0.36465810 \t Test Loss: 0.30267608\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')\n",
    "\n",
    "for test_fold in range(1, np.unique(folds_df['fold']).__len__() + 1):\n",
    "    # Split data into training and test sets\n",
    "    train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "    test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "    # train sequences\n",
    "    train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "\n",
    "    # test sequences\n",
    "    test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "    # target\n",
    "    target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "    target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "    y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy())\n",
    "    y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy())\n",
    "\n",
    "    # Split train into subtrain and validation (80% subtrain, 20% validation)\n",
    "    train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LSTMModel(1, 4, 1)\n",
    "    criterion = SquaredHingeLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Initialize variables for best validation loss and best model\n",
    "    best_val_loss = float('inf')    # Set best validation loss to infinity initially\n",
    "    patience_counter = 0            # Counter for early stopping\n",
    "    patience = 50                   # Number of epochs to wait for improvement before stopping\n",
    "    best_model_state = None         # Variable to store the best model parameters\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        # Shuffle both train_seqs and y_train together\n",
    "        combined = list(zip(train_seqs, y_train))\n",
    "        random.shuffle(combined)\n",
    "        train_seqs, y_train = zip(*combined)\n",
    "\n",
    "        total_loss = 0  # Total training loss\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, seq_input in enumerate(train_seqs):\n",
    "            # Prepare input and target for the current sequence\n",
    "            target = y_train[i].unsqueeze(0)  # Get the corresponding target for the sequence\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_length, input_size)\n",
    "            output_seq = model(seq_input)                     # Get model output for the sequence\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))  # Calculate loss\n",
    "\n",
    "            # Stop training if loss is NaN\n",
    "            if torch.isnan(loss):\n",
    "                print(f'Stopping training at epoch {epoch} due to NaN loss.')\n",
    "                break\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss for this epoch (training)\n",
    "        average_loss = total_loss / len(train_seqs)\n",
    "\n",
    "        # Validation step\n",
    "        average_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "        average_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Train Loss: {average_loss:.8f} \\t Val Loss: {average_val_loss:.8f} \\t Test Loss: {average_test_loss:.8f}')\n",
    "\n",
    "        # Early stopping logic based on validation loss\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss  # Update best validation loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "\n",
    "            # Save the best model parameters in memory\n",
    "            best_model_state = model.state_dict()  # Store the model parameters\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter\n",
    "\n",
    "        # Stop training if patience is exceeded or NaN is detected\n",
    "        if patience_counter >= patience or torch.isnan(loss):\n",
    "            if torch.isnan(loss):\n",
    "                print(f'Training stopped due to NaN loss at epoch {epoch}.')\n",
    "            else:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "            break\n",
    "\n",
    "\n",
    "    # After training, you can restore the best model parameters if needed\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Test the model and collect outputs\n",
    "    pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "    # Save to CSV\n",
    "    lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "    lldas_df.to_csv(f'predictions/proposed.{dataset}.{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
