{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'detailed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):               # x shape: (batch_size, seq_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)      # lstm_out shape: (batch_size, seq_length, hidden_size)\n",
    "        last_out = lstm_out[:, -1, :]   # last_out shape: (batch_size, hidden_size)\n",
    "        x = self.fc(last_out)           # x shape: (batch_size, 1)\n",
    "        x = 10 * torch.tanh(x)          # Apply 10 * tanh() to the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1)\n",
    "            output_seq = model(seq_input)\n",
    "            predictions.append(output_seq.item())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_value(model, seqs, y, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0  # Total validation loss\n",
    "    with torch.no_grad():\n",
    "        for i, seq_input in enumerate(seqs):\n",
    "            target = y[i].unsqueeze(0)\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_length, input_size)\n",
    "            output_seq = model(seq_input)\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(seqs)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Train Loss: 0.48229446 \t Val Loss: 0.45973233 \t Test Loss: 0.46931493\n",
      "Test fold 1 \t Epoch [  1] \t Train Loss: 0.46183553 \t Val Loss: 0.46010516 \t Test Loss: 0.47595648\n",
      "Test fold 1 \t Epoch [  2] \t Train Loss: 0.46043774 \t Val Loss: 0.46108376 \t Test Loss: 0.46055121\n",
      "Test fold 1 \t Epoch [  3] \t Train Loss: 0.45933777 \t Val Loss: 0.45893147 \t Test Loss: 0.46968400\n",
      "Test fold 1 \t Epoch [  4] \t Train Loss: 0.45744963 \t Val Loss: 0.45808145 \t Test Loss: 0.46337114\n",
      "Test fold 1 \t Epoch [  5] \t Train Loss: 0.44897848 \t Val Loss: 0.45130296 \t Test Loss: 0.45432134\n",
      "Test fold 1 \t Epoch [  6] \t Train Loss: 0.44766698 \t Val Loss: 0.44063950 \t Test Loss: 0.45377125\n",
      "Test fold 1 \t Epoch [  7] \t Train Loss: 0.44001161 \t Val Loss: 0.43058209 \t Test Loss: 0.43896954\n",
      "Test fold 1 \t Epoch [  8] \t Train Loss: 0.42063975 \t Val Loss: 0.42361816 \t Test Loss: 0.41511300\n",
      "Test fold 1 \t Epoch [  9] \t Train Loss: 0.39603630 \t Val Loss: 0.42874476 \t Test Loss: 0.42876356\n",
      "Test fold 1 \t Epoch [ 10] \t Train Loss: 0.37961553 \t Val Loss: 0.39061147 \t Test Loss: 0.37059641\n",
      "Test fold 1 \t Epoch [ 11] \t Train Loss: 0.37128548 \t Val Loss: 0.38474773 \t Test Loss: 0.36258899\n",
      "Test fold 1 \t Epoch [ 12] \t Train Loss: 0.35409202 \t Val Loss: 0.40595233 \t Test Loss: 0.36032181\n",
      "Test fold 1 \t Epoch [ 13] \t Train Loss: 0.34847596 \t Val Loss: 0.40178735 \t Test Loss: 0.40736105\n",
      "Test fold 1 \t Epoch [ 14] \t Train Loss: 0.34241658 \t Val Loss: 0.45425027 \t Test Loss: 0.34794149\n",
      "Test fold 1 \t Epoch [ 15] \t Train Loss: 0.32932610 \t Val Loss: 0.35470878 \t Test Loss: 0.29603648\n",
      "Test fold 1 \t Epoch [ 16] \t Train Loss: 0.33193606 \t Val Loss: 0.37028135 \t Test Loss: 0.30298954\n",
      "Test fold 1 \t Epoch [ 17] \t Train Loss: 0.32250446 \t Val Loss: 0.34654134 \t Test Loss: 0.28423577\n",
      "Test fold 1 \t Epoch [ 18] \t Train Loss: 0.31511865 \t Val Loss: 0.34925695 \t Test Loss: 0.31967994\n",
      "Test fold 1 \t Epoch [ 19] \t Train Loss: 0.32034575 \t Val Loss: 0.36108470 \t Test Loss: 0.29836082\n",
      "Test fold 1 \t Epoch [ 20] \t Train Loss: 0.30933244 \t Val Loss: 0.34571457 \t Test Loss: 0.29152801\n",
      "Test fold 1 \t Epoch [ 21] \t Train Loss: 0.30902236 \t Val Loss: 0.34129506 \t Test Loss: 0.30020388\n",
      "Test fold 1 \t Epoch [ 22] \t Train Loss: 0.30238421 \t Val Loss: 0.39198292 \t Test Loss: 0.34202162\n",
      "Test fold 1 \t Epoch [ 23] \t Train Loss: 0.30752990 \t Val Loss: 0.32879856 \t Test Loss: 0.28861265\n",
      "Test fold 1 \t Epoch [ 24] \t Train Loss: 0.30468753 \t Val Loss: 0.34407558 \t Test Loss: 0.28681175\n",
      "Test fold 1 \t Epoch [ 25] \t Train Loss: 0.30880641 \t Val Loss: 0.32976779 \t Test Loss: 0.32107036\n",
      "Test fold 1 \t Epoch [ 26] \t Train Loss: 0.29294447 \t Val Loss: 0.34112710 \t Test Loss: 0.31455874\n",
      "Test fold 1 \t Epoch [ 27] \t Train Loss: 0.29586439 \t Val Loss: 0.33843806 \t Test Loss: 0.31658752\n",
      "Test fold 1 \t Epoch [ 28] \t Train Loss: 0.28555667 \t Val Loss: 0.32842334 \t Test Loss: 0.30464010\n",
      "Test fold 1 \t Epoch [ 29] \t Train Loss: 0.28378318 \t Val Loss: 0.28965531 \t Test Loss: 0.30144756\n",
      "Test fold 1 \t Epoch [ 30] \t Train Loss: 0.28899046 \t Val Loss: 0.26954180 \t Test Loss: 0.35139060\n",
      "Test fold 1 \t Epoch [ 31] \t Train Loss: 0.29023339 \t Val Loss: 0.28461827 \t Test Loss: 0.31932147\n",
      "Test fold 1 \t Epoch [ 32] \t Train Loss: 0.27463103 \t Val Loss: 0.30928972 \t Test Loss: 0.33638350\n",
      "Test fold 1 \t Epoch [ 33] \t Train Loss: 0.28688365 \t Val Loss: 0.34821775 \t Test Loss: 0.32105229\n",
      "Test fold 1 \t Epoch [ 34] \t Train Loss: 0.27436706 \t Val Loss: 0.37276355 \t Test Loss: 0.30669786\n",
      "Test fold 1 \t Epoch [ 35] \t Train Loss: 0.27446488 \t Val Loss: 0.29297443 \t Test Loss: 0.33517818\n",
      "Test fold 1 \t Epoch [ 36] \t Train Loss: 0.27211022 \t Val Loss: 0.31742787 \t Test Loss: 0.31193313\n",
      "Test fold 1 \t Epoch [ 37] \t Train Loss: 0.26854089 \t Val Loss: 0.30049657 \t Test Loss: 0.33010525\n",
      "Test fold 1 \t Epoch [ 38] \t Train Loss: 0.26018924 \t Val Loss: 0.39828023 \t Test Loss: 0.31299038\n",
      "Test fold 1 \t Epoch [ 39] \t Train Loss: 0.26211049 \t Val Loss: 0.31768957 \t Test Loss: 0.36297678\n",
      "Test fold 1 \t Epoch [ 40] \t Train Loss: 0.26212676 \t Val Loss: 0.31715570 \t Test Loss: 0.34226038\n",
      "Test fold 1 \t Epoch [ 41] \t Train Loss: 0.25414084 \t Val Loss: 0.31959606 \t Test Loss: 0.34951189\n",
      "Test fold 1 \t Epoch [ 42] \t Train Loss: 0.27601895 \t Val Loss: 0.31579498 \t Test Loss: 0.32455175\n",
      "Test fold 1 \t Epoch [ 43] \t Train Loss: 0.25606634 \t Val Loss: 0.30236259 \t Test Loss: 0.31521030\n",
      "Test fold 1 \t Epoch [ 44] \t Train Loss: 0.25411919 \t Val Loss: 0.40025796 \t Test Loss: 0.37001836\n",
      "Test fold 1 \t Epoch [ 45] \t Train Loss: 0.29039426 \t Val Loss: 0.37321990 \t Test Loss: 0.33947850\n",
      "Test fold 1 \t Epoch [ 46] \t Train Loss: 0.26136709 \t Val Loss: 0.38451572 \t Test Loss: 0.39039127\n",
      "Test fold 1 \t Epoch [ 47] \t Train Loss: 0.25524168 \t Val Loss: 0.37214766 \t Test Loss: 0.31780927\n",
      "Test fold 1 \t Epoch [ 48] \t Train Loss: 0.24524483 \t Val Loss: 0.34873508 \t Test Loss: 0.32737973\n",
      "Test fold 1 \t Epoch [ 49] \t Train Loss: 0.25337599 \t Val Loss: 0.34665210 \t Test Loss: 0.32813883\n",
      "Test fold 1 \t Epoch [ 50] \t Train Loss: 0.24312616 \t Val Loss: 0.36165911 \t Test Loss: 0.40186647\n",
      "Test fold 1 \t Epoch [ 51] \t Train Loss: 0.23998136 \t Val Loss: 0.37862099 \t Test Loss: 0.38108686\n",
      "Test fold 1 \t Epoch [ 52] \t Train Loss: 0.25286934 \t Val Loss: 0.29848920 \t Test Loss: 0.34770673\n",
      "Test fold 1 \t Epoch [ 53] \t Train Loss: 0.23508266 \t Val Loss: 0.36262985 \t Test Loss: 0.34104170\n",
      "Test fold 1 \t Epoch [ 54] \t Train Loss: 0.23642344 \t Val Loss: 0.30491401 \t Test Loss: 0.32198067\n",
      "Test fold 1 \t Epoch [ 55] \t Train Loss: 0.27700254 \t Val Loss: 0.35981025 \t Test Loss: 0.32154066\n",
      "Test fold 1 \t Epoch [ 56] \t Train Loss: 0.24076385 \t Val Loss: 0.37815504 \t Test Loss: 0.33819631\n",
      "Test fold 1 \t Epoch [ 57] \t Train Loss: 0.22975716 \t Val Loss: 0.38509081 \t Test Loss: 0.33711290\n",
      "Test fold 1 \t Epoch [ 58] \t Train Loss: 0.22627235 \t Val Loss: 0.36775627 \t Test Loss: 0.31510268\n",
      "Test fold 1 \t Epoch [ 59] \t Train Loss: 0.22889777 \t Val Loss: 0.42847866 \t Test Loss: 0.35691391\n",
      "Test fold 1 \t Epoch [ 60] \t Train Loss: 0.23546544 \t Val Loss: 0.40953018 \t Test Loss: 0.32902915\n",
      "Test fold 1 \t Epoch [ 61] \t Train Loss: 0.22235941 \t Val Loss: 0.37427847 \t Test Loss: 0.32113009\n",
      "Test fold 1 \t Epoch [ 62] \t Train Loss: 0.22223482 \t Val Loss: 0.33811280 \t Test Loss: 0.32823003\n",
      "Test fold 1 \t Epoch [ 63] \t Train Loss: 0.20959818 \t Val Loss: 0.41546474 \t Test Loss: 0.34130794\n",
      "Test fold 1 \t Epoch [ 64] \t Train Loss: 0.22371487 \t Val Loss: 0.39731007 \t Test Loss: 0.29102078\n",
      "Test fold 1 \t Epoch [ 65] \t Train Loss: 0.23586381 \t Val Loss: 0.31863966 \t Test Loss: 0.34413603\n",
      "Test fold 1 \t Epoch [ 66] \t Train Loss: 0.22631960 \t Val Loss: 0.41590451 \t Test Loss: 0.33917059\n",
      "Test fold 1 \t Epoch [ 67] \t Train Loss: 0.22962576 \t Val Loss: 0.36193705 \t Test Loss: 0.33342049\n",
      "Test fold 1 \t Epoch [ 68] \t Train Loss: 0.21901570 \t Val Loss: 0.35115186 \t Test Loss: 0.33847081\n",
      "Test fold 1 \t Epoch [ 69] \t Train Loss: 0.21078755 \t Val Loss: 0.37055173 \t Test Loss: 0.32546941\n",
      "Test fold 1 \t Epoch [ 70] \t Train Loss: 0.21459919 \t Val Loss: 0.37355528 \t Test Loss: 0.31310033\n",
      "Test fold 1 \t Epoch [ 71] \t Train Loss: 0.21027743 \t Val Loss: 0.35111784 \t Test Loss: 0.31435665\n",
      "Test fold 1 \t Epoch [ 72] \t Train Loss: 0.20894688 \t Val Loss: 0.34142428 \t Test Loss: 0.33699099\n",
      "Test fold 1 \t Epoch [ 73] \t Train Loss: 0.22206239 \t Val Loss: 0.36762085 \t Test Loss: 0.34272066\n",
      "Test fold 1 \t Epoch [ 74] \t Train Loss: 0.20251846 \t Val Loss: 0.45098083 \t Test Loss: 0.35427542\n",
      "Test fold 1 \t Epoch [ 75] \t Train Loss: 0.20937595 \t Val Loss: 0.43725793 \t Test Loss: 0.35337639\n",
      "Test fold 1 \t Epoch [ 76] \t Train Loss: 0.19923356 \t Val Loss: 0.38129475 \t Test Loss: 0.38441790\n",
      "Test fold 1 \t Epoch [ 77] \t Train Loss: 0.19796692 \t Val Loss: 0.43026231 \t Test Loss: 0.34149002\n",
      "Test fold 1 \t Epoch [ 78] \t Train Loss: 0.20146104 \t Val Loss: 0.34828135 \t Test Loss: 0.32694461\n",
      "Test fold 1 \t Epoch [ 79] \t Train Loss: 0.20822965 \t Val Loss: 0.39287747 \t Test Loss: 0.36264383\n",
      "Test fold 1 \t Epoch [ 80] \t Train Loss: 0.19746567 \t Val Loss: 0.34785072 \t Test Loss: 0.40586225\n",
      "Early stopping triggered after 81 epochs.\n",
      "Test fold 2 \t Epoch [  0] \t Train Loss: 0.48377109 \t Val Loss: 0.40863636 \t Test Loss: 0.55234902\n",
      "Test fold 2 \t Epoch [  1] \t Train Loss: 0.47372840 \t Val Loss: 0.35797044 \t Test Loss: 0.51984645\n",
      "Test fold 2 \t Epoch [  2] \t Train Loss: 0.46582289 \t Val Loss: 0.37718607 \t Test Loss: 0.52820423\n",
      "Test fold 2 \t Epoch [  3] \t Train Loss: 0.46471474 \t Val Loss: 0.35005506 \t Test Loss: 0.52726422\n",
      "Test fold 2 \t Epoch [  4] \t Train Loss: 0.46254530 \t Val Loss: 0.37153557 \t Test Loss: 0.55452712\n",
      "Test fold 2 \t Epoch [  5] \t Train Loss: 0.46512646 \t Val Loss: 0.36519999 \t Test Loss: 0.51809589\n",
      "Test fold 2 \t Epoch [  6] \t Train Loss: 0.45556560 \t Val Loss: 0.35340311 \t Test Loss: 0.50377073\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')\n",
    "\n",
    "for test_fold in range(1, np.unique(folds_df['fold']).__len__() + 1):\n",
    "    # Split data into training and test sets\n",
    "    train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "    test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "    # train sequences\n",
    "    train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "\n",
    "    # test sequences\n",
    "    test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "    # target\n",
    "    target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "    target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "    y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy())\n",
    "    y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy())\n",
    "\n",
    "    # Split train into subtrain and validation (80% subtrain, 20% validation)\n",
    "    train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LSTMModel(1, 16, 2)\n",
    "    criterion = SquaredHingeLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Initialize variables for best validation loss and best model\n",
    "    best_val_loss = float('inf')    # Set best validation loss to infinity initially\n",
    "    patience_counter = 0            # Counter for early stopping\n",
    "    patience = 50                   # Number of epochs to wait for improvement before stopping\n",
    "    best_model_state = None         # Variable to store the best model parameters\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1000):\n",
    "        # Shuffle both train_seqs and y_train together\n",
    "        combined = list(zip(train_seqs, y_train))\n",
    "        random.shuffle(combined)\n",
    "        train_seqs, y_train = zip(*combined)\n",
    "\n",
    "        total_loss = 0  # Total training loss\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, seq_input in enumerate(train_seqs):\n",
    "            # Prepare input and target for the current sequence\n",
    "            target = y_train[i].unsqueeze(0)  # Get the corresponding target for the sequence\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1)  # Shape: (1, seq_length, input_size)\n",
    "            output_seq = model(seq_input)                     # Get model output for the sequence\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))  # Calculate loss\n",
    "\n",
    "            # Stop training if loss is NaN\n",
    "            if torch.isnan(loss):\n",
    "                print(f'Stopping training at epoch {epoch} due to NaN loss.')\n",
    "                break\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss for this epoch (training)\n",
    "        average_loss = total_loss / len(train_seqs)\n",
    "\n",
    "        # Validation step\n",
    "        average_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "        average_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Train Loss: {average_loss:.8f} \\t Val Loss: {average_val_loss:.8f} \\t Test Loss: {average_test_loss:.8f}')\n",
    "\n",
    "        # Early stopping logic based on validation loss\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss  # Update best validation loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "\n",
    "            # Save the best model parameters in memory\n",
    "            best_model_state = model.state_dict()  # Store the model parameters\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter\n",
    "\n",
    "        # Stop training if patience is exceeded or NaN is detected\n",
    "        if patience_counter >= patience or torch.isnan(loss):\n",
    "            if torch.isnan(loss):\n",
    "                print(f'Training stopped due to NaN loss at epoch {epoch}.')\n",
    "            else:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "            break\n",
    "\n",
    "\n",
    "    # After training, you can restore the best model parameters if needed\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Test the model and collect outputs\n",
    "    pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "    # Save to CSV\n",
    "    lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "    lldas_df.to_csv(f'predictions/proposed.{dataset}.{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
