{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset = 'systematic'\n",
    "\n",
    "# Model parameters\n",
    "model_type = 'lstm'\n",
    "input_size = 1\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 50\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # LSTM\n",
    "        self.fc = nn.Linear(hidden_size, 1)                                         # Linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(x)  # Pass sequence through LSTM\n",
    "        last_out = lstm_out[:, -1, :]                        # Take the hidden state of the last time step\n",
    "        x = self.fc(last_out)                                # Linear combination\n",
    "        x = torch.relu(x + 10) - torch.relu(x - 10) - 10     # Clamp between -10 and 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()                                                        # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Add batch dimension and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            predictions.append(output_seq.item())                       # Store the prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss value\n",
    "def get_loss_value(model, test_seqs, y_test, criterion):\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for i, seq_input in enumerate(test_seqs):\n",
    "            target = y_test[i].unsqueeze(0).to(device)                  # Move target to device\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "            total_test_loss += loss.item()                              # Accumulate loss\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_seqs)                    # Calculate average loss\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence data from CSV\n",
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "# Group sequences by 'sequenceID'\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))\n",
    "\n",
    "# Load fold and target data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CSV file for logging\n",
    "report_path = f'report_{dataset}_{model_type}.csv'\n",
    "report_header = ['dataset', 'model', 'num_layers', 'hidden_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n",
    "if not os.path.exists(report_path):\n",
    "    pd.DataFrame(columns=report_header).to_csv(report_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.26531443 \t Avg Val Loss: 0.29339716 \t Avg Test Loss: 0.26542357\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.25540405 \t Avg Val Loss: 0.29833619 \t Avg Test Loss: 0.26467433\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.23730468 \t Avg Val Loss: 0.29162089 \t Avg Test Loss: 0.24972633\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.22974212 \t Avg Val Loss: 0.28795177 \t Avg Test Loss: 0.24929856\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.20528274 \t Avg Val Loss: 0.28127731 \t Avg Test Loss: 0.20878911\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.20303037 \t Avg Val Loss: 0.26840805 \t Avg Test Loss: 0.20285698\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.18730626 \t Avg Val Loss: 0.29213078 \t Avg Test Loss: 0.19331919\n",
      "Test fold 1 \t Epoch [140] \t Avg Train Loss: 0.18837465 \t Avg Val Loss: 0.29609277 \t Avg Test Loss: 0.21151809\n",
      "Test fold 1 \t Early stopping at Epoch [140]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.23722337 \t Best Test Loss: 0.23172667\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.29883869 \t Avg Val Loss: 0.32626914 \t Avg Test Loss: 0.23296058\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.25864046 \t Avg Val Loss: 0.30266654 \t Avg Test Loss: 0.22511581\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.24547579 \t Avg Val Loss: 0.27781509 \t Avg Test Loss: 0.20676525\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.20142722 \t Avg Val Loss: 0.22114111 \t Avg Test Loss: 0.17388057\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.14491465 \t Avg Val Loss: 0.15134642 \t Avg Test Loss: 0.14701368\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.13569185 \t Avg Val Loss: 0.14395212 \t Avg Test Loss: 0.13415485\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.13004748 \t Avg Val Loss: 0.15182707 \t Avg Test Loss: 0.14446510\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.12495217 \t Avg Val Loss: 0.14210656 \t Avg Test Loss: 0.14465508\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.12412458 \t Avg Val Loss: 0.14601510 \t Avg Test Loss: 0.14605447\n",
      "Test fold 2 \t Epoch [180] \t Avg Train Loss: 0.12227240 \t Avg Val Loss: 0.14255872 \t Avg Test Loss: 0.14762695\n",
      "Test fold 2 \t Epoch [200] \t Avg Train Loss: 0.12084944 \t Avg Val Loss: 0.14289460 \t Avg Test Loss: 0.15480910\n",
      "Test fold 2 \t Early stopping at Epoch [200]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.13859437 \t Best Test Loss: 0.14681594\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.28122308 \t Avg Val Loss: 0.28642830 \t Avg Test Loss: 0.17992013\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.27905585 \t Avg Val Loss: 0.29164848 \t Avg Test Loss: 0.18009903\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.27807446 \t Avg Val Loss: 0.29029774 \t Avg Test Loss: 0.17978961\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.27495789 \t Avg Val Loss: 0.29553063 \t Avg Test Loss: 0.18049405\n",
      "Test fold 3 \t Early stopping at Epoch [62]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.28552707 \t Best Test Loss: 0.17967289\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.28616225 \t Avg Val Loss: 0.23967027 \t Avg Test Loss: 0.26972124\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.26944350 \t Avg Val Loss: 0.23992877 \t Avg Test Loss: 0.26906085\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.26949290 \t Avg Val Loss: 0.24066776 \t Avg Test Loss: 0.26864102\n",
      "Test fold 4 \t Early stopping at Epoch [58]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.23861719 \t Best Test Loss: 0.27094105\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.28997523 \t Avg Val Loss: 0.22460312 \t Avg Test Loss: 0.32106426\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.26067976 \t Avg Val Loss: 0.22449223 \t Avg Test Loss: 0.32147906\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.25977882 \t Avg Val Loss: 0.22426307 \t Avg Test Loss: 0.32261183\n",
      "Test fold 5 \t Early stopping at Epoch [53]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.22388139 \t Best Test Loss: 0.32136906\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.32428572 \t Avg Val Loss: 0.26640780 \t Avg Test Loss: 0.32808689\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.25194643 \t Avg Val Loss: 0.25453466 \t Avg Test Loss: 0.32444530\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.21837227 \t Avg Val Loss: 0.22409133 \t Avg Test Loss: 0.28492229\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.22381037 \t Avg Val Loss: 0.22706624 \t Avg Test Loss: 0.28261854\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.20697634 \t Avg Val Loss: 0.20325784 \t Avg Test Loss: 0.25321612\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.21033563 \t Avg Val Loss: 0.22372318 \t Avg Test Loss: 0.27918951\n",
      "Test fold 6 \t Early stopping at Epoch [104]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.18402215 \t Best Test Loss: 0.24991940\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.29487995 \t Avg Val Loss: 0.29680723 \t Avg Test Loss: 0.26674265\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.25092167 \t Avg Val Loss: 0.29976348 \t Avg Test Loss: 0.26069693\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.23490420 \t Avg Val Loss: 0.30268126 \t Avg Test Loss: 0.25179380\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.25622736 \t Avg Val Loss: 0.29583676 \t Avg Test Loss: 0.26715505\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.22039461 \t Avg Val Loss: 0.25548707 \t Avg Test Loss: 0.22629218\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.19794551 \t Avg Val Loss: 0.22901309 \t Avg Test Loss: 0.18837591\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.13662867 \t Avg Val Loss: 0.20864535 \t Avg Test Loss: 0.15603585\n",
      "Test fold 1 \t Epoch [140] \t Avg Train Loss: 0.12957172 \t Avg Val Loss: 0.17970144 \t Avg Test Loss: 0.14895682\n",
      "Test fold 1 \t Epoch [160] \t Avg Train Loss: 0.11190630 \t Avg Val Loss: 0.16276930 \t Avg Test Loss: 0.10796153\n",
      "Test fold 1 \t Epoch [180] \t Avg Train Loss: 0.10684509 \t Avg Val Loss: 0.16478764 \t Avg Test Loss: 0.12899963\n",
      "Test fold 1 \t Epoch [200] \t Avg Train Loss: 0.09971840 \t Avg Val Loss: 0.16706485 \t Avg Test Loss: 0.14262448\n",
      "Test fold 1 \t Epoch [220] \t Avg Train Loss: 0.10346600 \t Avg Val Loss: 0.17890641 \t Avg Test Loss: 0.12886889\n",
      "Test fold 1 \t Early stopping at Epoch [225]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.15813545 \t Best Test Loss: 0.11255016\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.27775528 \t Avg Val Loss: 0.31135787 \t Avg Test Loss: 0.22662983\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.21816611 \t Avg Val Loss: 0.23251969 \t Avg Test Loss: 0.19821369\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.25298935 \t Avg Val Loss: 0.28318916 \t Avg Test Loss: 0.22256331\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.20677611 \t Avg Val Loss: 0.22000066 \t Avg Test Loss: 0.18839308\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.15304688 \t Avg Val Loss: 0.16314131 \t Avg Test Loss: 0.15366473\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.13150812 \t Avg Val Loss: 0.16124881 \t Avg Test Loss: 0.17082739\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.12179705 \t Avg Val Loss: 0.17027998 \t Avg Test Loss: 0.16607470\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.15378556 \t Avg Val Loss: 0.16945375 \t Avg Test Loss: 0.22197437\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.13712898 \t Avg Val Loss: 0.15701863 \t Avg Test Loss: 0.17567562\n",
      "Test fold 2 \t Epoch [180] \t Avg Train Loss: 0.12799574 \t Avg Val Loss: 0.15296026 \t Avg Test Loss: 0.15562177\n",
      "Test fold 2 \t Epoch [200] \t Avg Train Loss: 0.11848670 \t Avg Val Loss: 0.17173877 \t Avg Test Loss: 0.21604572\n",
      "Test fold 2 \t Epoch [220] \t Avg Train Loss: 0.11903461 \t Avg Val Loss: 0.16641907 \t Avg Test Loss: 0.15395404\n",
      "NaN loss detected at Epoch [233], Step [650]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.13479855 \t Best Test Loss: 0.18881521\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.28664628 \t Avg Val Loss: 0.29424388 \t Avg Test Loss: 0.18117524\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.25844161 \t Avg Val Loss: 0.28488139 \t Avg Test Loss: 0.18548136\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.24592178 \t Avg Val Loss: 0.26948042 \t Avg Test Loss: 0.18074470\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.14536858 \t Avg Val Loss: 0.14110492 \t Avg Test Loss: 0.13323552\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.14036828 \t Avg Val Loss: 0.13592189 \t Avg Test Loss: 0.12112696\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.13242443 \t Avg Val Loss: 0.14329607 \t Avg Test Loss: 0.13006480\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.12654716 \t Avg Val Loss: 0.12888126 \t Avg Test Loss: 0.13579554\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.14019034 \t Avg Val Loss: 0.12425258 \t Avg Test Loss: 0.12691460\n",
      "Test fold 3 \t Epoch [160] \t Avg Train Loss: 0.12920456 \t Avg Val Loss: 0.12572364 \t Avg Test Loss: 0.11678488\n",
      "Test fold 3 \t Epoch [180] \t Avg Train Loss: 0.12998706 \t Avg Val Loss: 0.14706463 \t Avg Test Loss: 0.13031266\n",
      "Test fold 3 \t Epoch [200] \t Avg Train Loss: 0.12888916 \t Avg Val Loss: 0.13228074 \t Avg Test Loss: 0.12141064\n",
      "Test fold 3 \t Epoch [220] \t Avg Train Loss: 0.11714902 \t Avg Val Loss: 0.15189501 \t Avg Test Loss: 0.14303129\n",
      "Test fold 3 \t Epoch [240] \t Avg Train Loss: 0.11986636 \t Avg Val Loss: 0.12584403 \t Avg Test Loss: 0.14281762\n",
      "Test fold 3 \t Epoch [260] \t Avg Train Loss: 0.11074327 \t Avg Val Loss: 0.12261221 \t Avg Test Loss: 0.12466565\n",
      "Test fold 3 \t Epoch [280] \t Avg Train Loss: 0.10832784 \t Avg Val Loss: 0.13437812 \t Avg Test Loss: 0.13220117\n",
      "Test fold 3 \t Epoch [300] \t Avg Train Loss: 0.11171575 \t Avg Val Loss: 0.15110368 \t Avg Test Loss: 0.13920733\n",
      "Test fold 3 \t Early stopping at Epoch [307]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.11786298 \t Best Test Loss: 0.14433478\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.27876798 \t Avg Val Loss: 0.23857676 \t Avg Test Loss: 0.27132578\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.24005641 \t Avg Val Loss: 0.19174752 \t Avg Test Loss: 0.25450416\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.25832029 \t Avg Val Loss: 0.20873276 \t Avg Test Loss: 0.25295831\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.16096205 \t Avg Val Loss: 0.10986318 \t Avg Test Loss: 0.17750578\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.14744588 \t Avg Val Loss: 0.10538122 \t Avg Test Loss: 0.15940304\n",
      "Test fold 4 \t Epoch [100] \t Avg Train Loss: 0.13623769 \t Avg Val Loss: 0.10352830 \t Avg Test Loss: 0.13727778\n",
      "Test fold 4 \t Epoch [120] \t Avg Train Loss: 0.21255675 \t Avg Val Loss: 0.20965262 \t Avg Test Loss: 0.23393899\n",
      "Test fold 4 \t Early stopping at Epoch [125]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.09454144 \t Best Test Loss: 0.14293691\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.31006415 \t Avg Val Loss: 0.22393075 \t Avg Test Loss: 0.32142693\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.25578235 \t Avg Val Loss: 0.22509593 \t Avg Test Loss: 0.32421758\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.25406154 \t Avg Val Loss: 0.23348965 \t Avg Test Loss: 0.31418817\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.22557505 \t Avg Val Loss: 0.23311296 \t Avg Test Loss: 0.29271718\n",
      "Test fold 5 \t Epoch [ 80] \t Avg Train Loss: 0.13249637 \t Avg Val Loss: 0.15044850 \t Avg Test Loss: 0.15745686\n",
      "Test fold 5 \t Epoch [100] \t Avg Train Loss: 0.11626203 \t Avg Val Loss: 0.15884694 \t Avg Test Loss: 0.16227237\n",
      "Test fold 5 \t Epoch [120] \t Avg Train Loss: 0.10784920 \t Avg Val Loss: 0.12738638 \t Avg Test Loss: 0.16516820\n",
      "Test fold 5 \t Epoch [140] \t Avg Train Loss: 0.10741832 \t Avg Val Loss: 0.15844581 \t Avg Test Loss: 0.18401875\n",
      "Test fold 5 \t Epoch [160] \t Avg Train Loss: 0.10435341 \t Avg Val Loss: 0.14528970 \t Avg Test Loss: 0.14129278\n",
      "Test fold 5 \t Early stopping at Epoch [173]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.12485903 \t Best Test Loss: 0.20017238\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.26635919 \t Avg Val Loss: 0.25241773 \t Avg Test Loss: 0.32420283\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.25340555 \t Avg Val Loss: 0.25450426 \t Avg Test Loss: 0.32448304\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.25243096 \t Avg Val Loss: 0.25524979 \t Avg Test Loss: 0.32460657\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.13752397 \t Avg Val Loss: 0.13811437 \t Avg Test Loss: 0.17954413\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.12791177 \t Avg Val Loss: 0.15313157 \t Avg Test Loss: 0.18506730\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.12168761 \t Avg Val Loss: 0.13760962 \t Avg Test Loss: 0.15969994\n",
      "Test fold 6 \t Epoch [120] \t Avg Train Loss: 0.11836888 \t Avg Val Loss: 0.12498367 \t Avg Test Loss: 0.15987437\n",
      "Test fold 6 \t Epoch [140] \t Avg Train Loss: 0.11390453 \t Avg Val Loss: 0.12067193 \t Avg Test Loss: 0.15708818\n",
      "Test fold 6 \t Epoch [160] \t Avg Train Loss: 0.10882069 \t Avg Val Loss: 0.11805499 \t Avg Test Loss: 0.15629653\n",
      "Test fold 6 \t Epoch [180] \t Avg Train Loss: 0.10455285 \t Avg Val Loss: 0.11666390 \t Avg Test Loss: 0.15515967\n",
      "Test fold 6 \t Epoch [200] \t Avg Train Loss: 0.10081137 \t Avg Val Loss: 0.13184861 \t Avg Test Loss: 0.14906409\n",
      "Test fold 6 \t Epoch [220] \t Avg Train Loss: 0.10110019 \t Avg Val Loss: 0.11191884 \t Avg Test Loss: 0.15539015\n",
      "Test fold 6 \t Epoch [240] \t Avg Train Loss: 0.09829136 \t Avg Val Loss: 0.11858875 \t Avg Test Loss: 0.14337532\n",
      "Test fold 6 \t Epoch [260] \t Avg Train Loss: 0.09747399 \t Avg Val Loss: 0.10867404 \t Avg Test Loss: 0.15112491\n",
      "Test fold 6 \t Epoch [280] \t Avg Train Loss: 0.09967408 \t Avg Val Loss: 0.13292718 \t Avg Test Loss: 0.15135861\n",
      "Test fold 6 \t Epoch [300] \t Avg Train Loss: 0.09276238 \t Avg Val Loss: 0.11313430 \t Avg Test Loss: 0.14958544\n",
      "Test fold 6 \t Epoch [320] \t Avg Train Loss: 0.09126704 \t Avg Val Loss: 0.10948373 \t Avg Test Loss: 0.14727679\n",
      "Test fold 6 \t Early stopping at Epoch [332]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.09865311 \t Best Test Loss: 0.16964450\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.28691496 \t Avg Val Loss: 0.29403733 \t Avg Test Loss: 0.26382534\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.14914991 \t Avg Val Loss: 0.19637843 \t Avg Test Loss: 0.13012597\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.11754348 \t Avg Val Loss: 0.16071847 \t Avg Test Loss: 0.12830795\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.10843790 \t Avg Val Loss: 0.16890486 \t Avg Test Loss: 0.11840202\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.10171157 \t Avg Val Loss: 0.16155015 \t Avg Test Loss: 0.11263194\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.08941826 \t Avg Val Loss: 0.16608411 \t Avg Test Loss: 0.11979951\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.08666779 \t Avg Val Loss: 0.16223895 \t Avg Test Loss: 0.12923708\n",
      "Test fold 1 \t Early stopping at Epoch [121]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.14836328 \t Best Test Loss: 0.11139819\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.27984244 \t Avg Val Loss: 0.31334161 \t Avg Test Loss: 0.22581974\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.19590131 \t Avg Val Loss: 0.20355829 \t Avg Test Loss: 0.17848879\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.12304563 \t Avg Val Loss: 0.13866022 \t Avg Test Loss: 0.13766942\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.12148035 \t Avg Val Loss: 0.13180752 \t Avg Test Loss: 0.14966520\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.10459311 \t Avg Val Loss: 0.12333045 \t Avg Test Loss: 0.14961813\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.10420527 \t Avg Val Loss: 0.11439394 \t Avg Test Loss: 0.16970949\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.10777810 \t Avg Val Loss: 0.13626295 \t Avg Test Loss: 0.15844407\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.10257060 \t Avg Val Loss: 0.12079577 \t Avg Test Loss: 0.15302609\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.09551088 \t Avg Val Loss: 0.12689099 \t Avg Test Loss: 0.14389826\n",
      "Test fold 2 \t Epoch [180] \t Avg Train Loss: 0.08862637 \t Avg Val Loss: 0.13502037 \t Avg Test Loss: 0.17346069\n",
      "Test fold 2 \t Early stopping at Epoch [188]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.11090686 \t Best Test Loss: 0.15605463\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.30244420 \t Avg Val Loss: 0.28368001 \t Avg Test Loss: 0.18015985\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.27565680 \t Avg Val Loss: 0.28743682 \t Avg Test Loss: 0.18687868\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.26007273 \t Avg Val Loss: 0.29731517 \t Avg Test Loss: 0.18457149\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.13414208 \t Avg Val Loss: 0.12046626 \t Avg Test Loss: 0.10353177\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.11892686 \t Avg Val Loss: 0.13382421 \t Avg Test Loss: 0.12914965\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.11709408 \t Avg Val Loss: 0.12118134 \t Avg Test Loss: 0.11880543\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.10726444 \t Avg Val Loss: 0.11764889 \t Avg Test Loss: 0.12026019\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.10733899 \t Avg Val Loss: 0.11128004 \t Avg Test Loss: 0.10714213\n",
      "Test fold 3 \t Epoch [160] \t Avg Train Loss: 0.09592385 \t Avg Val Loss: 0.11563627 \t Avg Test Loss: 0.11939473\n",
      "Test fold 3 \t Early stopping at Epoch [164]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.10084460 \t Best Test Loss: 0.10963644\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.29057765 \t Avg Val Loss: 0.23993154 \t Avg Test Loss: 0.27924994\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.20488340 \t Avg Val Loss: 0.15402526 \t Avg Test Loss: 0.19975229\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.13865818 \t Avg Val Loss: 0.11500570 \t Avg Test Loss: 0.14690510\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.11794193 \t Avg Val Loss: 0.12330723 \t Avg Test Loss: 0.12840625\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.10155374 \t Avg Val Loss: 0.12310849 \t Avg Test Loss: 0.14533357\n",
      "Test fold 4 \t Early stopping at Epoch [87]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.09936571 \t Best Test Loss: 0.13190230\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.27874071 \t Avg Val Loss: 0.22647848 \t Avg Test Loss: 0.32310082\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.26264666 \t Avg Val Loss: 0.22301217 \t Avg Test Loss: 0.32208588\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.26009975 \t Avg Val Loss: 0.22193504 \t Avg Test Loss: 0.31803706\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.14216064 \t Avg Val Loss: 0.17069478 \t Avg Test Loss: 0.19162069\n",
      "Test fold 5 \t Epoch [ 80] \t Avg Train Loss: 0.12695072 \t Avg Val Loss: 0.14372714 \t Avg Test Loss: 0.12555781\n",
      "Test fold 5 \t Epoch [100] \t Avg Train Loss: 0.10406344 \t Avg Val Loss: 0.14307859 \t Avg Test Loss: 0.13895977\n",
      "Test fold 5 \t Epoch [120] \t Avg Train Loss: 0.07233411 \t Avg Val Loss: 0.10694821 \t Avg Test Loss: 0.07883745\n",
      "Test fold 5 \t Epoch [140] \t Avg Train Loss: 0.06466849 \t Avg Val Loss: 0.09701059 \t Avg Test Loss: 0.08291474\n",
      "Test fold 5 \t Epoch [160] \t Avg Train Loss: 0.11936165 \t Avg Val Loss: 0.12356709 \t Avg Test Loss: 0.14003715\n",
      "NaN loss detected at Epoch [180], Step [1743]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.09342220 \t Best Test Loss: 0.10298063\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.27348215 \t Avg Val Loss: 0.26672602 \t Avg Test Loss: 0.32903845\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.24015968 \t Avg Val Loss: 0.24172106 \t Avg Test Loss: 0.31971103\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.21929977 \t Avg Val Loss: 0.21157431 \t Avg Test Loss: 0.28138713\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.13569836 \t Avg Val Loss: 0.14051298 \t Avg Test Loss: 0.18491861\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.12121318 \t Avg Val Loss: 0.15982746 \t Avg Test Loss: 0.13649690\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.11737524 \t Avg Val Loss: 0.16534835 \t Avg Test Loss: 0.19710961\n",
      "Test fold 6 \t Epoch [120] \t Avg Train Loss: 0.10501169 \t Avg Val Loss: 0.13552773 \t Avg Test Loss: 0.18933243\n",
      "Test fold 6 \t Epoch [140] \t Avg Train Loss: 0.10382772 \t Avg Val Loss: 0.11560827 \t Avg Test Loss: 0.14012082\n",
      "Test fold 6 \t Epoch [160] \t Avg Train Loss: 0.13808593 \t Avg Val Loss: 0.12925542 \t Avg Test Loss: 0.22961753\n",
      "Test fold 6 \t Epoch [180] \t Avg Train Loss: 0.12145632 \t Avg Val Loss: 0.18206505 \t Avg Test Loss: 0.22261725\n",
      "Test fold 6 \t Epoch [200] \t Avg Train Loss: 0.08966900 \t Avg Val Loss: 0.10162839 \t Avg Test Loss: 0.14375021\n",
      "Test fold 6 \t Epoch [220] \t Avg Train Loss: 0.08417628 \t Avg Val Loss: 0.11699763 \t Avg Test Loss: 0.17869550\n",
      "Test fold 6 \t Epoch [240] \t Avg Train Loss: 0.14394415 \t Avg Val Loss: 0.17567205 \t Avg Test Loss: 0.24311715\n",
      "Test fold 6 \t Early stopping at Epoch [240]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.09836998 \t Best Test Loss: 0.13362549\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.27928360 \t Avg Val Loss: 0.29513642 \t Avg Test Loss: 0.26793211\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.14798403 \t Avg Val Loss: 0.19766313 \t Avg Test Loss: 0.12731527\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.10615547 \t Avg Val Loss: 0.19168029 \t Avg Test Loss: 0.11647198\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.11564013 \t Avg Val Loss: 0.26818746 \t Avg Test Loss: 0.19357793\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.08489957 \t Avg Val Loss: 0.21275014 \t Avg Test Loss: 0.12879421\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.07804414 \t Avg Val Loss: 0.22097760 \t Avg Test Loss: 0.13567652\n",
      "Test fold 1 \t Early stopping at Epoch [114]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.15834764 \t Best Test Loss: 0.14693343\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.28680272 \t Avg Val Loss: 0.31799147 \t Avg Test Loss: 0.22921135\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.22746665 \t Avg Val Loss: 0.22586497 \t Avg Test Loss: 0.20115697\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.11793882 \t Avg Val Loss: 0.16885898 \t Avg Test Loss: 0.16758442\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.10236325 \t Avg Val Loss: 0.12118725 \t Avg Test Loss: 0.17927116\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.12419981 \t Avg Val Loss: 0.13359593 \t Avg Test Loss: 0.16845489\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.08908548 \t Avg Val Loss: 0.14547292 \t Avg Test Loss: 0.18206300\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.08505057 \t Avg Val Loss: 0.12609962 \t Avg Test Loss: 0.14992799\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.10916403 \t Avg Val Loss: 0.12588599 \t Avg Test Loss: 0.18123076\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.23816625 \t Avg Val Loss: 0.25408309 \t Avg Test Loss: 0.22074919\n",
      "Test fold 2 \t Early stopping at Epoch [165]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.10442808 \t Best Test Loss: 0.14651933\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.29676451 \t Avg Val Loss: 0.31093504 \t Avg Test Loss: 0.18762766\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.24196110 \t Avg Val Loss: 0.20736760 \t Avg Test Loss: 0.14389490\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.12565504 \t Avg Val Loss: 0.11629086 \t Avg Test Loss: 0.10829654\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.12838337 \t Avg Val Loss: 0.13314607 \t Avg Test Loss: 0.12787111\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.09842362 \t Avg Val Loss: 0.11374755 \t Avg Test Loss: 0.10714747\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.08994028 \t Avg Val Loss: 0.11538440 \t Avg Test Loss: 0.10310486\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.09216285 \t Avg Val Loss: 0.14778086 \t Avg Test Loss: 0.14308604\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.08561172 \t Avg Val Loss: 0.12303903 \t Avg Test Loss: 0.10453126\n",
      "Test fold 3 \t Epoch [160] \t Avg Train Loss: 0.08926490 \t Avg Val Loss: 0.11894276 \t Avg Test Loss: 0.12196382\n",
      "Test fold 3 \t Early stopping at Epoch [173]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.10361669 \t Best Test Loss: 0.11460947\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.27764470 \t Avg Val Loss: 0.23884989 \t Avg Test Loss: 0.27633827\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.19587370 \t Avg Val Loss: 0.14419812 \t Avg Test Loss: 0.17736518\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.12451745 \t Avg Val Loss: 0.11181578 \t Avg Test Loss: 0.18107976\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.10722875 \t Avg Val Loss: 0.11738186 \t Avg Test Loss: 0.13415926\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.09179564 \t Avg Val Loss: 0.13850852 \t Avg Test Loss: 0.18377615\n",
      "Test fold 4 \t Epoch [100] \t Avg Train Loss: 0.17785748 \t Avg Val Loss: 0.12634799 \t Avg Test Loss: 0.19635093\n",
      "Test fold 4 \t Epoch [120] \t Avg Train Loss: 0.11722963 \t Avg Val Loss: 0.09144487 \t Avg Test Loss: 0.13686065\n",
      "Test fold 4 \t Epoch [140] \t Avg Train Loss: 0.15570954 \t Avg Val Loss: 0.12607313 \t Avg Test Loss: 0.16441190\n",
      "Test fold 4 \t Epoch [160] \t Avg Train Loss: 0.08751770 \t Avg Val Loss: 0.11891451 \t Avg Test Loss: 0.12935043\n",
      "Test fold 4 \t Early stopping at Epoch [171]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.09144487 \t Best Test Loss: 0.13686065\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.27056642 \t Avg Val Loss: 0.22488828 \t Avg Test Loss: 0.32556885\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.25905209 \t Avg Val Loss: 0.22639412 \t Avg Test Loss: 0.32233474\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.17469657 \t Avg Val Loss: 0.21081393 \t Avg Test Loss: 0.16947745\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.11958862 \t Avg Val Loss: 0.52161288 \t Avg Test Loss: 0.33752792\n",
      "Test fold 5 \t Epoch [ 80] \t Avg Train Loss: 0.10879639 \t Avg Val Loss: 0.11160131 \t Avg Test Loss: 0.11841169\n",
      "Test fold 5 \t Epoch [100] \t Avg Train Loss: 0.09146414 \t Avg Val Loss: 0.12795801 \t Avg Test Loss: 0.13133800\n",
      "Test fold 5 \t Epoch [120] \t Avg Train Loss: 0.11077876 \t Avg Val Loss: 0.14425790 \t Avg Test Loss: 0.13423798\n",
      "Test fold 5 \t Epoch [140] \t Avg Train Loss: 0.15710757 \t Avg Val Loss: 0.18063758 \t Avg Test Loss: 0.19261469\n",
      "Test fold 5 \t Epoch [160] \t Avg Train Loss: 0.16770305 \t Avg Val Loss: 0.16088382 \t Avg Test Loss: 0.16915014\n",
      "Test fold 5 \t Early stopping at Epoch [178]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.10628548 \t Best Test Loss: 0.12250690\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.26317997 \t Avg Val Loss: 0.26246123 \t Avg Test Loss: 0.32702726\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.25479738 \t Avg Val Loss: 0.26245089 \t Avg Test Loss: 0.32658882\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.13563774 \t Avg Val Loss: 0.13539778 \t Avg Test Loss: 0.19270497\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.11442906 \t Avg Val Loss: 0.10372595 \t Avg Test Loss: 0.16886600\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.09468882 \t Avg Val Loss: 0.11569173 \t Avg Test Loss: 0.19692251\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.14290168 \t Avg Val Loss: 0.16028099 \t Avg Test Loss: 0.18737241\n",
      "Test fold 6 \t Epoch [120] \t Avg Train Loss: 0.11068200 \t Avg Val Loss: 0.12220206 \t Avg Test Loss: 0.16855092\n",
      "Test fold 6 \t Early stopping at Epoch [123]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.08893746 \t Best Test Loss: 0.18698244\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.30850392 \t Avg Val Loss: 0.30527811 \t Avg Test Loss: 0.26999119\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.19349285 \t Avg Val Loss: 0.21745397 \t Avg Test Loss: 0.20875767\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.18014006 \t Avg Val Loss: 0.22110852 \t Avg Test Loss: 0.21920554\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.13234794 \t Avg Val Loss: 0.18284273 \t Avg Test Loss: 0.15045443\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.12900405 \t Avg Val Loss: 0.21198030 \t Avg Test Loss: 0.13354738\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.13515208 \t Avg Val Loss: 0.17981892 \t Avg Test Loss: 0.13743042\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.12237744 \t Avg Val Loss: 0.16166362 \t Avg Test Loss: 0.14015616\n",
      "Test fold 1 \t Epoch [140] \t Avg Train Loss: 0.12673224 \t Avg Val Loss: 0.16722149 \t Avg Test Loss: 0.14173750\n",
      "Test fold 1 \t Epoch [160] \t Avg Train Loss: 0.12025811 \t Avg Val Loss: 0.16784640 \t Avg Test Loss: 0.13037605\n",
      "Test fold 1 \t Early stopping at Epoch [176]\n",
      "Test fold 1 \t Training completed for GRU layers 2 \t Hidden size 2 \t Best Val Loss: 0.15484119 \t Best Test Loss: 0.15436772\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.26787455 \t Avg Val Loss: 0.31680995 \t Avg Test Loss: 0.22772613\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.24316561 \t Avg Val Loss: 0.27980703 \t Avg Test Loss: 0.20673172\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.18304100 \t Avg Val Loss: 0.19768372 \t Avg Test Loss: 0.15772557\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.15715665 \t Avg Val Loss: 0.22047826 \t Avg Test Loss: 0.17094019\n"
     ]
    }
   ],
   "source": [
    "for num_layers in [1, 2]:\n",
    "    for hidden_size in [2, 4, 8, 16]:\n",
    "        for test_fold in np.unique(folds_df['fold']):\n",
    "            # Record start time\n",
    "            fold_start_time = time.time()\n",
    "\n",
    "            # Split data into training and test sets based on fold\n",
    "            train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "            test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "            # Prepare train and test sequences as tensors\n",
    "            train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "            test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "            # Prepare target values for training and testing\n",
    "            target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "            y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "            target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "            y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "            # Split training data into subtrain and validation sets\n",
    "            train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Initialize the GRU model, loss function, and optimizer\n",
    "            model = LSTMModel(input_size, hidden_size, num_layers).to(device)    # Move model to device\n",
    "            criterion = SquaredHingeLoss().to(device)                           # Move loss function to device\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            # Variables for early stopping\n",
    "            best_train_loss = float('inf')    # Best training loss initialized to infinity\n",
    "            best_val_loss = float('inf')      # Best validation loss initialized to infinity\n",
    "            best_test_loss = float('inf')     # Best test loss corresponding to best validation\n",
    "            patience_counter = 0              # Early stopping patience counter\n",
    "            best_model_state = None           # Store the best model parameters\n",
    "            stop_epoch = 0                    # Epoch when training stops\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(max_epochs):\n",
    "                # Shuffle training sequences and targets\n",
    "                combined = list(zip(train_seqs, y_train))\n",
    "                random.shuffle(combined)\n",
    "                train_seqs, y_train = zip(*combined)\n",
    "\n",
    "                total_train_loss = 0\n",
    "                nan_flag = False  # Flag to detect NaN loss\n",
    "\n",
    "                # Train on subtrain data\n",
    "                model.train()  # Set model to training mode\n",
    "                for i, seq_input in enumerate(train_seqs):\n",
    "                    target = y_train[i].unsqueeze(0).to(device)  # Prepare target and move to device\n",
    "\n",
    "                    optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "                    # Forward pass\n",
    "                    seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "                    output_seq = model(seq_input)                               # Get model output\n",
    "                    loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "\n",
    "                    if torch.isnan(loss).any():  # Check for NaN loss\n",
    "                        print(f\"NaN loss detected at Epoch [{epoch}], Step [{i}]\")\n",
    "                        nan_flag = True\n",
    "                        break\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "                if nan_flag:\n",
    "                    break  # Stop training if NaN was encountered\n",
    "\n",
    "                # Calculate average training loss\n",
    "                avg_train_loss = total_train_loss / len(train_seqs)\n",
    "\n",
    "                # Calculate validation and test losses\n",
    "                avg_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "                avg_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {avg_val_loss:.8f} \\t Avg Test Loss: {avg_test_loss:.8f}')\n",
    "\n",
    "                # Early stopping based on validation loss\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss        # Update best validation loss\n",
    "                    best_train_loss = avg_train_loss    # Store best training loss\n",
    "                    best_test_loss = avg_test_loss      # Store test loss for best validation\n",
    "                    patience_counter = 0                # Reset patience counter\n",
    "\n",
    "                    # Save best model parameters\n",
    "                    best_model_state = model.state_dict()\n",
    "                    stop_epoch = epoch + 1              # Record stopping epoch\n",
    "                else:\n",
    "                    patience_counter += 1               # Increment patience counter\n",
    "\n",
    "                # Stop training if patience is exceeded\n",
    "                if patience_counter > patience:\n",
    "                    print(f\"Test fold {test_fold} \\t Early stopping at Epoch [{epoch}]\")\n",
    "                    break\n",
    "\n",
    "            # Record total time taken for this fold\n",
    "            fold_duration = time.time() - fold_start_time\n",
    "\n",
    "            # Save results to CSV\n",
    "            report_entry = {\n",
    "                'dataset': dataset,\n",
    "                'model': model_type,\n",
    "                'num_layers': num_layers,\n",
    "                'hidden_size': hidden_size,\n",
    "                'test_fold': test_fold,\n",
    "                'stop_epoch': stop_epoch,\n",
    "                'train_loss': best_train_loss,\n",
    "                'val_loss': best_val_loss,\n",
    "                'test_loss': best_test_loss,\n",
    "                'time': fold_duration\n",
    "            }\n",
    "\n",
    "            pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)  # Append entry to CSV\n",
    "\n",
    "            print(f\"Test fold {test_fold} \\t Training completed for GRU layers {num_layers} \\t Hidden size {hidden_size} \\t Best Val Loss: {best_val_loss:.8f} \\t Best Test Loss: {best_test_loss:.8f}\")\n",
    "            \n",
    "            # Restore best model parameters after training\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "            # Test the model and collect outputs\n",
    "            pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "            # Save model parameters\n",
    "            torch.save(model.state_dict(), f'saved_models/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.pth')\n",
    "\n",
    "            # Save predictions to CSV\n",
    "            lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "            lldas_df.to_csv(f'predictions/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
