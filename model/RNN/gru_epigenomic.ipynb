{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import lzma\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split\n","import time\n","import os\n","import gzip"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["random_seed = 42\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","np.random.seed(random_seed)\n","random.seed(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_type = 'gru'\n","input_size = 1"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["patience = 50\n","max_epochs = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"using {device}\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class SquaredHingeLoss(nn.Module):\n","    def __init__(self, margin=1):\n","        super(SquaredHingeLoss, self).__init__()\n","        self.margin = margin\n","    def forward(self, predicted, y):\n","        low, high = y[:, 0:1], y[:, 1:2]\n","        loss_low = torch.relu(low - predicted + self.margin)\n","        loss_high = torch.relu(predicted - high + self.margin)\n","        loss = loss_low + loss_high\n","        return torch.mean(torch.square(loss))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)    # GRU\n","        self.fc = nn.Linear(hidden_size, 1)                                         # Linear\n","    def forward(self, x):               \n","        gru_out, _ = self.gru(x)                            # Pass sequence through GRU    \n","        last_out = gru_out[:, -1, :]                        # Take the hidden state of the last time step \n","        x = self.fc(last_out)                               # Linear combination         \n","        x = torch.relu(x + 10) - torch.relu(x - 10) - 10    # clamp between -10 and 10\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_model(model, inputs):\n","    model.eval()                                                        # Set model to evaluation mode\n","    predictions = []\n","    with torch.no_grad():                                               # Disable gradient calculation\n","        for seq_input in inputs:\n","            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Add batch dimension and move to device\n","            output_seq = model(seq_input)                               # Get model output\n","            predictions.append(output_seq.item())                       # Store the prediction\n","    return predictions"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["An error occurred: Unable to allocate 1.00 MiB for an array with shape (131072,) and data type int64\n","An error occurred: Unable to allocate output buffer.\n","An error occurred: Unable to allocate output buffer.\n","An error occurred: Unable to allocate output buffer.\n","An error occurred: \n","An error occurred: Unable to allocate output buffer.\n","An error occurred: Unable to allocate output buffer.\n","An error occurred: Unable to allocate output buffer.\n","An error occurred: \n","An error occurred: \n","An error occurred: cannot allocate memory for array\n","An error occurred: Unable to allocate 22.4 MiB for an array with shape (2937376,) and data type int64\n","An error occurred: \n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["def list_files(folder_path):\n","    return os.listdir(folder_path)\n","\n","folder_path = \"C:/Users/nguye/Downloads/data/\"\n","\n","result_list = []\n","for data in list_files(folder_path):\n","    for sample in list_files(os.path.join(folder_path, data, \"samples\")):\n","        for problem in list_files(os.path.join(folder_path, data, \"samples\", sample)):\n","            for chr in list_files(os.path.join(folder_path, data, \"samples\", sample, problem, \"problems\")):\n","                sequenceID = os.path.join(data, \"samples\", sample, problem, \"problems\", chr.replace('_', ':'))\n","                try:\n","                    sequence_path = os.path.join(folder_path, data, \"samples\", sample, problem, \"problems\", chr, \"coverage.bedGraph.gz\")\n","                    extracted_file = sequence_path.replace('.gz', '')\n","\n","                    # Open the compressed file and read the content\n","                    with gzip.open(sequence_path, 'rb') as f_in:\n","                        with open(extracted_file, 'wb') as f_out:\n","                            f_out.write(f_in.read())\n","\n","                    # Load the data into a Pandas DataFrame\n","                    signal_df = pd.read_csv(extracted_file, sep='\\t', header=None, names=['value'])\n","\n","                    # Add the (sequenceID, signal_df) to the result list\n","                    result_list.append((sequenceID, signal_df))\n","                except Exception as e:\n","                    print(\"An error occurred:\", e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(result_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_loss_value(model, test_seqs, y_test, criterion):\n","    total_test_loss = 0\n","    with torch.no_grad():                                               # Disable gradient calculation\n","        for i, seq_input in enumerate(test_seqs):\n","            target = y_test[i].unsqueeze(0).to(device)                  # Move target to device\n","            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n","            output_seq = model(seq_input)                               # Get model output\n","            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n","            total_test_loss += loss.item()                              # Accumulate loss\n","    avg_test_loss = total_test_loss / len(test_seqs)                    # Calculate average loss\n","    return avg_test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n","with lzma.open(file_path, 'rt') as file:\n","    signal_df = pd.read_csv(file)\n","\n","seqs = tuple(signal_df.groupby('sequenceID'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n","target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["report_path = f'report_{dataset}_{model_type}.csv'\n","report_header = ['dataset', 'model', 'num_layers', 'hidden_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n","if not os.path.exists(report_path):\n","    pd.DataFrame(columns=report_header).to_csv(report_path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["configurations = {\n","    f'num_layers_{num_layers}_hidden_size_{hidden_size}_test_fold_{test_fold}': {\n","        'num_layers': num_layers,\n","        'hidden_size': hidden_size,\n","        'test_fold': test_fold\n","    }\n","    for num_layers in [1, 2]\n","    for hidden_size in [2, 4, 8, 16]\n","    for test_fold in range(1, 7)\n","}\n","config_list = list(configurations.values())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for config in config_list:\n","    num_layers = config['num_layers']\n","    hidden_size = config['hidden_size']\n","    test_fold = config['test_fold']\n","            \n","    # Record start time\n","    fold_start_time = time.time()\n","\n","    # Split data into training and test sets based on fold\n","    train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n","    test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n","\n","    # Prepare train and test sequences as tensors\n","    train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n","    test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n","\n","    # Prepare target values for training and testing\n","    target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n","    y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n","    target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n","    y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n","\n","    # Split training data into subtrain and validation sets\n","    train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n","\n","    # Initialize the GRU model, loss function, and optimizer\n","    model = GRUModel(input_size, hidden_size, num_layers).to(device)    # Move model to device\n","    criterion = SquaredHingeLoss().to(device)                           # Move loss function to device\n","    optimizer = torch.optim.Adam(model.parameters())\n","\n","    # Variables for early stopping\n","    best_train_loss = float('inf')    # Best training loss initialized to infinity\n","    best_val_loss = float('inf')      # Best validation loss initialized to infinity\n","    best_test_loss = float('inf')     # Best test loss corresponding to best validation\n","    patience_counter = 0              # Early stopping patience counter\n","    best_model_state = None           # Store the best model parameters\n","    stop_epoch = 0                    # Epoch when training stops\n","\n","    # Training loop\n","    for epoch in range(max_epochs):\n","        # Shuffle training sequences and targets\n","        combined = list(zip(train_seqs, y_train))\n","        random.shuffle(combined)\n","        train_seqs, y_train = zip(*combined)\n","        total_train_loss = 0\n","        nan_flag = False  # Flag to detect NaN loss\n","\n","        # Train on subtrain data\n","        model.train()  # Set model to training mode\n","        for i, seq_input in enumerate(train_seqs):\n","            target = y_train[i].unsqueeze(0).to(device)  # Prepare target and move to device\n","            optimizer.zero_grad()  # Zero gradients\n","\n","            # Forward pass\n","            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n","            output_seq = model(seq_input)                               # Get model output\n","            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n","            if torch.isnan(loss).any():  # Check for NaN loss\n","                print(f\"NaN loss detected at Epoch [{epoch}], Step [{i}]\")\n","                nan_flag = True\n","                break\n","\n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()  # Accumulate training loss\n","        if nan_flag:\n","            break  # Stop training if NaN was encountered\n","\n","        # Calculate average training loss\n","        avg_train_loss = total_train_loss / len(train_seqs)\n","\n","        # Calculate validation and test losses\n","        avg_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n","        avg_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n","        if epoch % 20 == 0:\n","            print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {avg_val_loss:.8f} \\t Avg Test Loss: {avg_test_loss:.8f}')\n","\n","        # Early stopping based on validation loss\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss        # Update best validation loss\n","            best_train_loss = avg_train_loss    # Store best training loss\n","            best_test_loss = avg_test_loss      # Store test loss for best validation\n","            patience_counter = 0                # Reset patience counter\n","\n","            # Save best model parameters\n","            best_model_state = model.state_dict()\n","            stop_epoch = epoch + 1              # Record stopping epoch\n","        else:\n","            patience_counter += 1               # Increment patience counter\n","\n","        # Stop training if patience is exceeded\n","        if patience_counter > patience:\n","            print(f\"Test fold {test_fold} \\t Early stopping at Epoch [{epoch}]\")\n","            break\n","\n","    # Record total time taken for this fold\n","    fold_duration = time.time() - fold_start_time\n","\n","    # Save results to CSV\n","    report_entry = {\n","        'dataset': dataset,\n","        'model': model_type,\n","        'num_layers': num_layers,\n","        'hidden_size': hidden_size,\n","        'test_fold': test_fold,\n","        'stop_epoch': stop_epoch,\n","        'train_loss': best_train_loss,\n","        'val_loss': best_val_loss,\n","        'test_loss': best_test_loss,\n","        'time': fold_duration\n","    }\n","    pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)  # Append entry to CSV\n","    print(f\"Test fold {test_fold} \\t Training completed for GRU layers {num_layers} \\t Hidden size {hidden_size} \\t Best Val Loss: {best_val_loss:.8f} \\t Best Test Loss: {best_test_loss:.8f}\")\n","    \n","    # Restore best model parameters after training\n","    if best_model_state is not None:\n","        model.load_state_dict(best_model_state)\n","        model.eval()  # Set the model to evaluation mode\n","\n","    # Test the model and collect outputs\n","    pred_lldas = test_model(model, test_seqs)\n","\n","    # Save model parameters\n","    torch.save(model.state_dict(), f'saved_models/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.pth')\n","\n","    # Save predictions to CSV\n","    lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n","    lldas_df.to_csv(f'predictions/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
