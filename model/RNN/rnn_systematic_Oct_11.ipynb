{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset = 'systematic'\n",
    "\n",
    "# Model parameters\n",
    "model_type = 'rnn'\n",
    "input_size = 1\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 50\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='relu')    # RNN\n",
    "        self.fc = nn.Linear(hidden_size, 1)                                         # Linear\n",
    "\n",
    "    def forward(self, x):               \n",
    "        rnn_out, _ = self.rnn(x)                            # Pass sequence through RNN    \n",
    "        last_out = rnn_out[:, -1, :]                        # Take the hidden state of the last time step\n",
    "        x = self.fc(last_out)                               # Linear combination         \n",
    "        x = torch.relu(x + 10) - torch.relu(x - 10) - 10    # Clamp between -10 and 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()                                                        # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Add batch dimension and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            predictions.append(output_seq.item())                       # Store the prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss value\n",
    "def get_loss_value(model, test_seqs, y_test, criterion):\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for i, seq_input in enumerate(test_seqs):\n",
    "            target = y_test[i].unsqueeze(0).to(device)                  # Move target to device\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "            total_test_loss += loss.item()                              # Accumulate loss\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_seqs)                    # Calculate average loss\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence data from CSV\n",
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "# Group sequences by 'sequenceID'\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))\n",
    "\n",
    "# Load fold and target data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CSV file for logging\n",
    "report_path = f'report_{dataset}_{model_type}.csv'\n",
    "report_header = ['dataset', 'model', 'num_layers', 'hidden_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n",
    "if not os.path.exists(report_path):\n",
    "    pd.DataFrame(columns=report_header).to_csv(report_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.25778225 \t Avg Val Loss: 0.29258313 \t Avg Test Loss: 0.26229704\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.24511132 \t Avg Val Loss: 0.29058539 \t Avg Test Loss: 0.24707640\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.24181142 \t Avg Val Loss: 0.28029427 \t Avg Test Loss: 0.24816381\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.25177272 \t Avg Val Loss: 0.30750496 \t Avg Test Loss: 0.25418318\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.23746198 \t Avg Val Loss: 0.26854063 \t Avg Test Loss: 0.23428735\n",
      "Test fold 1 \t Early stopping at Epoch [98]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.26450116 \t Best Test Loss: 0.23643785\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.31106042 \t Avg Val Loss: 0.30893266 \t Avg Test Loss: 0.22557565\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.26371945 \t Avg Val Loss: 0.30836454 \t Avg Test Loss: 0.22573067\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.26332708 \t Avg Val Loss: 0.31006662 \t Avg Test Loss: 0.22734419\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.26158643 \t Avg Val Loss: 0.30386156 \t Avg Test Loss: 0.22827415\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.25920272 \t Avg Val Loss: 0.30949363 \t Avg Test Loss: 0.22997370\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.25931981 \t Avg Val Loss: 0.31603218 \t Avg Test Loss: 0.23355228\n",
      "Test fold 2 \t Early stopping at Epoch [117]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.30231077 \t Best Test Loss: 0.22920327\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.29695832 \t Avg Val Loss: 0.28721242 \t Avg Test Loss: 0.18011461\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.28017310 \t Avg Val Loss: 0.30117140 \t Avg Test Loss: 0.22540558\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.26946742 \t Avg Val Loss: 0.28621537 \t Avg Test Loss: 0.18128509\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.25966670 \t Avg Val Loss: 0.29273434 \t Avg Test Loss: 0.19451889\n",
      "Test fold 3 \t Early stopping at Epoch [61]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.27709622 \t Best Test Loss: 0.18927748\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.49347858 \t Avg Val Loss: 0.28055932 \t Avg Test Loss: 0.34428964\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.26973757 \t Avg Val Loss: 0.23869576 \t Avg Test Loss: 0.26991621\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.26968611 \t Avg Val Loss: 0.23905044 \t Avg Test Loss: 0.26940172\n",
      "Test fold 4 \t Early stopping at Epoch [54]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.23798382 \t Best Test Loss: 0.27288523\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.27637507 \t Avg Val Loss: 0.22643430 \t Avg Test Loss: 0.32322720\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.25985396 \t Avg Val Loss: 0.22417888 \t Avg Test Loss: 0.32139857\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.26004863 \t Avg Val Loss: 0.22436906 \t Avg Test Loss: 0.32197825\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.26028416 \t Avg Val Loss: 0.22424693 \t Avg Test Loss: 0.32295157\n",
      "Test fold 5 \t Early stopping at Epoch [64]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.22377365 \t Best Test Loss: 0.32201422\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.27977276 \t Avg Val Loss: 0.28642531 \t Avg Test Loss: 0.33957926\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.25193453 \t Avg Val Loss: 0.26069910 \t Avg Test Loss: 0.32732577\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.25195520 \t Avg Val Loss: 0.25447462 \t Avg Test Loss: 0.32544990\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.25200440 \t Avg Val Loss: 0.25364258 \t Avg Test Loss: 0.32528963\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.25187344 \t Avg Val Loss: 0.25560065 \t Avg Test Loss: 0.32547167\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.25164552 \t Avg Val Loss: 0.25385701 \t Avg Test Loss: 0.32502508\n",
      "Test fold 6 \t Epoch [120] \t Avg Train Loss: 0.25122997 \t Avg Val Loss: 0.25368853 \t Avg Test Loss: 0.32497358\n",
      "Test fold 6 \t Epoch [140] \t Avg Train Loss: 0.25101416 \t Avg Val Loss: 0.25312676 \t Avg Test Loss: 0.32480238\n",
      "Test fold 6 \t Epoch [160] \t Avg Train Loss: 0.25064389 \t Avg Val Loss: 0.25472590 \t Avg Test Loss: 0.32498926\n",
      "Test fold 6 \t Epoch [180] \t Avg Train Loss: 0.25050450 \t Avg Val Loss: 0.25802609 \t Avg Test Loss: 0.32492017\n",
      "Test fold 6 \t Early stopping at Epoch [185]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.25164328 \t Best Test Loss: 0.32463475\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.27030384 \t Avg Val Loss: 0.29762842 \t Avg Test Loss: 0.26516615\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.25625078 \t Avg Val Loss: 0.29626589 \t Avg Test Loss: 0.26430762\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.25574988 \t Avg Val Loss: 0.29635253 \t Avg Test Loss: 0.26526358\n",
      "Test fold 1 \t Early stopping at Epoch [52]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.29402434 \t Best Test Loss: 0.26407778\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.31867239 \t Avg Val Loss: 0.32199110 \t Avg Test Loss: 0.23050870\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.25932503 \t Avg Val Loss: 0.30261292 \t Avg Test Loss: 0.22617229\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.23064387 \t Avg Val Loss: 0.25892283 \t Avg Test Loss: 0.19284526\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.19590026 \t Avg Val Loss: 0.21555391 \t Avg Test Loss: 0.16759393\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.20620290 \t Avg Val Loss: 0.20585529 \t Avg Test Loss: 0.17097169\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.21098386 \t Avg Val Loss: 0.18949611 \t Avg Test Loss: 0.15888427\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.21812821 \t Avg Val Loss: 0.23008709 \t Avg Test Loss: 0.18656454\n",
      "NaN loss detected at Epoch [125], Step [373]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.15070604 \t Best Test Loss: 0.16997709\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.32097730 \t Avg Val Loss: 0.29616294 \t Avg Test Loss: 0.18197040\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.27179217 \t Avg Val Loss: 0.29081515 \t Avg Test Loss: 0.18090324\n",
      "NaN loss detected at Epoch [40], Step [1761]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.24599682 \t Best Test Loss: 0.16182313\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.32738502 \t Avg Val Loss: 0.25235291 \t Avg Test Loss: 0.30346312\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.26632558 \t Avg Val Loss: 0.23972677 \t Avg Test Loss: 0.26957686\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.26476998 \t Avg Val Loss: 0.24402244 \t Avg Test Loss: 0.26183287\n",
      "Test fold 4 \t Early stopping at Epoch [53]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.23810235 \t Best Test Loss: 0.27230506\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.31184584 \t Avg Val Loss: 0.22824335 \t Avg Test Loss: 0.32599597\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.26049048 \t Avg Val Loss: 0.22439005 \t Avg Test Loss: 0.31978531\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.26023855 \t Avg Val Loss: 0.22398690 \t Avg Test Loss: 0.32332459\n"
     ]
    }
   ],
   "source": [
    "for num_layers in [1, 2]:\n",
    "    for hidden_size in [2, 4, 8, 16]:\n",
    "        for test_fold in np.unique(folds_df['fold']):\n",
    "            # Record start time\n",
    "            fold_start_time = time.time()\n",
    "\n",
    "            # Split data into training and test sets based on fold\n",
    "            train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "            test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "            # Prepare train and test sequences as tensors\n",
    "            train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "            test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "            # Prepare target values for training and testing\n",
    "            target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "            y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "            target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "            y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "            # Split training data into subtrain and validation sets\n",
    "            train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Initialize the GRU model, loss function, and optimizer\n",
    "            model = RNNModel(input_size, hidden_size, num_layers).to(device)    # Move model to device\n",
    "            criterion = SquaredHingeLoss().to(device)                           # Move loss function to device\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            # Variables for early stopping\n",
    "            best_train_loss = float('inf')    # Best training loss initialized to infinity\n",
    "            best_val_loss = float('inf')      # Best validation loss initialized to infinity\n",
    "            best_test_loss = float('inf')     # Best test loss corresponding to best validation\n",
    "            patience_counter = 0              # Early stopping patience counter\n",
    "            best_model_state = None           # Store the best model parameters\n",
    "            stop_epoch = 0                    # Epoch when training stops\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(max_epochs):\n",
    "                # Shuffle training sequences and targets\n",
    "                combined = list(zip(train_seqs, y_train))\n",
    "                random.shuffle(combined)\n",
    "                train_seqs, y_train = zip(*combined)\n",
    "\n",
    "                total_train_loss = 0\n",
    "                nan_flag = False  # Flag to detect NaN loss\n",
    "\n",
    "                # Train on subtrain data\n",
    "                model.train()  # Set model to training mode\n",
    "                for i, seq_input in enumerate(train_seqs):\n",
    "                    target = y_train[i].unsqueeze(0).to(device)  # Prepare target and move to device\n",
    "\n",
    "                    optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "                    # Forward pass\n",
    "                    seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "                    output_seq = model(seq_input)                               # Get model output\n",
    "                    loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "\n",
    "                    if torch.isnan(loss).any():  # Check for NaN loss\n",
    "                        print(f\"NaN loss detected at Epoch [{epoch}], Step [{i}]\")\n",
    "                        nan_flag = True\n",
    "                        break\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "                if nan_flag:\n",
    "                    break  # Stop training if NaN was encountered\n",
    "\n",
    "                # Calculate average training loss\n",
    "                avg_train_loss = total_train_loss / len(train_seqs)\n",
    "\n",
    "                # Calculate validation and test losses\n",
    "                avg_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "                avg_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {avg_val_loss:.8f} \\t Avg Test Loss: {avg_test_loss:.8f}')\n",
    "\n",
    "                # Early stopping based on validation loss\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss        # Update best validation loss\n",
    "                    best_train_loss = avg_train_loss    # Store best training loss\n",
    "                    best_test_loss = avg_test_loss      # Store test loss for best validation\n",
    "                    patience_counter = 0                # Reset patience counter\n",
    "\n",
    "                    # Save best model parameters\n",
    "                    best_model_state = model.state_dict()\n",
    "                    stop_epoch = epoch + 1              # Record stopping epoch\n",
    "                else:\n",
    "                    patience_counter += 1               # Increment patience counter\n",
    "\n",
    "                # Stop training if patience is exceeded\n",
    "                if patience_counter > patience:\n",
    "                    print(f\"Test fold {test_fold} \\t Early stopping at Epoch [{epoch}]\")\n",
    "                    break\n",
    "\n",
    "            # Record total time taken for this fold\n",
    "            fold_duration = time.time() - fold_start_time\n",
    "\n",
    "            # Save results to CSV\n",
    "            report_entry = {\n",
    "                'dataset': dataset,\n",
    "                'model': model_type,\n",
    "                'num_layers': num_layers,\n",
    "                'hidden_size': hidden_size,\n",
    "                'test_fold': test_fold,\n",
    "                'stop_epoch': stop_epoch,\n",
    "                'train_loss': best_train_loss,\n",
    "                'val_loss': best_val_loss,\n",
    "                'test_loss': best_test_loss,\n",
    "                'time': fold_duration\n",
    "            }\n",
    "\n",
    "            pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)  # Append entry to CSV\n",
    "\n",
    "            print(f\"Test fold {test_fold} \\t Training completed for GRU layers {num_layers} \\t Hidden size {hidden_size} \\t Best Val Loss: {best_val_loss:.8f} \\t Best Test Loss: {best_test_loss:.8f}\")\n",
    "            \n",
    "            # Restore best model parameters after training\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "            # Test the model and collect outputs\n",
    "            pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "            # Save model parameters\n",
    "            torch.save(model.state_dict(), f'saved_models/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.pth')\n",
    "\n",
    "            # Save predictions to CSV\n",
    "            lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "            lldas_df.to_csv(f'predictions/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
