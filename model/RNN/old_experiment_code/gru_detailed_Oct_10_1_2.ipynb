{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset = 'detailed'\n",
    "\n",
    "# Model parameters\n",
    "input_size = 1\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 50\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)    # GRU\n",
    "        self.fc = nn.Linear(hidden_size, 1)                                         # Linear\n",
    "\n",
    "    def forward(self, x):               \n",
    "        gru_out, _ = self.gru(x)                            # Pass sequence through GRU    \n",
    "        last_out = gru_out[:, -1, :]                        # Take the hidden state of the last time step \n",
    "        x = self.fc(last_out)                               # Linear combination         \n",
    "        x = torch.relu(x + 10) - torch.relu(x - 10) - 10    # clamp between -10 and 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()                                                        # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Add batch dimension and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            predictions.append(output_seq.item())                       # Store the prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss value\n",
    "def get_loss_value(model, test_seqs, y_test, criterion):\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for i, seq_input in enumerate(test_seqs):\n",
    "            target = y_test[i].unsqueeze(0).to(device)                  # Move target to device\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "            total_test_loss += loss.item()                              # Accumulate loss\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_seqs)                    # Calculate average loss\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence data from CSV\n",
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "# Group sequences by 'sequenceID'\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))\n",
    "\n",
    "# Load fold and target data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.48146798 \t Avg Val Loss: 0.46392805 \t Avg Test Loss: 0.46049657\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.31294164 \t Avg Val Loss: 0.30319801 \t Avg Test Loss: 0.27550215\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.21379089 \t Avg Val Loss: 0.22758728 \t Avg Test Loss: 0.21854952\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.20555037 \t Avg Val Loss: 0.19853043 \t Avg Test Loss: 0.18162431\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.19905234 \t Avg Val Loss: 0.19322582 \t Avg Test Loss: 0.18486269\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.19323741 \t Avg Val Loss: 0.18484961 \t Avg Test Loss: 0.17511031\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.19204089 \t Avg Val Loss: 0.18092533 \t Avg Test Loss: 0.17215663\n",
      "Test fold 1 \t Epoch [140] \t Avg Train Loss: 0.19210485 \t Avg Val Loss: 0.18167364 \t Avg Test Loss: 0.17568239\n",
      "Test fold 1 \t Epoch [160] \t Avg Train Loss: 0.19300469 \t Avg Val Loss: 0.17741820 \t Avg Test Loss: 0.17019194\n",
      "Test fold 1 \t Epoch [180] \t Avg Train Loss: 0.19118612 \t Avg Val Loss: 0.18033432 \t Avg Test Loss: 0.16663500\n",
      "Test fold 1 \t Epoch [200] \t Avg Train Loss: 0.19097829 \t Avg Val Loss: 0.17957453 \t Avg Test Loss: 0.17446540\n",
      "Test fold 1 \t Epoch [220] \t Avg Train Loss: 0.19219874 \t Avg Val Loss: 0.17778239 \t Avg Test Loss: 0.17116068\n",
      "Test fold 1 \t Epoch [240] \t Avg Train Loss: 0.18899729 \t Avg Val Loss: 0.17800912 \t Avg Test Loss: 0.16934357\n",
      "Test fold 1 \t Epoch [260] \t Avg Train Loss: 0.18981202 \t Avg Val Loss: 0.17690839 \t Avg Test Loss: 0.17157538\n",
      "Test fold 1 \t Epoch [280] \t Avg Train Loss: 0.18968340 \t Avg Val Loss: 0.17753054 \t Avg Test Loss: 0.16599699\n",
      "Test fold 1 \t Epoch [300] \t Avg Train Loss: 0.18759779 \t Avg Val Loss: 0.17635443 \t Avg Test Loss: 0.17194543\n",
      "Test fold 1 \t Epoch [320] \t Avg Train Loss: 0.18772930 \t Avg Val Loss: 0.17544605 \t Avg Test Loss: 0.16748485\n",
      "Test fold 1 \t Epoch [340] \t Avg Train Loss: 0.18706940 \t Avg Val Loss: 0.17540533 \t Avg Test Loss: 0.16864241\n",
      "Early stopping triggered after 350 epochs.\n",
      "Best Train Loss: 0.18722334\n",
      "Best Validation Loss: 0.17428400\n",
      "Test Loss associated with Best Validation Loss: 0.16823622\n",
      "Running time for fold 1: 14747.37 seconds\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.46304695 \t Avg Val Loss: 0.35201004 \t Avg Test Loss: 0.52048232\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.30333699 \t Avg Val Loss: 0.27261229 \t Avg Test Loss: 0.42647220\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.29330692 \t Avg Val Loss: 0.29555972 \t Avg Test Loss: 0.43916912\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.30166463 \t Avg Val Loss: 0.27592342 \t Avg Test Loss: 0.37451636\n",
      "Early stopping triggered after 73 epochs.\n",
      "Best Train Loss: 0.30310215\n",
      "Best Validation Loss: 0.23993231\n",
      "Test Loss associated with Best Validation Loss: 0.34888715\n",
      "Running time for fold 2: 3089.74 seconds\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.46990130 \t Avg Val Loss: 0.53871003 \t Avg Test Loss: 0.39865263\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.37375050 \t Avg Val Loss: 0.43142174 \t Avg Test Loss: 0.30158340\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.25416039 \t Avg Val Loss: 0.30108878 \t Avg Test Loss: 0.21884554\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.22652516 \t Avg Val Loss: 0.28527353 \t Avg Test Loss: 0.20621777\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.21995918 \t Avg Val Loss: 0.28023547 \t Avg Test Loss: 0.20941753\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.21048566 \t Avg Val Loss: 0.27513793 \t Avg Test Loss: 0.20121855\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.20685137 \t Avg Val Loss: 0.25688450 \t Avg Test Loss: 0.19200750\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.20131640 \t Avg Val Loss: 0.24850995 \t Avg Test Loss: 0.19228868\n"
     ]
    }
   ],
   "source": [
    "for test_fold in np.unique(folds_df['fold']):\n",
    "    # Record start time\n",
    "    fold_start_time = time.time()\n",
    "\n",
    "    # Split data into training and test sets based on fold\n",
    "    train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "    test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "    # Prepare train and test sequences as tensors\n",
    "    train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "    test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "    # Prepare target values for training and testing\n",
    "    target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "    y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "    target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "    y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    # Split training data into subtrain and validation sets\n",
    "    train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the GRU model, loss function, and optimizer\n",
    "    model = GRUModel(input_size, hidden_size, num_layers).to(device)    # Move model to device\n",
    "    criterion = SquaredHingeLoss().to(device)                           # Move loss function to device\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Variables for early stopping\n",
    "    best_train_loss = float('inf')    # Best training loss initialized to infinity\n",
    "    best_val_loss = float('inf')      # Best validation loss initialized to infinity\n",
    "    best_test_loss = float('inf')     # Best test loss corresponding to best validation\n",
    "    patience_counter = 0              # Early stopping patience counter\n",
    "    best_model_state = None           # Store the best model parameters\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Shuffle training sequences and targets\n",
    "        combined = list(zip(train_seqs, y_train))\n",
    "        random.shuffle(combined)\n",
    "        train_seqs, y_train = zip(*combined)\n",
    "\n",
    "        total_train_loss = 0\n",
    "        nan_flag = False  # Flag to detect NaN loss\n",
    "\n",
    "        # Train on subtrain data\n",
    "        model.train()  # Set model to training mode\n",
    "        for i, seq_input in enumerate(train_seqs):\n",
    "            target = y_train[i].unsqueeze(0).to(device)  # Prepare target and move to device\n",
    "\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            # Forward pass\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "\n",
    "            if torch.isnan(loss).any():  # Check for NaN loss\n",
    "                print(f\"NaN loss detected at Epoch [{epoch}], Step [{i}]\")\n",
    "                nan_flag = True\n",
    "                break\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "        if nan_flag:\n",
    "            break  # Stop training if NaN was encountered\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_seqs)\n",
    "\n",
    "        # Calculate validation and test losses\n",
    "        avg_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "        avg_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {avg_val_loss:.8f} \\t Avg Test Loss: {avg_test_loss:.8f}')\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss        # Update best validation loss\n",
    "            best_train_loss = avg_train_loss    # Store best training loss\n",
    "            best_test_loss = avg_test_loss      # Store test loss for best validation\n",
    "            patience_counter = 0                # Reset patience counter\n",
    "\n",
    "            # Save best model parameters\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1               # Increment patience counter\n",
    "\n",
    "        # Stop training if patience is exceeded\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "            break\n",
    "\n",
    "    # Restore best model parameters after training\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Print the best losses\n",
    "    print(f'Best Train Loss: {best_train_loss:.8f}')\n",
    "    print(f'Best Validation Loss: {best_val_loss:.8f}')\n",
    "    print(f'Test Loss associated with Best Validation Loss: {best_test_loss:.8f}')\n",
    "\n",
    "    # Test the model and collect outputs\n",
    "    pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "    # Save model parameters\n",
    "    torch.save(model.state_dict(), f'saved_models/gru_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.pth')\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "    lldas_df.to_csv(f'predictions/{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.csv', index=False)\n",
    "\n",
    "    # Record end time and calculate running time\n",
    "    fold_end_time = time.time()\n",
    "    fold_runtime = fold_end_time - fold_start_time\n",
    "    print(f'Running time for fold {test_fold}: {fold_runtime:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
