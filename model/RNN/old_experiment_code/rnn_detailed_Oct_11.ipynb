{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset = 'detailed'\n",
    "\n",
    "# Model parameters\n",
    "model_type = 'rnn'\n",
    "input_size = 1\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 50\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity='relu')    # RNN\n",
    "        self.fc = nn.Linear(hidden_size, 1)                                         # Linear\n",
    "\n",
    "    def forward(self, x):               \n",
    "        rnn_out, _ = self.rnn(x)                            # Pass sequence through RNN    \n",
    "        last_out = rnn_out[:, -1, :]                        # Take the hidden state of the last time step\n",
    "        x = self.fc(last_out)                               # Linear combination         \n",
    "        x = torch.relu(x + 10) - torch.relu(x - 10) - 10    # Clamp between -10 and 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()                                                        # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Add batch dimension and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            predictions.append(output_seq.item())                       # Store the prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss value\n",
    "def get_loss_value(model, test_seqs, y_test, criterion):\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for i, seq_input in enumerate(test_seqs):\n",
    "            target = y_test[i].unsqueeze(0).to(device)                  # Move target to device\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "            total_test_loss += loss.item()                              # Accumulate loss\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_seqs)                    # Calculate average loss\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence data from CSV\n",
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "# Group sequences by 'sequenceID'\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))\n",
    "\n",
    "# Load fold and target data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CSV file for logging\n",
    "report_path = f'report_{dataset}_{model_type}.csv'\n",
    "report_header = ['dataset', 'model', 'num_layers', 'hidden_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n",
    "if not os.path.exists(report_path):\n",
    "    pd.DataFrame(columns=report_header).to_csv(report_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.44986367 \t Avg Val Loss: 0.45744221 \t Avg Test Loss: 0.46105845\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.42693304 \t Avg Val Loss: 0.45411779 \t Avg Test Loss: 0.45012597\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.42341402 \t Avg Val Loss: 0.46564972 \t Avg Test Loss: 0.44968767\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.42211764 \t Avg Val Loss: 0.46743447 \t Avg Test Loss: 0.45103665\n",
      "Test fold 1 \t Early stopping at Epoch [79]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.44075890 \t Best Test Loss: 0.45314933\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.48885401 \t Avg Val Loss: 0.35446021 \t Avg Test Loss: 0.51909972\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.45958199 \t Avg Val Loss: 0.35405190 \t Avg Test Loss: 0.52112017\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.45981228 \t Avg Val Loss: 0.35645596 \t Avg Test Loss: 0.51792302\n",
      "Test fold 2 \t Early stopping at Epoch [52]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.35233970 \t Best Test Loss: 0.52076989\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.44700276 \t Avg Val Loss: 0.53954330 \t Avg Test Loss: 0.40308729\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.42074182 \t Avg Val Loss: 0.49860057 \t Avg Test Loss: 0.37037568\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.40944519 \t Avg Val Loss: 0.49891845 \t Avg Test Loss: 0.36135610\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.40743619 \t Avg Val Loss: 0.51552206 \t Avg Test Loss: 0.36880689\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.44368380 \t Avg Val Loss: 0.52961712 \t Avg Test Loss: 0.39976954\n",
      "Test fold 3 \t Early stopping at Epoch [84]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.48603855 \t Best Test Loss: 0.37253141\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.59505815 \t Avg Val Loss: 0.46297115 \t Avg Test Loss: 0.44356793\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.45295889 \t Avg Val Loss: 0.46960504 \t Avg Test Loss: 0.43483397\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.45315145 \t Avg Val Loss: 0.47081291 \t Avg Test Loss: 0.43509358\n",
      "Test fold 4 \t Early stopping at Epoch [51]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.46297115 \t Best Test Loss: 0.44356793\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.46044296 \t Avg Val Loss: 0.41292519 \t Avg Test Loss: 0.48015789\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.44890348 \t Avg Val Loss: 0.41798219 \t Avg Test Loss: 0.49230682\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.44782187 \t Avg Val Loss: 0.41937412 \t Avg Test Loss: 0.49622021\n",
      "Test fold 5 \t Early stopping at Epoch [57]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.40879730 \t Best Test Loss: 0.48352620\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.48074521 \t Avg Val Loss: 0.40279586 \t Avg Test Loss: 0.41698693\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.47347249 \t Avg Val Loss: 0.40708143 \t Avg Test Loss: 0.41570748\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.47345102 \t Avg Val Loss: 0.40716948 \t Avg Test Loss: 0.41570572\n",
      "Test fold 6 \t Early stopping at Epoch [51]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.40279586 \t Best Test Loss: 0.41698693\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.44858122 \t Avg Val Loss: 0.46209987 \t Avg Test Loss: 0.46240954\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.44788282 \t Avg Val Loss: 0.46226932 \t Avg Test Loss: 0.46177396\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.44810904 \t Avg Val Loss: 0.46314978 \t Avg Test Loss: 0.46036116\n",
      "Test fold 1 \t Early stopping at Epoch [51]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.46209987 \t Best Test Loss: 0.46240954\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.50261618 \t Avg Val Loss: 0.34772207 \t Avg Test Loss: 0.53721518\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.45516795 \t Avg Val Loss: 0.35090597 \t Avg Test Loss: 0.50939890\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.38315114 \t Avg Val Loss: 0.32573108 \t Avg Test Loss: 0.47149419\n",
      "NaN loss detected at Epoch [50], Step [458]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.29277751 \t Best Test Loss: 0.40221256\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.46521911 \t Avg Val Loss: 0.53964200 \t Avg Test Loss: 0.39893071\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.44030515 \t Avg Val Loss: 0.53848413 \t Avg Test Loss: 0.39221294\n",
      "NaN loss detected at Epoch [40], Step [999]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.45300641 \t Best Test Loss: 0.33162973\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.47267301 \t Avg Val Loss: 0.46651393 \t Avg Test Loss: 0.43583763\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.44183359 \t Avg Val Loss: 0.47736855 \t Avg Test Loss: 0.43643922\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.43650212 \t Avg Val Loss: 0.48347880 \t Avg Test Loss: 0.43846060\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.44316157 \t Avg Val Loss: 0.48453881 \t Avg Test Loss: 0.42774842\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.42649643 \t Avg Val Loss: 0.48450414 \t Avg Test Loss: 0.43801546\n",
      "Test fold 4 \t Epoch [100] \t Avg Train Loss: 0.42825581 \t Avg Val Loss: 0.47256727 \t Avg Test Loss: 0.43071219\n",
      "Test fold 4 \t Early stopping at Epoch [117]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.46000206 \t Best Test Loss: 0.43963002\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.48314661 \t Avg Val Loss: 0.41422695 \t Avg Test Loss: 0.47994168\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.44643502 \t Avg Val Loss: 0.41163060 \t Avg Test Loss: 0.49392132\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.42926258 \t Avg Val Loss: 0.41476740 \t Avg Test Loss: 0.49342560\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.42292326 \t Avg Val Loss: 0.41891088 \t Avg Test Loss: 0.49657520\n",
      "Test fold 5 \t Early stopping at Epoch [75]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.40685782 \t Best Test Loss: 0.49848912\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.48855775 \t Avg Val Loss: 0.40345015 \t Avg Test Loss: 0.41489055\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.43823341 \t Avg Val Loss: 0.38980945 \t Avg Test Loss: 0.39341467\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.41354975 \t Avg Val Loss: 0.39977765 \t Avg Test Loss: 0.38749124\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.37736712 \t Avg Val Loss: 0.36293660 \t Avg Test Loss: 0.38312998\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.35217811 \t Avg Val Loss: 0.36229329 \t Avg Test Loss: 0.37716163\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.34440557 \t Avg Val Loss: 0.35511015 \t Avg Test Loss: 0.36705701\n",
      "NaN loss detected at Epoch [109], Step [1678]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.34636971 \t Best Test Loss: 0.39157971\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.47319640 \t Avg Val Loss: 0.46433697 \t Avg Test Loss: 0.46011482\n",
      "NaN loss detected at Epoch [17], Step [1345]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.36786914 \t Best Test Loss: 0.38108319\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.45952570 \t Avg Val Loss: 0.35722713 \t Avg Test Loss: 0.51724324\n",
      "NaN loss detected at Epoch [17], Step [1274]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.31362239 \t Best Test Loss: 0.45372443\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45338422 \t Avg Val Loss: 0.54435560 \t Avg Test Loss: 0.40531740\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.32249851 \t Avg Val Loss: 0.43945223 \t Avg Test Loss: 0.29360412\n"
     ]
    }
   ],
   "source": [
    "for num_layers in [1, 2]:\n",
    "    for hidden_size in [2, 4, 8, 16]:\n",
    "        for test_fold in np.unique(folds_df['fold']):\n",
    "            # Record start time\n",
    "            fold_start_time = time.time()\n",
    "\n",
    "            # Split data into training and test sets based on fold\n",
    "            train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "            test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "            # Prepare train and test sequences as tensors\n",
    "            train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "            test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "            # Prepare target values for training and testing\n",
    "            target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "            y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "            target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "            y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "            # Split training data into subtrain and validation sets\n",
    "            train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Initialize the GRU model, loss function, and optimizer\n",
    "            model = RNNModel(input_size, hidden_size, num_layers).to(device)    # Move model to device\n",
    "            criterion = SquaredHingeLoss().to(device)                           # Move loss function to device\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            # Variables for early stopping\n",
    "            best_train_loss = float('inf')    # Best training loss initialized to infinity\n",
    "            best_val_loss = float('inf')      # Best validation loss initialized to infinity\n",
    "            best_test_loss = float('inf')     # Best test loss corresponding to best validation\n",
    "            patience_counter = 0              # Early stopping patience counter\n",
    "            best_model_state = None           # Store the best model parameters\n",
    "            stop_epoch = 0                    # Epoch when training stops\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(max_epochs):\n",
    "                # Shuffle training sequences and targets\n",
    "                combined = list(zip(train_seqs, y_train))\n",
    "                random.shuffle(combined)\n",
    "                train_seqs, y_train = zip(*combined)\n",
    "\n",
    "                total_train_loss = 0\n",
    "                nan_flag = False  # Flag to detect NaN loss\n",
    "\n",
    "                # Train on subtrain data\n",
    "                model.train()  # Set model to training mode\n",
    "                for i, seq_input in enumerate(train_seqs):\n",
    "                    target = y_train[i].unsqueeze(0).to(device)  # Prepare target and move to device\n",
    "\n",
    "                    optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "                    # Forward pass\n",
    "                    seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "                    output_seq = model(seq_input)                               # Get model output\n",
    "                    loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "\n",
    "                    if torch.isnan(loss).any():  # Check for NaN loss\n",
    "                        print(f\"NaN loss detected at Epoch [{epoch}], Step [{i}]\")\n",
    "                        nan_flag = True\n",
    "                        break\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "                if nan_flag:\n",
    "                    break  # Stop training if NaN was encountered\n",
    "\n",
    "                # Calculate average training loss\n",
    "                avg_train_loss = total_train_loss / len(train_seqs)\n",
    "\n",
    "                # Calculate validation and test losses\n",
    "                avg_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "                avg_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {avg_val_loss:.8f} \\t Avg Test Loss: {avg_test_loss:.8f}')\n",
    "\n",
    "                # Early stopping based on validation loss\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss        # Update best validation loss\n",
    "                    best_train_loss = avg_train_loss    # Store best training loss\n",
    "                    best_test_loss = avg_test_loss      # Store test loss for best validation\n",
    "                    patience_counter = 0                # Reset patience counter\n",
    "\n",
    "                    # Save best model parameters\n",
    "                    best_model_state = model.state_dict()\n",
    "                    stop_epoch = epoch + 1              # Record stopping epoch\n",
    "                else:\n",
    "                    patience_counter += 1               # Increment patience counter\n",
    "\n",
    "                # Stop training if patience is exceeded\n",
    "                if patience_counter > patience:\n",
    "                    print(f\"Test fold {test_fold} \\t Early stopping at Epoch [{epoch}]\")\n",
    "                    break\n",
    "\n",
    "            # Record total time taken for this fold\n",
    "            fold_duration = time.time() - fold_start_time\n",
    "\n",
    "            # Save results to CSV\n",
    "            report_entry = {\n",
    "                'dataset': dataset,\n",
    "                'model': model_type,\n",
    "                'num_layers': num_layers,\n",
    "                'hidden_size': hidden_size,\n",
    "                'test_fold': test_fold,\n",
    "                'stop_epoch': stop_epoch,\n",
    "                'train_loss': best_train_loss,\n",
    "                'val_loss': best_val_loss,\n",
    "                'test_loss': best_test_loss,\n",
    "                'time': fold_duration\n",
    "            }\n",
    "\n",
    "            pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)  # Append entry to CSV\n",
    "\n",
    "            print(f\"Test fold {test_fold} \\t Training completed for GRU layers {num_layers} \\t Hidden size {hidden_size} \\t Best Val Loss: {best_val_loss:.8f} \\t Best Test Loss: {best_test_loss:.8f}\")\n",
    "            \n",
    "            # Restore best model parameters after training\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "            # Test the model and collect outputs\n",
    "            pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "            # Save model parameters\n",
    "            torch.save(model.state_dict(), f'saved_models/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.pth')\n",
    "\n",
    "            # Save predictions to CSV\n",
    "            lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "            lldas_df.to_csv(f'predictions/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
