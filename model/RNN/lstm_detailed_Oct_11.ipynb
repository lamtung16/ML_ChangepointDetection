{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset = 'detailed'\n",
    "\n",
    "# Model parameters\n",
    "model_type = 'lstm'\n",
    "input_size = 1\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 50\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinged Square Loss\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(SquaredHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, predicted, y):\n",
    "        low, high = y[:, 0:1], y[:, 1:2]\n",
    "        loss_low = torch.relu(low - predicted + self.margin)\n",
    "        loss_high = torch.relu(predicted - high + self.margin)\n",
    "        loss = loss_low + loss_high\n",
    "        return torch.mean(torch.square(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # LSTM\n",
    "        self.fc = nn.Linear(hidden_size, 1)                                         # Linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(x)  # Pass sequence through LSTM\n",
    "        last_out = lstm_out[:, -1, :]                        # Take the hidden state of the last time step\n",
    "        x = self.fc(last_out)                                # Linear combination\n",
    "        x = torch.relu(x + 10) - torch.relu(x - 10) - 10     # Clamp between -10 and 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, inputs):\n",
    "    model.eval()                                                        # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for seq_input in inputs:\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Add batch dimension and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            predictions.append(output_seq.item())                       # Store the prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss value\n",
    "def get_loss_value(model, test_seqs, y_test, criterion):\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():                                               # Disable gradient calculation\n",
    "        for i, seq_input in enumerate(test_seqs):\n",
    "            target = y_test[i].unsqueeze(0).to(device)                  # Move target to device\n",
    "            seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "            output_seq = model(seq_input)                               # Get model output\n",
    "            loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "            total_test_loss += loss.item()                              # Accumulate loss\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_seqs)                    # Calculate average loss\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence data from CSV\n",
    "file_path = f'../../sequence_data/{dataset}/profiles.csv.xz'\n",
    "with lzma.open(file_path, 'rt') as file:\n",
    "    signal_df = pd.read_csv(file)\n",
    "\n",
    "# Group sequences by 'sequenceID'\n",
    "seqs = tuple(signal_df.groupby('sequenceID'))\n",
    "\n",
    "# Load fold and target data\n",
    "folds_df = pd.read_csv(f'../../training_data/{dataset}/folds.csv')\n",
    "target_df = pd.read_csv(f'../../training_data/{dataset}/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CSV file for logging\n",
    "report_path = f'report_{dataset}_{model_type}.csv'\n",
    "report_header = ['dataset', 'model', 'num_layers', 'hidden_size', 'test_fold', 'stop_epoch', 'train_loss', 'val_loss', 'test_loss', 'time']\n",
    "if not os.path.exists(report_path):\n",
    "    pd.DataFrame(columns=report_header).to_csv(report_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.44897857 \t Avg Val Loss: 0.46127735 \t Avg Test Loss: 0.46302498\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.44524332 \t Avg Val Loss: 0.45470730 \t Avg Test Loss: 0.45528470\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.36984734 \t Avg Val Loss: 0.37177407 \t Avg Test Loss: 0.35664845\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.31251610 \t Avg Val Loss: 0.27999906 \t Avg Test Loss: 0.28062326\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.31655721 \t Avg Val Loss: 0.32621149 \t Avg Test Loss: 0.31921525\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.28907238 \t Avg Val Loss: 0.27771570 \t Avg Test Loss: 0.27224661\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.30992071 \t Avg Val Loss: 0.28841435 \t Avg Test Loss: 0.27662972\n",
      "Test fold 1 \t Early stopping at Epoch [130]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.27067708 \t Best Test Loss: 0.27563094\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.48198109 \t Avg Val Loss: 0.35247066 \t Avg Test Loss: 0.52707673\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.44546328 \t Avg Val Loss: 0.35684494 \t Avg Test Loss: 0.50699699\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.40030411 \t Avg Val Loss: 0.31511426 \t Avg Test Loss: 0.44801380\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.32291488 \t Avg Val Loss: 0.26790928 \t Avg Test Loss: 0.35270273\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.31734746 \t Avg Val Loss: 0.25914701 \t Avg Test Loss: 0.35988827\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.31161226 \t Avg Val Loss: 0.25707521 \t Avg Test Loss: 0.34694678\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.30756015 \t Avg Val Loss: 0.25491482 \t Avg Test Loss: 0.34322801\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.30496628 \t Avg Val Loss: 0.25445297 \t Avg Test Loss: 0.33302054\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.29559344 \t Avg Val Loss: 0.25153814 \t Avg Test Loss: 0.31611664\n",
      "Test fold 2 \t Epoch [180] \t Avg Train Loss: 0.28401495 \t Avg Val Loss: 0.25229941 \t Avg Test Loss: 0.31748997\n",
      "Test fold 2 \t Epoch [200] \t Avg Train Loss: 0.26356001 \t Avg Val Loss: 0.23341100 \t Avg Test Loss: 0.28784859\n",
      "Test fold 2 \t Epoch [220] \t Avg Train Loss: 0.23277481 \t Avg Val Loss: 0.19946662 \t Avg Test Loss: 0.24502074\n",
      "Test fold 2 \t Epoch [240] \t Avg Train Loss: 0.22487861 \t Avg Val Loss: 0.18814101 \t Avg Test Loss: 0.24114871\n",
      "Test fold 2 \t Epoch [260] \t Avg Train Loss: 0.21890050 \t Avg Val Loss: 0.18655145 \t Avg Test Loss: 0.23136925\n",
      "Test fold 2 \t Epoch [280] \t Avg Train Loss: 0.21323828 \t Avg Val Loss: 0.20807200 \t Avg Test Loss: 0.26741205\n",
      "Test fold 2 \t Epoch [300] \t Avg Train Loss: 0.21246636 \t Avg Val Loss: 0.18853406 \t Avg Test Loss: 0.24050469\n",
      "Test fold 2 \t Epoch [320] \t Avg Train Loss: 0.20790847 \t Avg Val Loss: 0.18284266 \t Avg Test Loss: 0.22823971\n",
      "Test fold 2 \t Epoch [340] \t Avg Train Loss: 0.20437154 \t Avg Val Loss: 0.18771253 \t Avg Test Loss: 0.22115192\n",
      "Test fold 2 \t Early stopping at Epoch [357]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.18005860 \t Best Test Loss: 0.22647081\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45205119 \t Avg Val Loss: 0.53886084 \t Avg Test Loss: 0.39654048\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.43775157 \t Avg Val Loss: 0.53393826 \t Avg Test Loss: 0.38470546\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.38890171 \t Avg Val Loss: 0.48287106 \t Avg Test Loss: 0.36738162\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.32387892 \t Avg Val Loss: 0.42919056 \t Avg Test Loss: 0.32258471\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.29605688 \t Avg Val Loss: 0.39536167 \t Avg Test Loss: 0.28261310\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.28770751 \t Avg Val Loss: 0.39959963 \t Avg Test Loss: 0.30759388\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.27721318 \t Avg Val Loss: 0.39652556 \t Avg Test Loss: 0.29607895\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.27225248 \t Avg Val Loss: 0.37583739 \t Avg Test Loss: 0.28515507\n",
      "Test fold 3 \t Epoch [160] \t Avg Train Loss: 0.27422809 \t Avg Val Loss: 0.38834835 \t Avg Test Loss: 0.30073938\n",
      "Test fold 3 \t Epoch [180] \t Avg Train Loss: 0.26583645 \t Avg Val Loss: 0.38612879 \t Avg Test Loss: 0.31171504\n",
      "Test fold 3 \t Early stopping at Epoch [191]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.37583739 \t Best Test Loss: 0.28515507\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.46098067 \t Avg Val Loss: 0.46651417 \t Avg Test Loss: 0.43707799\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.44441736 \t Avg Val Loss: 0.46422406 \t Avg Test Loss: 0.42693408\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.40711374 \t Avg Val Loss: 0.44083826 \t Avg Test Loss: 0.41710803\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.29249053 \t Avg Val Loss: 0.30946640 \t Avg Test Loss: 0.26854229\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.28699995 \t Avg Val Loss: 0.28081785 \t Avg Test Loss: 0.27342313\n",
      "Test fold 4 \t Epoch [100] \t Avg Train Loss: 0.33779871 \t Avg Val Loss: 0.40362388 \t Avg Test Loss: 0.37968183\n",
      "Test fold 4 \t Epoch [120] \t Avg Train Loss: 0.30652203 \t Avg Val Loss: 0.31416637 \t Avg Test Loss: 0.27989994\n",
      "Test fold 4 \t Early stopping at Epoch [126]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.26662704 \t Best Test Loss: 0.26565553\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.46094266 \t Avg Val Loss: 0.41539092 \t Avg Test Loss: 0.48236731\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.43635673 \t Avg Val Loss: 0.41654467 \t Avg Test Loss: 0.48444098\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.39580280 \t Avg Val Loss: 0.38740413 \t Avg Test Loss: 0.44663988\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.27598027 \t Avg Val Loss: 0.32530269 \t Avg Test Loss: 0.40914872\n",
      "Test fold 5 \t Epoch [ 80] \t Avg Train Loss: 0.27739926 \t Avg Val Loss: 0.27688592 \t Avg Test Loss: 0.35002022\n",
      "Test fold 5 \t Epoch [100] \t Avg Train Loss: 0.27620679 \t Avg Val Loss: 0.26398701 \t Avg Test Loss: 0.36680619\n",
      "Test fold 5 \t Epoch [120] \t Avg Train Loss: 0.26796966 \t Avg Val Loss: 0.27703077 \t Avg Test Loss: 0.38684303\n",
      "Test fold 5 \t Epoch [140] \t Avg Train Loss: 0.26269952 \t Avg Val Loss: 0.27803781 \t Avg Test Loss: 0.38920169\n",
      "Test fold 5 \t Early stopping at Epoch [156]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.25858031 \t Best Test Loss: 0.35791133\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.51744455 \t Avg Val Loss: 0.40214971 \t Avg Test Loss: 0.41788690\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.47080892 \t Avg Val Loss: 0.40022745 \t Avg Test Loss: 0.41740375\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.43236663 \t Avg Val Loss: 0.36632438 \t Avg Test Loss: 0.40832170\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.32581329 \t Avg Val Loss: 0.32319821 \t Avg Test Loss: 0.32969517\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.33138457 \t Avg Val Loss: 0.33011351 \t Avg Test Loss: 0.35259102\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.31811259 \t Avg Val Loss: 0.33132391 \t Avg Test Loss: 0.34575722\n",
      "Test fold 6 \t Early stopping at Epoch [101]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 2 \t Best Val Loss: 0.31771666 \t Best Test Loss: 0.33239390\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.46818757 \t Avg Val Loss: 0.46707108 \t Avg Test Loss: 0.45615706\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.43878602 \t Avg Val Loss: 0.44353784 \t Avg Test Loss: 0.44731964\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.33761890 \t Avg Val Loss: 0.40755535 \t Avg Test Loss: 0.37425446\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.31102221 \t Avg Val Loss: 0.31950757 \t Avg Test Loss: 0.30584238\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.29827540 \t Avg Val Loss: 0.32177952 \t Avg Test Loss: 0.30104356\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.43480289 \t Avg Val Loss: 0.43085840 \t Avg Test Loss: 0.43409536\n",
      "Test fold 1 \t Early stopping at Epoch [108]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.28986680 \t Best Test Loss: 0.29730344\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.46515712 \t Avg Val Loss: 0.35070778 \t Avg Test Loss: 0.52463508\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.34484838 \t Avg Val Loss: 0.29885563 \t Avg Test Loss: 0.43326995\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.31867446 \t Avg Val Loss: 0.26949627 \t Avg Test Loss: 0.37708816\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.31218498 \t Avg Val Loss: 0.26658807 \t Avg Test Loss: 0.36703984\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.30811094 \t Avg Val Loss: 0.26337528 \t Avg Test Loss: 0.37026427\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.30133040 \t Avg Val Loss: 0.26746267 \t Avg Test Loss: 0.35702708\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.29821503 \t Avg Val Loss: 0.26239352 \t Avg Test Loss: 0.35497791\n",
      "Test fold 2 \t Early stopping at Epoch [139]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.26022414 \t Best Test Loss: 0.36814445\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.44508252 \t Avg Val Loss: 0.54081073 \t Avg Test Loss: 0.40073340\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.40118689 \t Avg Val Loss: 0.49424221 \t Avg Test Loss: 0.39796365\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.36046674 \t Avg Val Loss: 0.42282522 \t Avg Test Loss: 0.28635034\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.29401850 \t Avg Val Loss: 0.40720136 \t Avg Test Loss: 0.29874738\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.27643629 \t Avg Val Loss: 0.39952090 \t Avg Test Loss: 0.30196922\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.26123801 \t Avg Val Loss: 0.39494511 \t Avg Test Loss: 0.30924766\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.25142653 \t Avg Val Loss: 0.37599205 \t Avg Test Loss: 0.28141289\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.25258963 \t Avg Val Loss: 0.39130862 \t Avg Test Loss: 0.29052824\n",
      "Test fold 3 \t Early stopping at Epoch [141]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.36933365 \t Best Test Loss: 0.29332344\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.45604821 \t Avg Val Loss: 0.47055696 \t Avg Test Loss: 0.43572024\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.44003442 \t Avg Val Loss: 0.46256499 \t Avg Test Loss: 0.41955350\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.31859544 \t Avg Val Loss: 0.32364069 \t Avg Test Loss: 0.30240908\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.30228829 \t Avg Val Loss: 0.28959362 \t Avg Test Loss: 0.29924504\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.29026840 \t Avg Val Loss: 0.32981546 \t Avg Test Loss: 0.30073135\n",
      "Test fold 4 \t Epoch [100] \t Avg Train Loss: 0.27533264 \t Avg Val Loss: 0.27866247 \t Avg Test Loss: 0.27957148\n",
      "Test fold 4 \t Epoch [120] \t Avg Train Loss: 0.26940597 \t Avg Val Loss: 0.27987831 \t Avg Test Loss: 0.29190800\n",
      "Test fold 4 \t Early stopping at Epoch [126]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.27079177 \t Best Test Loss: 0.27746162\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.47935617 \t Avg Val Loss: 0.41382467 \t Avg Test Loss: 0.48349620\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.41644879 \t Avg Val Loss: 0.39349093 \t Avg Test Loss: 0.45510677\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.28923392 \t Avg Val Loss: 0.33648095 \t Avg Test Loss: 0.34181005\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.27655033 \t Avg Val Loss: 0.30626820 \t Avg Test Loss: 0.36683639\n",
      "Test fold 5 \t Epoch [ 80] \t Avg Train Loss: 0.26652235 \t Avg Val Loss: 0.32073451 \t Avg Test Loss: 0.34812002\n",
      "Test fold 5 \t Early stopping at Epoch [83]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.25823106 \t Best Test Loss: 0.39069138\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.48013196 \t Avg Val Loss: 0.40299712 \t Avg Test Loss: 0.41682104\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.47074415 \t Avg Val Loss: 0.40259291 \t Avg Test Loss: 0.41749776\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.44267010 \t Avg Val Loss: 0.41192255 \t Avg Test Loss: 0.41883979\n",
      "Test fold 6 \t Early stopping at Epoch [52]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 4 \t Best Val Loss: 0.39996703 \t Best Test Loss: 0.42046568\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.46851380 \t Avg Val Loss: 0.46375875 \t Avg Test Loss: 0.45819021\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.44365548 \t Avg Val Loss: 0.44879166 \t Avg Test Loss: 0.45780705\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.32092709 \t Avg Val Loss: 0.40472719 \t Avg Test Loss: 0.33034563\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.30685756 \t Avg Val Loss: 0.34244706 \t Avg Test Loss: 0.28238335\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.28199379 \t Avg Val Loss: 0.31883473 \t Avg Test Loss: 0.27600253\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.24658443 \t Avg Val Loss: 0.34726045 \t Avg Test Loss: 0.28789470\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.23242853 \t Avg Val Loss: 0.31606727 \t Avg Test Loss: 0.27792769\n",
      "Test fold 1 \t Early stopping at Epoch [127]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.29471168 \t Best Test Loss: 0.28269569\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.47220256 \t Avg Val Loss: 0.35095072 \t Avg Test Loss: 0.52211091\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.40521871 \t Avg Val Loss: 0.33080772 \t Avg Test Loss: 0.46164093\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.32880357 \t Avg Val Loss: 0.29761687 \t Avg Test Loss: 0.40537774\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.30457113 \t Avg Val Loss: 0.28050068 \t Avg Test Loss: 0.40174558\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.28770308 \t Avg Val Loss: 0.25140746 \t Avg Test Loss: 0.39369237\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.30205009 \t Avg Val Loss: 0.26539971 \t Avg Test Loss: 0.43469470\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.26110199 \t Avg Val Loss: 0.25674883 \t Avg Test Loss: 0.39934246\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.26283290 \t Avg Val Loss: 0.31073516 \t Avg Test Loss: 0.36557076\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.31499940 \t Avg Val Loss: 0.28410521 \t Avg Test Loss: 0.38685026\n",
      "Test fold 2 \t Epoch [180] \t Avg Train Loss: 0.31519133 \t Avg Val Loss: 0.30245634 \t Avg Test Loss: 0.38990791\n",
      "Test fold 2 \t Early stopping at Epoch [194]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.23306044 \t Best Test Loss: 0.38246433\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45896229 \t Avg Val Loss: 0.53715606 \t Avg Test Loss: 0.39310628\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.43485058 \t Avg Val Loss: 0.53086852 \t Avg Test Loss: 0.38574983\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.37719475 \t Avg Val Loss: 0.46904623 \t Avg Test Loss: 0.35609980\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.28365902 \t Avg Val Loss: 0.41857774 \t Avg Test Loss: 0.31554383\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.25142512 \t Avg Val Loss: 0.38084694 \t Avg Test Loss: 0.29608573\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.24243924 \t Avg Val Loss: 0.36178092 \t Avg Test Loss: 0.30358302\n",
      "Test fold 3 \t Epoch [120] \t Avg Train Loss: 0.25804570 \t Avg Val Loss: 0.37783052 \t Avg Test Loss: 0.26827476\n",
      "Test fold 3 \t Epoch [140] \t Avg Train Loss: 0.22811285 \t Avg Val Loss: 0.35714451 \t Avg Test Loss: 0.32202597\n",
      "Test fold 3 \t Epoch [160] \t Avg Train Loss: 0.22692864 \t Avg Val Loss: 0.34669821 \t Avg Test Loss: 0.28223132\n",
      "Test fold 3 \t Epoch [180] \t Avg Train Loss: 0.21527558 \t Avg Val Loss: 0.38633699 \t Avg Test Loss: 0.31122791\n",
      "Test fold 3 \t Early stopping at Epoch [198]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.34108836 \t Best Test Loss: 0.30338078\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.45742355 \t Avg Val Loss: 0.47379245 \t Avg Test Loss: 0.43432364\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.33671591 \t Avg Val Loss: 0.37144596 \t Avg Test Loss: 0.33717339\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.40864269 \t Avg Val Loss: 0.46705198 \t Avg Test Loss: 0.38570168\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.44887028 \t Avg Val Loss: 0.47807828 \t Avg Test Loss: 0.43784561\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.34676625 \t Avg Val Loss: 0.36176567 \t Avg Test Loss: 0.36037578\n",
      "Test fold 4 \t Early stopping at Epoch [88]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.30438662 \t Best Test Loss: 0.30190707\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.46320485 \t Avg Val Loss: 0.41241750 \t Avg Test Loss: 0.48680456\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.29397411 \t Avg Val Loss: 0.27265585 \t Avg Test Loss: 0.32623268\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.28702234 \t Avg Val Loss: 0.33619492 \t Avg Test Loss: 0.39349691\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.27014480 \t Avg Val Loss: 0.30140310 \t Avg Test Loss: 0.39195556\n",
      "Test fold 5 \t Early stopping at Epoch [71]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.27265585 \t Best Test Loss: 0.32623268\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.49163335 \t Avg Val Loss: 0.40236375 \t Avg Test Loss: 0.41689736\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.34877495 \t Avg Val Loss: 0.34262217 \t Avg Test Loss: 0.35880068\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.28571895 \t Avg Val Loss: 0.32519719 \t Avg Test Loss: 0.35509967\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.27076815 \t Avg Val Loss: 0.33193425 \t Avg Test Loss: 0.36095662\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.33044317 \t Avg Val Loss: 0.35810606 \t Avg Test Loss: 0.42984765\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.23588795 \t Avg Val Loss: 0.31389762 \t Avg Test Loss: 0.39524198\n",
      "Test fold 6 \t Epoch [120] \t Avg Train Loss: 0.27488803 \t Avg Val Loss: 0.33201229 \t Avg Test Loss: 0.37523809\n",
      "Test fold 6 \t Epoch [140] \t Avg Train Loss: 0.31771455 \t Avg Val Loss: 0.38641900 \t Avg Test Loss: 0.38151058\n",
      "NaN loss detected at Epoch [153], Step [1369]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 8 \t Best Val Loss: 0.30520711 \t Best Test Loss: 0.39292183\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.45736381 \t Avg Val Loss: 0.46007667 \t Avg Test Loss: 0.46968797\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.33405777 \t Avg Val Loss: 0.32672036 \t Avg Test Loss: 0.26090219\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.27498712 \t Avg Val Loss: 0.30504319 \t Avg Test Loss: 0.28075713\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.25054173 \t Avg Val Loss: 0.31970838 \t Avg Test Loss: 0.29133195\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.38404043 \t Avg Val Loss: 0.46978453 \t Avg Test Loss: 0.41450881\n",
      "Test fold 1 \t Early stopping at Epoch [82]\n",
      "Test fold 1 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.29433672 \t Best Test Loss: 0.26399720\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.47734740 \t Avg Val Loss: 0.35245589 \t Avg Test Loss: 0.52090911\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.37936250 \t Avg Val Loss: 0.35696960 \t Avg Test Loss: 0.50528428\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.19764609 \t Avg Val Loss: 0.17213972 \t Avg Test Loss: 0.23387915\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.16383486 \t Avg Val Loss: 0.18381525 \t Avg Test Loss: 0.23657079\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.18155012 \t Avg Val Loss: 0.17754447 \t Avg Test Loss: 0.25224669\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.17921946 \t Avg Val Loss: 0.18752606 \t Avg Test Loss: 0.21996940\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.14127097 \t Avg Val Loss: 0.19358795 \t Avg Test Loss: 0.27081186\n",
      "Test fold 2 \t Early stopping at Epoch [130]\n",
      "Test fold 2 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.16743896 \t Best Test Loss: 0.22425640\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.44882681 \t Avg Val Loss: 0.53940657 \t Avg Test Loss: 0.39882382\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.44513125 \t Avg Val Loss: 0.53926116 \t Avg Test Loss: 0.39484260\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.42771356 \t Avg Val Loss: 0.52570103 \t Avg Test Loss: 0.37078344\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.31503335 \t Avg Val Loss: 0.41580834 \t Avg Test Loss: 0.31494991\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.25123546 \t Avg Val Loss: 0.38760576 \t Avg Test Loss: 0.31175765\n",
      "Test fold 3 \t Epoch [100] \t Avg Train Loss: 0.22111557 \t Avg Val Loss: 0.38182238 \t Avg Test Loss: 0.33953573\n",
      "Test fold 3 \t Early stopping at Epoch [119]\n",
      "Test fold 3 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.36848282 \t Best Test Loss: 0.31546229\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.45489590 \t Avg Val Loss: 0.46454871 \t Avg Test Loss: 0.43359755\n",
      "NaN loss detected at Epoch [11], Step [211]\n",
      "Test fold 4 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.46454871 \t Best Test Loss: 0.43359755\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.45748904 \t Avg Val Loss: 0.40946550 \t Avg Test Loss: 0.48505835\n",
      "NaN loss detected at Epoch [9], Step [222]\n",
      "Test fold 5 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.39519031 \t Best Test Loss: 0.46444481\n",
      "Test fold 6 \t Epoch [  0] \t Avg Train Loss: 0.48112681 \t Avg Val Loss: 0.40098781 \t Avg Test Loss: 0.41615389\n",
      "Test fold 6 \t Epoch [ 20] \t Avg Train Loss: 0.37985486 \t Avg Val Loss: 0.36960616 \t Avg Test Loss: 0.37000829\n",
      "Test fold 6 \t Epoch [ 40] \t Avg Train Loss: 0.27864547 \t Avg Val Loss: 0.36081533 \t Avg Test Loss: 0.39543690\n",
      "Test fold 6 \t Epoch [ 60] \t Avg Train Loss: 0.23041745 \t Avg Val Loss: 0.34834851 \t Avg Test Loss: 0.35039750\n",
      "Test fold 6 \t Epoch [ 80] \t Avg Train Loss: 0.22125998 \t Avg Val Loss: 0.34001076 \t Avg Test Loss: 0.36559628\n",
      "Test fold 6 \t Epoch [100] \t Avg Train Loss: 0.18504179 \t Avg Val Loss: 0.36458484 \t Avg Test Loss: 0.37539919\n",
      "Test fold 6 \t Epoch [120] \t Avg Train Loss: 0.17872631 \t Avg Val Loss: 0.34937238 \t Avg Test Loss: 0.37840314\n",
      "Test fold 6 \t Early stopping at Epoch [129]\n",
      "Test fold 6 \t Training completed for GRU layers 1 \t Hidden size 16 \t Best Val Loss: 0.31534124 \t Best Test Loss: 0.33637011\n",
      "Test fold 1 \t Epoch [  0] \t Avg Train Loss: 0.46210069 \t Avg Val Loss: 0.46643990 \t Avg Test Loss: 0.45836690\n",
      "Test fold 1 \t Epoch [ 20] \t Avg Train Loss: 0.43223442 \t Avg Val Loss: 0.44425098 \t Avg Test Loss: 0.45454892\n",
      "Test fold 1 \t Epoch [ 40] \t Avg Train Loss: 0.42357332 \t Avg Val Loss: 0.42102991 \t Avg Test Loss: 0.44221150\n",
      "Test fold 1 \t Epoch [ 60] \t Avg Train Loss: 0.36543060 \t Avg Val Loss: 0.37364874 \t Avg Test Loss: 0.35702169\n",
      "Test fold 1 \t Epoch [ 80] \t Avg Train Loss: 0.34458974 \t Avg Val Loss: 0.37421844 \t Avg Test Loss: 0.32320669\n",
      "Test fold 1 \t Epoch [100] \t Avg Train Loss: 0.32105617 \t Avg Val Loss: 0.35451347 \t Avg Test Loss: 0.30269166\n",
      "Test fold 1 \t Epoch [120] \t Avg Train Loss: 0.30600088 \t Avg Val Loss: 0.35676175 \t Avg Test Loss: 0.30131701\n",
      "Test fold 1 \t Early stopping at Epoch [123]\n",
      "Test fold 1 \t Training completed for GRU layers 2 \t Hidden size 2 \t Best Val Loss: 0.30926354 \t Best Test Loss: 0.30420336\n",
      "Test fold 2 \t Epoch [  0] \t Avg Train Loss: 0.46013773 \t Avg Val Loss: 0.34860005 \t Avg Test Loss: 0.52575447\n",
      "Test fold 2 \t Epoch [ 20] \t Avg Train Loss: 0.37447939 \t Avg Val Loss: 0.49037463 \t Avg Test Loss: 0.54604199\n",
      "Test fold 2 \t Epoch [ 40] \t Avg Train Loss: 0.33222561 \t Avg Val Loss: 0.27973089 \t Avg Test Loss: 0.35760216\n",
      "Test fold 2 \t Epoch [ 60] \t Avg Train Loss: 0.33368538 \t Avg Val Loss: 0.27564445 \t Avg Test Loss: 0.37263168\n",
      "Test fold 2 \t Epoch [ 80] \t Avg Train Loss: 0.30157657 \t Avg Val Loss: 0.26770502 \t Avg Test Loss: 0.37605730\n",
      "Test fold 2 \t Epoch [100] \t Avg Train Loss: 0.32430158 \t Avg Val Loss: 0.29725699 \t Avg Test Loss: 0.42646520\n",
      "Test fold 2 \t Epoch [120] \t Avg Train Loss: 0.29849917 \t Avg Val Loss: 0.25985132 \t Avg Test Loss: 0.35439689\n",
      "Test fold 2 \t Epoch [140] \t Avg Train Loss: 0.29676567 \t Avg Val Loss: 0.26308014 \t Avg Test Loss: 0.36718423\n",
      "Test fold 2 \t Epoch [160] \t Avg Train Loss: 0.27859541 \t Avg Val Loss: 0.25358840 \t Avg Test Loss: 0.34428938\n",
      "Test fold 2 \t Epoch [180] \t Avg Train Loss: 0.28123996 \t Avg Val Loss: 0.24748021 \t Avg Test Loss: 0.35458147\n",
      "Test fold 2 \t Epoch [200] \t Avg Train Loss: 0.30015710 \t Avg Val Loss: 0.28346132 \t Avg Test Loss: 0.40173705\n",
      "Test fold 2 \t Early stopping at Epoch [210]\n",
      "Test fold 2 \t Training completed for GRU layers 2 \t Hidden size 2 \t Best Val Loss: 0.24364808 \t Best Test Loss: 0.34771289\n",
      "Test fold 3 \t Epoch [  0] \t Avg Train Loss: 0.45139658 \t Avg Val Loss: 0.53906224 \t Avg Test Loss: 0.39784178\n",
      "Test fold 3 \t Epoch [ 20] \t Avg Train Loss: 0.41284391 \t Avg Val Loss: 0.51636898 \t Avg Test Loss: 0.39581565\n",
      "Test fold 3 \t Epoch [ 40] \t Avg Train Loss: 0.32371699 \t Avg Val Loss: 0.39350323 \t Avg Test Loss: 0.34137958\n",
      "Test fold 3 \t Epoch [ 60] \t Avg Train Loss: 0.30284861 \t Avg Val Loss: 0.39453197 \t Avg Test Loss: 0.32491862\n",
      "Test fold 3 \t Epoch [ 80] \t Avg Train Loss: 0.27644146 \t Avg Val Loss: 0.39064207 \t Avg Test Loss: 0.33702560\n",
      "Test fold 3 \t Early stopping at Epoch [99]\n",
      "Test fold 3 \t Training completed for GRU layers 2 \t Hidden size 2 \t Best Val Loss: 0.36873288 \t Best Test Loss: 0.35752023\n",
      "Test fold 4 \t Epoch [  0] \t Avg Train Loss: 0.46717538 \t Avg Val Loss: 0.46704290 \t Avg Test Loss: 0.43421844\n",
      "Test fold 4 \t Epoch [ 20] \t Avg Train Loss: 0.37145490 \t Avg Val Loss: 0.35300985 \t Avg Test Loss: 0.29473520\n",
      "Test fold 4 \t Epoch [ 40] \t Avg Train Loss: 0.33427724 \t Avg Val Loss: 0.33027912 \t Avg Test Loss: 0.28225508\n",
      "Test fold 4 \t Epoch [ 60] \t Avg Train Loss: 0.32482905 \t Avg Val Loss: 0.30807744 \t Avg Test Loss: 0.29173207\n",
      "Test fold 4 \t Epoch [ 80] \t Avg Train Loss: 0.30775290 \t Avg Val Loss: 0.30624742 \t Avg Test Loss: 0.29299476\n",
      "Test fold 4 \t Epoch [100] \t Avg Train Loss: 0.30485302 \t Avg Val Loss: 0.33266882 \t Avg Test Loss: 0.29680767\n",
      "Test fold 4 \t Epoch [120] \t Avg Train Loss: 0.29451624 \t Avg Val Loss: 0.31555814 \t Avg Test Loss: 0.30092709\n",
      "Test fold 4 \t Epoch [140] \t Avg Train Loss: 0.29315334 \t Avg Val Loss: 0.30535560 \t Avg Test Loss: 0.29914194\n",
      "Test fold 4 \t Epoch [160] \t Avg Train Loss: 0.29865325 \t Avg Val Loss: 0.31560852 \t Avg Test Loss: 0.30141600\n",
      "Test fold 4 \t Early stopping at Epoch [174]\n",
      "Test fold 4 \t Training completed for GRU layers 2 \t Hidden size 2 \t Best Val Loss: 0.29637552 \t Best Test Loss: 0.29447903\n",
      "Test fold 5 \t Epoch [  0] \t Avg Train Loss: 0.48252547 \t Avg Val Loss: 0.41001142 \t Avg Test Loss: 0.48215110\n",
      "Test fold 5 \t Epoch [ 20] \t Avg Train Loss: 0.39923275 \t Avg Val Loss: 0.41411182 \t Avg Test Loss: 0.44048901\n",
      "Test fold 5 \t Epoch [ 40] \t Avg Train Loss: 0.34311319 \t Avg Val Loss: 0.34490694 \t Avg Test Loss: 0.43152520\n",
      "Test fold 5 \t Epoch [ 60] \t Avg Train Loss: 0.30458041 \t Avg Val Loss: 0.36911642 \t Avg Test Loss: 0.34403502\n",
      "Test fold 5 \t Epoch [ 80] \t Avg Train Loss: 0.29231008 \t Avg Val Loss: 0.32026450 \t Avg Test Loss: 0.37576782\n",
      "Test fold 5 \t Epoch [100] \t Avg Train Loss: 0.27821964 \t Avg Val Loss: 0.33070397 \t Avg Test Loss: 0.35368885\n",
      "Test fold 5 \t Epoch [120] \t Avg Train Loss: 0.26495529 \t Avg Val Loss: 0.30278112 \t Avg Test Loss: 0.35409703\n",
      "Test fold 5 \t Epoch [140] \t Avg Train Loss: 0.26013678 \t Avg Val Loss: 0.30026711 \t Avg Test Loss: 0.36324091\n"
     ]
    }
   ],
   "source": [
    "for num_layers in [1, 2]:\n",
    "    for hidden_size in [2, 4, 8, 16]:\n",
    "        for test_fold in np.unique(folds_df['fold']):\n",
    "            # Record start time\n",
    "            fold_start_time = time.time()\n",
    "\n",
    "            # Split data into training and test sets based on fold\n",
    "            train_ids = folds_df[folds_df['fold'] != test_fold]['sequenceID']\n",
    "            test_ids = folds_df[folds_df['fold'] == test_fold]['sequenceID']\n",
    "\n",
    "            # Prepare train and test sequences as tensors\n",
    "            train_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(train_ids)]\n",
    "            test_seqs = [torch.tensor(seq[1]['signal'].to_numpy(), dtype=torch.float32) for seq in seqs if seq[0] in list(test_ids)]\n",
    "\n",
    "            # Prepare target values for training and testing\n",
    "            target_df_train = target_df[target_df['sequenceID'].isin(train_ids)]\n",
    "            y_train = torch.tensor(target_df_train.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "            target_df_test = target_df[target_df['sequenceID'].isin(test_ids)]\n",
    "            y_test = torch.tensor(target_df_test.iloc[:, 1:].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "            # Split training data into subtrain and validation sets\n",
    "            train_seqs, val_seqs, y_train, y_val = train_test_split(train_seqs, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Initialize the GRU model, loss function, and optimizer\n",
    "            model = LSTMModel(input_size, hidden_size, num_layers).to(device)    # Move model to device\n",
    "            criterion = SquaredHingeLoss().to(device)                           # Move loss function to device\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            # Variables for early stopping\n",
    "            best_train_loss = float('inf')    # Best training loss initialized to infinity\n",
    "            best_val_loss = float('inf')      # Best validation loss initialized to infinity\n",
    "            best_test_loss = float('inf')     # Best test loss corresponding to best validation\n",
    "            patience_counter = 0              # Early stopping patience counter\n",
    "            best_model_state = None           # Store the best model parameters\n",
    "            stop_epoch = 0                    # Epoch when training stops\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(max_epochs):\n",
    "                # Shuffle training sequences and targets\n",
    "                combined = list(zip(train_seqs, y_train))\n",
    "                random.shuffle(combined)\n",
    "                train_seqs, y_train = zip(*combined)\n",
    "\n",
    "                total_train_loss = 0\n",
    "                nan_flag = False  # Flag to detect NaN loss\n",
    "\n",
    "                # Train on subtrain data\n",
    "                model.train()  # Set model to training mode\n",
    "                for i, seq_input in enumerate(train_seqs):\n",
    "                    target = y_train[i].unsqueeze(0).to(device)  # Prepare target and move to device\n",
    "\n",
    "                    optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "                    # Forward pass\n",
    "                    seq_input = seq_input.unsqueeze(0).unsqueeze(-1).to(device) # Prepare input and move to device\n",
    "                    output_seq = model(seq_input)                               # Get model output\n",
    "                    loss = criterion(output_seq, target.unsqueeze(-1))          # Compute loss\n",
    "\n",
    "                    if torch.isnan(loss).any():  # Check for NaN loss\n",
    "                        print(f\"NaN loss detected at Epoch [{epoch}], Step [{i}]\")\n",
    "                        nan_flag = True\n",
    "                        break\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "                if nan_flag:\n",
    "                    break  # Stop training if NaN was encountered\n",
    "\n",
    "                # Calculate average training loss\n",
    "                avg_train_loss = total_train_loss / len(train_seqs)\n",
    "\n",
    "                # Calculate validation and test losses\n",
    "                avg_val_loss = get_loss_value(model, val_seqs, y_val, criterion)\n",
    "                avg_test_loss = get_loss_value(model, test_seqs, y_test, criterion)\n",
    "\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f'Test fold {test_fold} \\t Epoch [{epoch:3d}] \\t Avg Train Loss: {avg_train_loss:.8f} \\t Avg Val Loss: {avg_val_loss:.8f} \\t Avg Test Loss: {avg_test_loss:.8f}')\n",
    "\n",
    "                # Early stopping based on validation loss\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss        # Update best validation loss\n",
    "                    best_train_loss = avg_train_loss    # Store best training loss\n",
    "                    best_test_loss = avg_test_loss      # Store test loss for best validation\n",
    "                    patience_counter = 0                # Reset patience counter\n",
    "\n",
    "                    # Save best model parameters\n",
    "                    best_model_state = model.state_dict()\n",
    "                    stop_epoch = epoch + 1              # Record stopping epoch\n",
    "                else:\n",
    "                    patience_counter += 1               # Increment patience counter\n",
    "\n",
    "                # Stop training if patience is exceeded\n",
    "                if patience_counter > patience:\n",
    "                    print(f\"Test fold {test_fold} \\t Early stopping at Epoch [{epoch}]\")\n",
    "                    break\n",
    "\n",
    "            # Record total time taken for this fold\n",
    "            fold_duration = time.time() - fold_start_time\n",
    "\n",
    "            # Save results to CSV\n",
    "            report_entry = {\n",
    "                'dataset': dataset,\n",
    "                'model': model_type,\n",
    "                'num_layers': num_layers,\n",
    "                'hidden_size': hidden_size,\n",
    "                'test_fold': test_fold,\n",
    "                'stop_epoch': stop_epoch,\n",
    "                'train_loss': best_train_loss,\n",
    "                'val_loss': best_val_loss,\n",
    "                'test_loss': best_test_loss,\n",
    "                'time': fold_duration\n",
    "            }\n",
    "\n",
    "            pd.DataFrame([report_entry]).to_csv(report_path, mode='a', header=False, index=False)  # Append entry to CSV\n",
    "\n",
    "            print(f\"Test fold {test_fold} \\t Training completed for GRU layers {num_layers} \\t Hidden size {hidden_size} \\t Best Val Loss: {best_val_loss:.8f} \\t Best Test Loss: {best_test_loss:.8f}\")\n",
    "            \n",
    "            # Restore best model parameters after training\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "            # Test the model and collect outputs\n",
    "            pred_lldas = test_model(model, test_seqs)\n",
    "\n",
    "            # Save model parameters\n",
    "            torch.save(model.state_dict(), f'saved_models/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.pth')\n",
    "\n",
    "            # Save predictions to CSV\n",
    "            lldas_df = pd.DataFrame(list(zip(test_ids, pred_lldas)), columns=['sequenceID', 'llda'])\n",
    "            lldas_df.to_csv(f'predictions/{model_type}_{dataset}_{num_layers}layers_{hidden_size}features_fold{test_fold}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
