{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x266712afcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.utility_functions import gen_data_dict, get_data, get_cumsum, error_count, write_to_csv, opart, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "seqs = gen_data_dict('sequence_label_data/signals.gz')\n",
    "\n",
    "# target \n",
    "target_df_1 = pd.read_csv('learning_data/target_lambda_fold1.csv')\n",
    "target_df_2 = pd.read_csv('learning_data/target_lambda_fold2.csv')\n",
    "\n",
    "targets_low_1  = target_df_1.iloc[:, 1:2].to_numpy()\n",
    "targets_high_1 = target_df_1.iloc[:, 2:3].to_numpy()\n",
    "targets_low_2  = target_df_2.iloc[:, 1:2].to_numpy()\n",
    "targets_high_2 = target_df_2.iloc[:, 2:3].to_numpy()\n",
    "\n",
    "targets_low_1  = torch.FloatTensor(targets_low_1)\n",
    "targets_high_1 = torch.FloatTensor(targets_high_1)\n",
    "targets_low_2  = torch.FloatTensor(targets_low_2)\n",
    "targets_high_2 = torch.FloatTensor(targets_high_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=8, num_layers=1, nonlinearity='relu', bias=True, batch_first=False)\n",
    "        self.fc1 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x = self.rnn(x)\n",
    "        x    = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, Loss_1: 9654.5010, Loss_1_test: 9578.5010, Loss_2: 9652.5215, Loss_2_test: 9728.9961\n",
      "Epoch     1, Loss_1: 9225.3545, Loss_1_test: 9152.0947, Loss_2: 9351.2129, Loss_2_test: 9425.7529\n",
      "Epoch     2, Loss_1: 8806.4150, Loss_1_test: 8735.8477, Loss_2: 9052.2520, Loss_2_test: 9124.8672\n",
      "Epoch     3, Loss_1: 8396.0195, Loss_1_test: 8328.1025, Loss_2: 8755.9150, Loss_2_test: 8826.6133\n",
      "Epoch     4, Loss_1: 7995.4492, Loss_1_test: 7930.1367, Loss_2: 8462.1855, Loss_2_test: 8530.9766\n",
      "Epoch     5, Loss_1: 7603.5156, Loss_1_test: 7540.7656, Loss_2: 8171.3125, Loss_2_test: 8238.2061\n",
      "Epoch     6, Loss_1: 7219.9746, Loss_1_test: 7159.7495, Loss_2: 7883.3311, Loss_2_test: 7948.3379\n",
      "Epoch     7, Loss_1: 6845.1875, Loss_1_test: 6787.4473, Loss_2: 7598.5225, Loss_2_test: 7661.6538\n",
      "Epoch     8, Loss_1: 6478.4014, Loss_1_test: 6423.1089, Loss_2: 7317.1768, Loss_2_test: 7378.4468\n",
      "Epoch     9, Loss_1: 6118.4492, Loss_1_test: 6065.5801, Loss_2: 7039.2290, Loss_2_test: 7098.6519\n",
      "Epoch    10, Loss_1: 5765.4189, Loss_1_test: 5714.9448, Loss_2: 6764.8262, Loss_2_test: 6822.4160\n",
      "Epoch    11, Loss_1: 5420.1167, Loss_1_test: 5372.0049, Loss_2: 6493.8286, Loss_2_test: 6549.5962\n",
      "Epoch    12, Loss_1: 5082.1475, Loss_1_test: 5036.3701, Loss_2: 6226.5464, Loss_2_test: 6280.5098\n",
      "Epoch    13, Loss_1: 4751.7129, Loss_1_test: 4708.2397, Loss_2: 5963.0576, Loss_2_test: 6015.2295\n",
      "Epoch    14, Loss_1: 4429.1123, Loss_1_test: 4387.9126, Loss_2: 5703.1870, Loss_2_test: 5753.5811\n",
      "Epoch    15, Loss_1: 4114.6567, Loss_1_test: 4075.6992, Loss_2: 5447.1875, Loss_2_test: 5495.8188\n",
      "Epoch    16, Loss_1: 3808.9233, Loss_1_test: 3772.1716, Loss_2: 5195.2173, Loss_2_test: 5242.1025\n",
      "Epoch    17, Loss_1: 3512.4751, Loss_1_test: 3477.8904, Loss_2: 4947.3906, Loss_2_test: 4992.5459\n",
      "Epoch    18, Loss_1: 3225.8472, Loss_1_test: 3193.3867, Loss_2: 4703.7056, Loss_2_test: 4747.1465\n",
      "Epoch    19, Loss_1: 2948.5579, Loss_1_test: 2918.1848, Loss_2: 4464.1787, Loss_2_test: 4505.9214\n",
      "Epoch    20, Loss_1: 2681.8208, Loss_1_test: 2653.4890, Loss_2: 4229.1792, Loss_2_test: 4269.2417\n",
      "Epoch    21, Loss_1: 2425.8389, Loss_1_test: 2399.5017, Loss_2: 3999.0256, Loss_2_test: 4037.4285\n",
      "Epoch    22, Loss_1: 2180.5642, Loss_1_test: 2156.1765, Loss_2: 3773.8447, Loss_2_test: 3810.6094\n",
      "Epoch    23, Loss_1: 1945.9067, Loss_1_test: 1923.4253, Loss_2: 3553.7490, Loss_2_test: 3588.8953\n",
      "Epoch    24, Loss_1: 1722.7709, Loss_1_test: 1702.1454, Loss_2: 3338.8826, Loss_2_test: 3372.4333\n",
      "Epoch    25, Loss_1: 1511.8518, Loss_1_test: 1493.0284, Loss_2: 3129.4529, Loss_2_test: 3161.4309\n",
      "Epoch    26, Loss_1: 1313.5687, Loss_1_test: 1296.4907, Loss_2: 2925.7336, Loss_2_test: 2956.1641\n",
      "Epoch    27, Loss_1: 1128.3423, Loss_1_test: 1112.9498, Loss_2: 2727.6428, Loss_2_test: 2756.5496\n",
      "Epoch    28, Loss_1: 956.8209, Loss_1_test: 943.0492, Loss_2: 2535.5217, Loss_2_test: 2562.9316\n",
      "Epoch    29, Loss_1: 799.4832, Loss_1_test: 787.2629, Loss_2: 2349.6509, Loss_2_test: 2375.5916\n",
      "Epoch    30, Loss_1: 656.6526, Loss_1_test: 645.9111, Loss_2: 2170.0098, Loss_2_test: 2194.5090\n",
      "Epoch    31, Loss_1: 528.4169, Loss_1_test: 519.0795, Loss_2: 1996.8470, Loss_2_test: 2019.9349\n",
      "Epoch    32, Loss_1: 415.0821, Loss_1_test: 407.0694, Loss_2: 1830.5425, Loss_2_test: 1852.2506\n",
      "Epoch    33, Loss_1: 316.6838, Loss_1_test: 309.9127, Loss_2: 1671.1920, Loss_2_test: 1691.5537\n",
      "Epoch    34, Loss_1: 232.9464, Loss_1_test: 227.3323, Loss_2: 1519.0493, Loss_2_test: 1538.0996\n",
      "Epoch    35, Loss_1: 163.4632, Loss_1_test: 158.9207, Loss_2: 1373.9418, Loss_2_test: 1391.7142\n",
      "Epoch    36, Loss_1: 107.6908, Loss_1_test: 104.1330, Loss_2: 1236.0873, Loss_2_test: 1252.6171\n",
      "Epoch    37, Loss_1:  64.8460, Loss_1_test:  62.1864, Loss_2: 1105.7589, Loss_2_test: 1121.0839\n",
      "Epoch    38, Loss_1:  33.9700, Loss_1_test:  32.1223, Loss_2: 983.0685, Loss_2_test: 997.2279\n",
      "Epoch    39, Loss_1:  13.8727, Loss_1_test:  12.7532, Loss_2: 867.8307, Loss_2_test: 880.8621\n",
      "Epoch    40, Loss_1:   3.3418, Loss_1_test:   2.9027, Loss_2: 760.0673, Loss_2_test: 772.0090\n",
      "Epoch    41, Loss_1:   1.3195, Loss_1_test:   2.0936, Loss_2: 660.1610, Loss_2_test: 671.0552\n",
      "Epoch    42, Loss_1:   4.1558, Loss_1_test:   7.6689, Loss_2: 567.9767, Loss_2_test: 577.8657\n",
      "Epoch    43, Loss_1:   9.3752, Loss_1_test:  16.8179, Loss_2: 483.3128, Loss_2_test: 492.2372\n",
      "Epoch    44, Loss_1:  15.9640, Loss_1_test:  28.2090, Loss_2: 406.0714, Loss_2_test: 414.0718\n",
      "Epoch    45, Loss_1:  23.3473, Loss_1_test:  40.8770, Loss_2: 336.2674, Loss_2_test: 343.3860\n",
      "Epoch    46, Loss_1:  31.0288, Loss_1_test:  53.9969, Loss_2: 273.8223, Loss_2_test: 280.1019\n",
      "Epoch    47, Loss_1:  38.5826, Loss_1_test:  66.8601, Loss_2: 218.6773, Loss_2_test: 224.1627\n",
      "Epoch    48, Loss_1:  45.6593, Loss_1_test:  78.8861, Loss_2: 170.4836, Loss_2_test: 175.2171\n",
      "Epoch    49, Loss_1:  51.9885, Loss_1_test:  89.6260, Loss_2: 129.0259, Loss_2_test: 133.0500\n",
      "Epoch    50, Loss_1:  57.3706, Loss_1_test:  98.7494, Loss_2:  93.9316, Loss_2_test:  97.2863\n",
      "Epoch    51, Loss_1:  61.6767, Loss_1_test: 106.0432, Loss_2:  65.0148, Loss_2_test:  67.7414\n",
      "Epoch    52, Loss_1:  64.8364, Loss_1_test: 111.3925, Loss_2:  42.0269, Loss_2_test:  44.1677\n",
      "Epoch    53, Loss_1:  66.8352, Loss_1_test: 114.7753, Loss_2:  24.5361, Loss_2_test:  26.1318\n",
      "Epoch    54, Loss_1:  67.7058, Loss_1_test: 116.2485, Loss_2:  12.1844, Loss_2_test:  13.2765\n",
      "Epoch    55, Loss_1:  67.5187, Loss_1_test: 115.9318, Loss_2:   4.5276, Loss_2_test:   5.1624\n",
      "Epoch    56, Loss_1:  66.3720, Loss_1_test: 113.9914, Loss_2:   1.6178, Loss_2_test:   1.5735\n",
      "Epoch    57, Loss_1:  64.3841, Loss_1_test: 110.6269, Loss_2:   2.5674, Loss_2_test:   1.5018\n",
      "Epoch    58, Loss_1:  61.6830, Loss_1_test: 106.0538, Loss_2:   6.3029, Loss_2_test:   3.4001\n",
      "Epoch    59, Loss_1:  58.4068, Loss_1_test: 100.5049, Loss_2:  11.4286, Loss_2_test:   6.2892\n",
      "Epoch    60, Loss_1:  54.6883, Loss_1_test:  94.2035, Loss_2:  17.1532, Loss_2_test:   9.5680\n",
      "Epoch    61, Loss_1:  50.6544, Loss_1_test:  87.3633, Loss_2:  22.8191, Loss_2_test:  12.8382\n",
      "Epoch    62, Loss_1:  46.4263, Loss_1_test:  80.1884, Loss_2:  27.8929, Loss_2_test:  15.7804\n",
      "Epoch    63, Loss_1:  42.1146, Loss_1_test:  72.8649, Loss_2:  31.9926, Loss_2_test:  18.1646\n",
      "Epoch    64, Loss_1:  37.8224, Loss_1_test:  65.5669, Loss_2:  34.9114, Loss_2_test:  19.8652\n",
      "Epoch    65, Loss_1:  33.6263, Loss_1_test:  58.4237, Loss_2:  36.5880, Loss_2_test:  20.8430\n",
      "Epoch    66, Loss_1:  29.5910, Loss_1_test:  51.5446, Loss_2:  37.0938, Loss_2_test:  21.1381\n",
      "Epoch    67, Loss_1:  25.7720, Loss_1_test:  45.0237, Loss_2:  36.5666, Loss_2_test:  20.8305\n",
      "Epoch    68, Loss_1:  22.2090, Loss_1_test:  38.9283, Loss_2:  35.1762, Loss_2_test:  20.0195\n",
      "Epoch    69, Loss_1:  18.9269, Loss_1_test:  33.3014, Loss_2:  33.1298, Loss_2_test:  18.8269\n",
      "Epoch    70, Loss_1:  15.9449, Loss_1_test:  28.1761, Loss_2:  30.6485, Loss_2_test:  17.3823\n",
      "Epoch    71, Loss_1:  13.2696, Loss_1_test:  23.5641, Loss_2:  27.9153, Loss_2_test:  15.7934\n",
      "Epoch    72, Loss_1:  10.8992, Loss_1_test:  19.4637, Loss_2:  25.0721, Loss_2_test:  14.1434\n",
      "Epoch    73, Loss_1:   8.8272, Loss_1_test:  15.8644, Loss_2:  22.2017, Loss_2_test:  12.4810\n",
      "Epoch    74, Loss_1:   7.0402, Loss_1_test:  12.7449, Loss_2:  19.3972, Loss_2_test:  10.8609\n",
      "Epoch    75, Loss_1:   5.5213, Loss_1_test:  10.0789, Loss_2:  16.7433, Loss_2_test:   9.3322\n",
      "Epoch    76, Loss_1:   4.2512, Loss_1_test:   7.8379, Loss_2:  14.2828, Loss_2_test:   7.9200\n",
      "Epoch    77, Loss_1:   3.2249, Loss_1_test:   5.9843, Loss_2:  12.0249, Loss_2_test:   6.6291\n",
      "Epoch    78, Loss_1:   2.4299, Loss_1_test:   4.4965, Loss_2:   9.9814, Loss_2_test:   5.4658\n",
      "Epoch    79, Loss_1:   1.8535, Loss_1_test:   3.3412, Loss_2:   8.1792, Loss_2_test:   4.4439\n",
      "Epoch    80, Loss_1:   1.4614, Loss_1_test:   2.4769, Loss_2:   6.6126, Loss_2_test:   3.5709\n",
      "Epoch    81, Loss_1:   1.2608, Loss_1_test:   1.9207, Loss_2:   5.2847, Loss_2_test:   2.8454\n",
      "Epoch    82, Loss_1:   1.1983, Loss_1_test:   1.6085, Loss_2:   4.1820, Loss_2_test:   2.2651\n",
      "Epoch    83, Loss_1:   1.2766, Loss_1_test:   1.5130, Loss_2:   3.2866, Loss_2_test:   1.8285\n",
      "Epoch    84, Loss_1:   1.4461, Loss_1_test:   1.5499, Loss_2:   2.5802, Loss_2_test:   1.5075\n",
      "Epoch    85, Loss_1:   1.6835, Loss_1_test:   1.6792, Loss_2:   2.0829, Loss_2_test:   1.3157\n",
      "Epoch    86, Loss_1:   1.9291, Loss_1_test:   1.8223, Loss_2:   1.7165, Loss_2_test:   1.1981\n",
      "Epoch    87, Loss_1:   2.2144, Loss_1_test:   2.0318, Loss_2:   1.5715, Loss_2_test:   1.2146\n",
      "Epoch    88, Loss_1:   2.4636, Loss_1_test:   2.2162, Loss_2:   1.5079, Loss_2_test:   1.2875\n",
      "Epoch    89, Loss_1:   2.6614, Loss_1_test:   2.3628, Loss_2:   1.5384, Loss_2_test:   1.4238\n",
      "Epoch    90, Loss_1:   2.7999, Loss_1_test:   2.4656, Loss_2:   1.6371, Loss_2_test:   1.6085\n",
      "Epoch    91, Loss_1:   2.8760, Loss_1_test:   2.5220, Loss_2:   1.7468, Loss_2_test:   1.8010\n",
      "Epoch    92, Loss_1:   2.8910, Loss_1_test:   2.5332, Loss_2:   1.8767, Loss_2_test:   2.0042\n",
      "Epoch    93, Loss_1:   2.8494, Loss_1_test:   2.5023, Loss_2:   2.0361, Loss_2_test:   2.2202\n",
      "Epoch    94, Loss_1:   2.7586, Loss_1_test:   2.4349, Loss_2:   2.1771, Loss_2_test:   2.4108\n",
      "Epoch    95, Loss_1:   2.6275, Loss_1_test:   2.3376, Loss_2:   2.2959, Loss_2_test:   2.5712\n",
      "Epoch    96, Loss_1:   2.4657, Loss_1_test:   2.2178, Loss_2:   2.3905, Loss_2_test:   2.6988\n",
      "Epoch    97, Loss_1:   2.2834, Loss_1_test:   2.0829, Loss_2:   2.4594, Loss_2_test:   2.7916\n",
      "Epoch    98, Loss_1:   2.0903, Loss_1_test:   1.9402, Loss_2:   2.5031, Loss_2_test:   2.8504\n",
      "Epoch    99, Loss_1:   1.9027, Loss_1_test:   1.8066, Loss_2:   2.5226, Loss_2_test:   2.8767\n",
      "Epoch   100, Loss_1:   1.7517, Loss_1_test:   1.7182, Loss_2:   2.5194, Loss_2_test:   2.8725\n",
      "Epoch   101, Loss_1:   1.6128, Loss_1_test:   1.6395, Loss_2:   2.4958, Loss_2_test:   2.8406\n",
      "Epoch   102, Loss_1:   1.4882, Loss_1_test:   1.5719, Loss_2:   2.4542, Loss_2_test:   2.7846\n",
      "Epoch   103, Loss_1:   1.3795, Loss_1_test:   1.5162, Loss_2:   2.3974, Loss_2_test:   2.7080\n",
      "Epoch   104, Loss_1:   1.3051, Loss_1_test:   1.5015, Loss_2:   2.3281, Loss_2_test:   2.6146\n",
      "Epoch   105, Loss_1:   1.2606, Loss_1_test:   1.5223, Loss_2:   2.2492, Loss_2_test:   2.5081\n",
      "Epoch   106, Loss_1:   1.2282, Loss_1_test:   1.5517, Loss_2:   2.1631, Loss_2_test:   2.3919\n",
      "Epoch   107, Loss_1:   1.2061, Loss_1_test:   1.5879, Loss_2:   2.0724, Loss_2_test:   2.2693\n",
      "Epoch   108, Loss_1:   1.1928, Loss_1_test:   1.6289, Loss_2:   1.9794, Loss_2_test:   2.1434\n",
      "Epoch   109, Loss_1:   1.1866, Loss_1_test:   1.6729, Loss_2:   1.8860, Loss_2_test:   2.0169\n",
      "Epoch   110, Loss_1:   1.2109, Loss_1_test:   1.7605, Loss_2:   1.8050, Loss_2_test:   1.9001\n",
      "Epoch   111, Loss_1:   1.2356, Loss_1_test:   1.8417, Loss_2:   1.7490, Loss_2_test:   1.8049\n",
      "Epoch   112, Loss_1:   1.2583, Loss_1_test:   1.9127, Loss_2:   1.6982, Loss_2_test:   1.7168\n",
      "Epoch   113, Loss_1:   1.2780, Loss_1_test:   1.9724, Loss_2:   1.6524, Loss_2_test:   1.6360\n",
      "Epoch   114, Loss_1:   1.2941, Loss_1_test:   2.0203, Loss_2:   1.6117, Loss_2_test:   1.5622\n",
      "Epoch   115, Loss_1:   1.3064, Loss_1_test:   2.0560, Loss_2:   1.5757, Loss_2_test:   1.4954\n",
      "Epoch   116, Loss_1:   1.3147, Loss_1_test:   2.0798, Loss_2:   1.5443, Loss_2_test:   1.4352\n",
      "Epoch   117, Loss_1:   1.3190, Loss_1_test:   2.0923, Loss_2:   1.5172, Loss_2_test:   1.3814\n",
      "Epoch   118, Loss_1:   1.3197, Loss_1_test:   2.0942, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   119, Loss_1:   1.3169, Loss_1_test:   2.0863, Loss_2:   1.5003, Loss_2_test:   1.3092\n",
      "Epoch   120, Loss_1:   1.3112, Loss_1_test:   2.0699, Loss_2:   1.5072, Loss_2_test:   1.2891\n",
      "Epoch   121, Loss_1:   1.3029, Loss_1_test:   2.0459, Loss_2:   1.5148, Loss_2_test:   1.2730\n",
      "Epoch   122, Loss_1:   1.2925, Loss_1_test:   2.0157, Loss_2:   1.5225, Loss_2_test:   1.2602\n",
      "Epoch   123, Loss_1:   1.2805, Loss_1_test:   1.9802, Loss_2:   1.5299, Loss_2_test:   1.2501\n",
      "Epoch   124, Loss_1:   1.2674, Loss_1_test:   1.9406, Loss_2:   1.5367, Loss_2_test:   1.2423\n",
      "Epoch   125, Loss_1:   1.2535, Loss_1_test:   1.8981, Loss_2:   1.5426, Loss_2_test:   1.2363\n",
      "Epoch   126, Loss_1:   1.2393, Loss_1_test:   1.8535, Loss_2:   1.5474, Loss_2_test:   1.2319\n",
      "Epoch   127, Loss_1:   1.2251, Loss_1_test:   1.8078, Loss_2:   1.5511, Loss_2_test:   1.2287\n",
      "Epoch   128, Loss_1:   1.2112, Loss_1_test:   1.7617, Loss_2:   1.5537, Loss_2_test:   1.2266\n",
      "Epoch   129, Loss_1:   1.1979, Loss_1_test:   1.7160, Loss_2:   1.5551, Loss_2_test:   1.2255\n",
      "Epoch   130, Loss_1:   1.1866, Loss_1_test:   1.6731, Loss_2:   1.5555, Loss_2_test:   1.2252\n",
      "Epoch   131, Loss_1:   1.1884, Loss_1_test:   1.6542, Loss_2:   1.5549, Loss_2_test:   1.2257\n",
      "Epoch   132, Loss_1:   1.1909, Loss_1_test:   1.6385, Loss_2:   1.5535, Loss_2_test:   1.2268\n",
      "Epoch   133, Loss_1:   1.1936, Loss_1_test:   1.6255, Loss_2:   1.5512, Loss_2_test:   1.2286\n",
      "Epoch   134, Loss_1:   1.1963, Loss_1_test:   1.6151, Loss_2:   1.5484, Loss_2_test:   1.2310\n",
      "Epoch   135, Loss_1:   1.1989, Loss_1_test:   1.6067, Loss_2:   1.5451, Loss_2_test:   1.2339\n",
      "Epoch   136, Loss_1:   1.2011, Loss_1_test:   1.6003, Loss_2:   1.5414, Loss_2_test:   1.2375\n",
      "Epoch   137, Loss_1:   1.2029, Loss_1_test:   1.5955, Loss_2:   1.5374, Loss_2_test:   1.2415\n",
      "Epoch   138, Loss_1:   1.2043, Loss_1_test:   1.5922, Loss_2:   1.5333, Loss_2_test:   1.2461\n",
      "Epoch   139, Loss_1:   1.2051, Loss_1_test:   1.5902, Loss_2:   1.5291, Loss_2_test:   1.2512\n",
      "Epoch   140, Loss_1:   1.2055, Loss_1_test:   1.5894, Loss_2:   1.5250, Loss_2_test:   1.2567\n",
      "Epoch   141, Loss_1:   1.2054, Loss_1_test:   1.5897, Loss_2:   1.5210, Loss_2_test:   1.2626\n",
      "Epoch   142, Loss_1:   1.2048, Loss_1_test:   1.5909, Loss_2:   1.5172, Loss_2_test:   1.2689\n",
      "Epoch   143, Loss_1:   1.2039, Loss_1_test:   1.5930, Loss_2:   1.5135, Loss_2_test:   1.2755\n",
      "Epoch   144, Loss_1:   1.2028, Loss_1_test:   1.5960, Loss_2:   1.5102, Loss_2_test:   1.2824\n",
      "Epoch   145, Loss_1:   1.2014, Loss_1_test:   1.5996, Loss_2:   1.5071, Loss_2_test:   1.2895\n",
      "Epoch   146, Loss_1:   1.1998, Loss_1_test:   1.6040, Loss_2:   1.5043, Loss_2_test:   1.2967\n",
      "Epoch   147, Loss_1:   1.1982, Loss_1_test:   1.6089, Loss_2:   1.5018, Loss_2_test:   1.3041\n",
      "Epoch   148, Loss_1:   1.1965, Loss_1_test:   1.6143, Loss_2:   1.4996, Loss_2_test:   1.3116\n",
      "Epoch   149, Loss_1:   1.1949, Loss_1_test:   1.6202, Loss_2:   1.4977, Loss_2_test:   1.3190\n",
      "Epoch   150, Loss_1:   1.1934, Loss_1_test:   1.6265, Loss_2:   1.4961, Loss_2_test:   1.3264\n",
      "Epoch   151, Loss_1:   1.1919, Loss_1_test:   1.6330, Loss_2:   1.4947, Loss_2_test:   1.3336\n",
      "Epoch   152, Loss_1:   1.1906, Loss_1_test:   1.6398, Loss_2:   1.4991, Loss_2_test:   1.3442\n",
      "Epoch   153, Loss_1:   1.1895, Loss_1_test:   1.6467, Loss_2:   1.5027, Loss_2_test:   1.3517\n",
      "Epoch   154, Loss_1:   1.1885, Loss_1_test:   1.6536, Loss_2:   1.5046, Loss_2_test:   1.3557\n",
      "Epoch   155, Loss_1:   1.1877, Loss_1_test:   1.6606, Loss_2:   1.5049, Loss_2_test:   1.3565\n",
      "Epoch   156, Loss_1:   1.1870, Loss_1_test:   1.6674, Loss_2:   1.5038, Loss_2_test:   1.3542\n",
      "Epoch   157, Loss_1:   1.1865, Loss_1_test:   1.6741, Loss_2:   1.5015, Loss_2_test:   1.3492\n",
      "Epoch   158, Loss_1:   1.1896, Loss_1_test:   1.6862, Loss_2:   1.4980, Loss_2_test:   1.3420\n",
      "Epoch   159, Loss_1:   1.1914, Loss_1_test:   1.6928, Loss_2:   1.4948, Loss_2_test:   1.3335\n",
      "Epoch   160, Loss_1:   1.1916, Loss_1_test:   1.6934, Loss_2:   1.4956, Loss_2_test:   1.3287\n",
      "Epoch   161, Loss_1:   1.1902, Loss_1_test:   1.6887, Loss_2:   1.4964, Loss_2_test:   1.3248\n",
      "Epoch   162, Loss_1:   1.1876, Loss_1_test:   1.6791, Loss_2:   1.4971, Loss_2_test:   1.3219\n",
      "Epoch   163, Loss_1:   1.1868, Loss_1_test:   1.6702, Loss_2:   1.4976, Loss_2_test:   1.3197\n",
      "Epoch   164, Loss_1:   1.1873, Loss_1_test:   1.6646, Loss_2:   1.4979, Loss_2_test:   1.3182\n",
      "Epoch   165, Loss_1:   1.1877, Loss_1_test:   1.6599, Loss_2:   1.4981, Loss_2_test:   1.3173\n",
      "Epoch   166, Loss_1:   1.1882, Loss_1_test:   1.6563, Loss_2:   1.4982, Loss_2_test:   1.3171\n",
      "Epoch   167, Loss_1:   1.1885, Loss_1_test:   1.6535, Loss_2:   1.4981, Loss_2_test:   1.3173\n",
      "Epoch   168, Loss_1:   1.1888, Loss_1_test:   1.6515, Loss_2:   1.4979, Loss_2_test:   1.3181\n",
      "Epoch   169, Loss_1:   1.1889, Loss_1_test:   1.6503, Loss_2:   1.4976, Loss_2_test:   1.3193\n",
      "Epoch   170, Loss_1:   1.1890, Loss_1_test:   1.6498, Loss_2:   1.4973, Loss_2_test:   1.3208\n",
      "Epoch   171, Loss_1:   1.1890, Loss_1_test:   1.6500, Loss_2:   1.4969, Loss_2_test:   1.3228\n",
      "Epoch   172, Loss_1:   1.1889, Loss_1_test:   1.6506, Loss_2:   1.4964, Loss_2_test:   1.3250\n",
      "Epoch   173, Loss_1:   1.1887, Loss_1_test:   1.6519, Loss_2:   1.4959, Loss_2_test:   1.3275\n",
      "Epoch   174, Loss_1:   1.1885, Loss_1_test:   1.6535, Loss_2:   1.4954, Loss_2_test:   1.3302\n",
      "Epoch   175, Loss_1:   1.1883, Loss_1_test:   1.6556, Loss_2:   1.4948, Loss_2_test:   1.3331\n",
      "Epoch   176, Loss_1:   1.1880, Loss_1_test:   1.6580, Loss_2:   1.4956, Loss_2_test:   1.3370\n",
      "Epoch   177, Loss_1:   1.1877, Loss_1_test:   1.6606, Loss_2:   1.4964, Loss_2_test:   1.3385\n",
      "Epoch   178, Loss_1:   1.1874, Loss_1_test:   1.6636, Loss_2:   1.4956, Loss_2_test:   1.3370\n",
      "Epoch   179, Loss_1:   1.1871, Loss_1_test:   1.6667, Loss_2:   1.4948, Loss_2_test:   1.3335\n",
      "Epoch   180, Loss_1:   1.1868, Loss_1_test:   1.6699, Loss_2:   1.4951, Loss_2_test:   1.3315\n",
      "Epoch   181, Loss_1:   1.1866, Loss_1_test:   1.6733, Loss_2:   1.4954, Loss_2_test:   1.3301\n",
      "Epoch   182, Loss_1:   1.1874, Loss_1_test:   1.6784, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   183, Loss_1:   1.1876, Loss_1_test:   1.6790, Loss_2:   1.4956, Loss_2_test:   1.3290\n",
      "Epoch   184, Loss_1:   1.1865, Loss_1_test:   1.6745, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   185, Loss_1:   1.1866, Loss_1_test:   1.6726, Loss_2:   1.4954, Loss_2_test:   1.3299\n",
      "Epoch   186, Loss_1:   1.1867, Loss_1_test:   1.6713, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   187, Loss_1:   1.1868, Loss_1_test:   1.6705, Loss_2:   1.4950, Loss_2_test:   1.3323\n",
      "Epoch   188, Loss_1:   1.1868, Loss_1_test:   1.6701, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   189, Loss_1:   1.1868, Loss_1_test:   1.6702, Loss_2:   1.4954, Loss_2_test:   1.3366\n",
      "Epoch   190, Loss_1:   1.1868, Loss_1_test:   1.6706, Loss_2:   1.4953, Loss_2_test:   1.3364\n",
      "Epoch   191, Loss_1:   1.1867, Loss_1_test:   1.6713, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   192, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4950, Loss_2_test:   1.3324\n",
      "Epoch   193, Loss_1:   1.1865, Loss_1_test:   1.6737, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   194, Loss_1:   1.1866, Loss_1_test:   1.6756, Loss_2:   1.4951, Loss_2_test:   1.3313\n",
      "Epoch   195, Loss_1:   1.1865, Loss_1_test:   1.6739, Loss_2:   1.4951, Loss_2_test:   1.3315\n",
      "Epoch   196, Loss_1:   1.1866, Loss_1_test:   1.6729, Loss_2:   1.4950, Loss_2_test:   1.3322\n",
      "Epoch   197, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4948, Loss_2_test:   1.3332\n",
      "Epoch   198, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4946, Loss_2_test:   1.3345\n",
      "Epoch   199, Loss_1:   1.1866, Loss_1_test:   1.6727, Loss_2:   1.4957, Loss_2_test:   1.3371\n",
      "Epoch   200, Loss_1:   1.1865, Loss_1_test:   1.6733, Loss_2:   1.4953, Loss_2_test:   1.3364\n",
      "Epoch   201, Loss_1:   1.1865, Loss_1_test:   1.6743, Loss_2:   1.4948, Loss_2_test:   1.3334\n",
      "Epoch   202, Loss_1:   1.1867, Loss_1_test:   1.6760, Loss_2:   1.4951, Loss_2_test:   1.3318\n",
      "Epoch   203, Loss_1:   1.1865, Loss_1_test:   1.6737, Loss_2:   1.4953, Loss_2_test:   1.3307\n",
      "Epoch   204, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4953, Loss_2_test:   1.3303\n",
      "Epoch   205, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4953, Loss_2_test:   1.3303\n",
      "Epoch   206, Loss_1:   1.1867, Loss_1_test:   1.6714, Loss_2:   1.4952, Loss_2_test:   1.3308\n",
      "Epoch   207, Loss_1:   1.1867, Loss_1_test:   1.6715, Loss_2:   1.4951, Loss_2_test:   1.3318\n",
      "Epoch   208, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4948, Loss_2_test:   1.3331\n",
      "Epoch   209, Loss_1:   1.1866, Loss_1_test:   1.6728, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   210, Loss_1:   1.1865, Loss_1_test:   1.6739, Loss_2:   1.4960, Loss_2_test:   1.3377\n",
      "Epoch   211, Loss_1:   1.1866, Loss_1_test:   1.6756, Loss_2:   1.4958, Loss_2_test:   1.3373\n",
      "Epoch   212, Loss_1:   1.1865, Loss_1_test:   1.6736, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   213, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   214, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   215, Loss_1:   1.1867, Loss_1_test:   1.6716, Loss_2:   1.4952, Loss_2_test:   1.3312\n",
      "Epoch   216, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4951, Loss_2_test:   1.3313\n",
      "Epoch   217, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4950, Loss_2_test:   1.3319\n",
      "Epoch   218, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4949, Loss_2_test:   1.3329\n",
      "Epoch   219, Loss_1:   1.1865, Loss_1_test:   1.6744, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   220, Loss_1:   1.1870, Loss_1_test:   1.6769, Loss_2:   1.4955, Loss_2_test:   1.3368\n",
      "Epoch   221, Loss_1:   1.1865, Loss_1_test:   1.6742, Loss_2:   1.4951, Loss_2_test:   1.3360\n",
      "Epoch   222, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   223, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4952, Loss_2_test:   1.3312\n",
      "Epoch   224, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4954, Loss_2_test:   1.3301\n",
      "Epoch   225, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4955, Loss_2_test:   1.3296\n",
      "Epoch   226, Loss_1:   1.1866, Loss_1_test:   1.6729, Loss_2:   1.4955, Loss_2_test:   1.3297\n",
      "Epoch   227, Loss_1:   1.1865, Loss_1_test:   1.6738, Loss_2:   1.4954, Loss_2_test:   1.3302\n",
      "Epoch   228, Loss_1:   1.1865, Loss_1_test:   1.6750, Loss_2:   1.4952, Loss_2_test:   1.3312\n",
      "Epoch   229, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   230, Loss_1:   1.1867, Loss_1_test:   1.6716, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   231, Loss_1:   1.1867, Loss_1_test:   1.6708, Loss_2:   1.4958, Loss_2_test:   1.3373\n",
      "Epoch   232, Loss_1:   1.1868, Loss_1_test:   1.6704, Loss_2:   1.4956, Loss_2_test:   1.3369\n",
      "Epoch   233, Loss_1:   1.1868, Loss_1_test:   1.6705, Loss_2:   1.4947, Loss_2_test:   1.3337\n",
      "Epoch   234, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4950, Loss_2_test:   1.3321\n",
      "Epoch   235, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   236, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4953, Loss_2_test:   1.3308\n",
      "Epoch   237, Loss_1:   1.1865, Loss_1_test:   1.6747, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   238, Loss_1:   1.1873, Loss_1_test:   1.6780, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   239, Loss_1:   1.1864, Loss_1_test:   1.6748, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   240, Loss_1:   1.1865, Loss_1_test:   1.6738, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   241, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4955, Loss_2_test:   1.3367\n",
      "Epoch   242, Loss_1:   1.1866, Loss_1_test:   1.6731, Loss_2:   1.4951, Loss_2_test:   1.3359\n",
      "Epoch   243, Loss_1:   1.1865, Loss_1_test:   1.6734, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   244, Loss_1:   1.1865, Loss_1_test:   1.6740, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   245, Loss_1:   1.1865, Loss_1_test:   1.6751, Loss_2:   1.4954, Loss_2_test:   1.3298\n",
      "Epoch   246, Loss_1:   1.1866, Loss_1_test:   1.6727, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   247, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4955, Loss_2_test:   1.3294\n",
      "Epoch   248, Loss_1:   1.1868, Loss_1_test:   1.6700, Loss_2:   1.4954, Loss_2_test:   1.3300\n",
      "Epoch   249, Loss_1:   1.1868, Loss_1_test:   1.6695, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   250, Loss_1:   1.1868, Loss_1_test:   1.6695, Loss_2:   1.4949, Loss_2_test:   1.3325\n",
      "Epoch   251, Loss_1:   1.1868, Loss_1_test:   1.6700, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   252, Loss_1:   1.1867, Loss_1_test:   1.6708, Loss_2:   1.4959, Loss_2_test:   1.3376\n",
      "Epoch   253, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4957, Loss_2_test:   1.3372\n",
      "Epoch   254, Loss_1:   1.1865, Loss_1_test:   1.6736, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   255, Loss_1:   1.1867, Loss_1_test:   1.6759, Loss_2:   1.4950, Loss_2_test:   1.3321\n",
      "Epoch   256, Loss_1:   1.1865, Loss_1_test:   1.6738, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   257, Loss_1:   1.1866, Loss_1_test:   1.6727, Loss_2:   1.4953, Loss_2_test:   1.3307\n",
      "Epoch   258, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   259, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   260, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   261, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   262, Loss_1:   1.1865, Loss_1_test:   1.6743, Loss_2:   1.4957, Loss_2_test:   1.3371\n",
      "Epoch   263, Loss_1:   1.1868, Loss_1_test:   1.6764, Loss_2:   1.4953, Loss_2_test:   1.3362\n",
      "Epoch   264, Loss_1:   1.1865, Loss_1_test:   1.6736, Loss_2:   1.4949, Loss_2_test:   1.3329\n",
      "Epoch   265, Loss_1:   1.1866, Loss_1_test:   1.6722, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   266, Loss_1:   1.1867, Loss_1_test:   1.6713, Loss_2:   1.4954, Loss_2_test:   1.3298\n",
      "Epoch   267, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   268, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   269, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4954, Loss_2_test:   1.3300\n",
      "Epoch   270, Loss_1:   1.1866, Loss_1_test:   1.6727, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   271, Loss_1:   1.1865, Loss_1_test:   1.6740, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   272, Loss_1:   1.1868, Loss_1_test:   1.6763, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   273, Loss_1:   1.1865, Loss_1_test:   1.6737, Loss_2:   1.4961, Loss_2_test:   1.3381\n",
      "Epoch   274, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4959, Loss_2_test:   1.3376\n",
      "Epoch   275, Loss_1:   1.1867, Loss_1_test:   1.6716, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   276, Loss_1:   1.1867, Loss_1_test:   1.6714, Loss_2:   1.4950, Loss_2_test:   1.3322\n",
      "Epoch   277, Loss_1:   1.1867, Loss_1_test:   1.6716, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   278, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4953, Loss_2_test:   1.3307\n",
      "Epoch   279, Loss_1:   1.1865, Loss_1_test:   1.6734, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   280, Loss_1:   1.1865, Loss_1_test:   1.6747, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   281, Loss_1:   1.1873, Loss_1_test:   1.6779, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   282, Loss_1:   1.1865, Loss_1_test:   1.6744, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   283, Loss_1:   1.1866, Loss_1_test:   1.6731, Loss_2:   1.4958, Loss_2_test:   1.3374\n",
      "Epoch   284, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4954, Loss_2_test:   1.3365\n",
      "Epoch   285, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4949, Loss_2_test:   1.3330\n",
      "Epoch   286, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   287, Loss_1:   1.1866, Loss_1_test:   1.6729, Loss_2:   1.4955, Loss_2_test:   1.3297\n",
      "Epoch   288, Loss_1:   1.1865, Loss_1_test:   1.6740, Loss_2:   1.4956, Loss_2_test:   1.3291\n",
      "Epoch   289, Loss_1:   1.1867, Loss_1_test:   1.6757, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   290, Loss_1:   1.1866, Loss_1_test:   1.6731, Loss_2:   1.4954, Loss_2_test:   1.3298\n",
      "Epoch   291, Loss_1:   1.1867, Loss_1_test:   1.6715, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   292, Loss_1:   1.1868, Loss_1_test:   1.6705, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   293, Loss_1:   1.1868, Loss_1_test:   1.6701, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   294, Loss_1:   1.1868, Loss_1_test:   1.6702, Loss_2:   1.4963, Loss_2_test:   1.3383\n",
      "Epoch   295, Loss_1:   1.1867, Loss_1_test:   1.6709, Loss_2:   1.4960, Loss_2_test:   1.3379\n",
      "Epoch   296, Loss_1:   1.1867, Loss_1_test:   1.6719, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   297, Loss_1:   1.1866, Loss_1_test:   1.6733, Loss_2:   1.4950, Loss_2_test:   1.3322\n",
      "Epoch   298, Loss_1:   1.1865, Loss_1_test:   1.6751, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   299, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4953, Loss_2_test:   1.3306\n",
      "Epoch   300, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4952, Loss_2_test:   1.3308\n",
      "Epoch   301, Loss_1:   1.1867, Loss_1_test:   1.6709, Loss_2:   1.4951, Loss_2_test:   1.3315\n",
      "Epoch   302, Loss_1:   1.1867, Loss_1_test:   1.6707, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   303, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4946, Loss_2_test:   1.3345\n",
      "Epoch   304, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4959, Loss_2_test:   1.3376\n",
      "Epoch   305, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4955, Loss_2_test:   1.3367\n",
      "Epoch   306, Loss_1:   1.1865, Loss_1_test:   1.6745, Loss_2:   1.4949, Loss_2_test:   1.3330\n",
      "Epoch   307, Loss_1:   1.1872, Loss_1_test:   1.6778, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   308, Loss_1:   1.1865, Loss_1_test:   1.6743, Loss_2:   1.4955, Loss_2_test:   1.3295\n",
      "Epoch   309, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4956, Loss_2_test:   1.3290\n",
      "Epoch   310, Loss_1:   1.1866, Loss_1_test:   1.6722, Loss_2:   1.4956, Loss_2_test:   1.3290\n",
      "Epoch   311, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4954, Loss_2_test:   1.3297\n",
      "Epoch   312, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   313, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   314, Loss_1:   1.1865, Loss_1_test:   1.6741, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   315, Loss_1:   1.1868, Loss_1_test:   1.6762, Loss_2:   1.4964, Loss_2_test:   1.3386\n",
      "Epoch   316, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4962, Loss_2_test:   1.3382\n",
      "Epoch   317, Loss_1:   1.1867, Loss_1_test:   1.6715, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   318, Loss_1:   1.1868, Loss_1_test:   1.6705, Loss_2:   1.4950, Loss_2_test:   1.3321\n",
      "Epoch   319, Loss_1:   1.1868, Loss_1_test:   1.6701, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   320, Loss_1:   1.1868, Loss_1_test:   1.6703, Loss_2:   1.4953, Loss_2_test:   1.3305\n",
      "Epoch   321, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4953, Loss_2_test:   1.3307\n",
      "Epoch   322, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4951, Loss_2_test:   1.3315\n",
      "Epoch   323, Loss_1:   1.1865, Loss_1_test:   1.6735, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   324, Loss_1:   1.1867, Loss_1_test:   1.6758, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   325, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4961, Loss_2_test:   1.3379\n",
      "Epoch   326, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4956, Loss_2_test:   1.3369\n",
      "Epoch   327, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   328, Loss_1:   1.1867, Loss_1_test:   1.6708, Loss_2:   1.4952, Loss_2_test:   1.3308\n",
      "Epoch   329, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4955, Loss_2_test:   1.3294\n",
      "Epoch   330, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4956, Loss_2_test:   1.3288\n",
      "Epoch   331, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4956, Loss_2_test:   1.3289\n",
      "Epoch   332, Loss_1:   1.1865, Loss_1_test:   1.6748, Loss_2:   1.4955, Loss_2_test:   1.3296\n",
      "Epoch   333, Loss_1:   1.1874, Loss_1_test:   1.6784, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   334, Loss_1:   1.1865, Loss_1_test:   1.6745, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   335, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4946, Loss_2_test:   1.3349\n",
      "Epoch   336, Loss_1:   1.1866, Loss_1_test:   1.6722, Loss_2:   1.4946, Loss_2_test:   1.3342\n",
      "Epoch   337, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   338, Loss_1:   1.1866, Loss_1_test:   1.6722, Loss_2:   1.4947, Loss_2_test:   1.3350\n",
      "Epoch   339, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   340, Loss_1:   1.1865, Loss_1_test:   1.6741, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   341, Loss_1:   1.1868, Loss_1_test:   1.6764, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   342, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4952, Loss_2_test:   1.3313\n",
      "Epoch   343, Loss_1:   1.1867, Loss_1_test:   1.6714, Loss_2:   1.4950, Loss_2_test:   1.3321\n",
      "Epoch   344, Loss_1:   1.1868, Loss_1_test:   1.6704, Loss_2:   1.4948, Loss_2_test:   1.3334\n",
      "Epoch   345, Loss_1:   1.1868, Loss_1_test:   1.6700, Loss_2:   1.4949, Loss_2_test:   1.3354\n",
      "Epoch   346, Loss_1:   1.1868, Loss_1_test:   1.6701, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   347, Loss_1:   1.1867, Loss_1_test:   1.6708, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   348, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   349, Loss_1:   1.1865, Loss_1_test:   1.6735, Loss_2:   1.4947, Loss_2_test:   1.3351\n",
      "Epoch   350, Loss_1:   1.1867, Loss_1_test:   1.6759, Loss_2:   1.4948, Loss_2_test:   1.3331\n",
      "Epoch   351, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4950, Loss_2_test:   1.3321\n",
      "Epoch   352, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4951, Loss_2_test:   1.3317\n",
      "Epoch   353, Loss_1:   1.1867, Loss_1_test:   1.6709, Loss_2:   1.4950, Loss_2_test:   1.3320\n",
      "Epoch   354, Loss_1:   1.1867, Loss_1_test:   1.6707, Loss_2:   1.4949, Loss_2_test:   1.3329\n",
      "Epoch   355, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   356, Loss_1:   1.1867, Loss_1_test:   1.6719, Loss_2:   1.4957, Loss_2_test:   1.3372\n",
      "Epoch   357, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4950, Loss_2_test:   1.3356\n",
      "Epoch   358, Loss_1:   1.1864, Loss_1_test:   1.6749, Loss_2:   1.4951, Loss_2_test:   1.3317\n",
      "Epoch   359, Loss_1:   1.1875, Loss_1_test:   1.6788, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   360, Loss_1:   1.1865, Loss_1_test:   1.6746, Loss_2:   1.4959, Loss_2_test:   1.3275\n",
      "Epoch   361, Loss_1:   1.1866, Loss_1_test:   1.6731, Loss_2:   1.4960, Loss_2_test:   1.3267\n",
      "Epoch   362, Loss_1:   1.1866, Loss_1_test:   1.6722, Loss_2:   1.4960, Loss_2_test:   1.3267\n",
      "Epoch   363, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4959, Loss_2_test:   1.3274\n",
      "Epoch   364, Loss_1:   1.1866, Loss_1_test:   1.6722, Loss_2:   1.4956, Loss_2_test:   1.3287\n",
      "Epoch   365, Loss_1:   1.1866, Loss_1_test:   1.6730, Loss_2:   1.4953, Loss_2_test:   1.3305\n",
      "Epoch   366, Loss_1:   1.1865, Loss_1_test:   1.6742, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   367, Loss_1:   1.1869, Loss_1_test:   1.6767, Loss_2:   1.4952, Loss_2_test:   1.3360\n",
      "Epoch   368, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4950, Loss_2_test:   1.3356\n",
      "Epoch   369, Loss_1:   1.1867, Loss_1_test:   1.6714, Loss_2:   1.4950, Loss_2_test:   1.3323\n",
      "Epoch   370, Loss_1:   1.1868, Loss_1_test:   1.6703, Loss_2:   1.4953, Loss_2_test:   1.3303\n",
      "Epoch   371, Loss_1:   1.1868, Loss_1_test:   1.6698, Loss_2:   1.4956, Loss_2_test:   1.3291\n",
      "Epoch   372, Loss_1:   1.1868, Loss_1_test:   1.6700, Loss_2:   1.4956, Loss_2_test:   1.3288\n",
      "Epoch   373, Loss_1:   1.1867, Loss_1_test:   1.6707, Loss_2:   1.4956, Loss_2_test:   1.3291\n",
      "Epoch   374, Loss_1:   1.1866, Loss_1_test:   1.6719, Loss_2:   1.4954, Loss_2_test:   1.3301\n",
      "Epoch   375, Loss_1:   1.1865, Loss_1_test:   1.6736, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   376, Loss_1:   1.1868, Loss_1_test:   1.6762, Loss_2:   1.4947, Loss_2_test:   1.3337\n",
      "Epoch   377, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4957, Loss_2_test:   1.3371\n",
      "Epoch   378, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4952, Loss_2_test:   1.3362\n",
      "Epoch   379, Loss_1:   1.1867, Loss_1_test:   1.6708, Loss_2:   1.4950, Loss_2_test:   1.3324\n",
      "Epoch   380, Loss_1:   1.1867, Loss_1_test:   1.6706, Loss_2:   1.4954, Loss_2_test:   1.3301\n",
      "Epoch   381, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4956, Loss_2_test:   1.3287\n",
      "Epoch   382, Loss_1:   1.1867, Loss_1_test:   1.6719, Loss_2:   1.4957, Loss_2_test:   1.3282\n",
      "Epoch   383, Loss_1:   1.1866, Loss_1_test:   1.6732, Loss_2:   1.4957, Loss_2_test:   1.3284\n",
      "Epoch   384, Loss_1:   1.1865, Loss_1_test:   1.6750, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   385, Loss_1:   1.1866, Loss_1_test:   1.6724, Loss_2:   1.4952, Loss_2_test:   1.3308\n",
      "Epoch   386, Loss_1:   1.1867, Loss_1_test:   1.6707, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   387, Loss_1:   1.1868, Loss_1_test:   1.6696, Loss_2:   1.4949, Loss_2_test:   1.3355\n",
      "Epoch   388, Loss_1:   1.1869, Loss_1_test:   1.6693, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   389, Loss_1:   1.1868, Loss_1_test:   1.6696, Loss_2:   1.4946, Loss_2_test:   1.3348\n",
      "Epoch   390, Loss_1:   1.1868, Loss_1_test:   1.6705, Loss_2:   1.4950, Loss_2_test:   1.3320\n",
      "Epoch   391, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4953, Loss_2_test:   1.3302\n",
      "Epoch   392, Loss_1:   1.1865, Loss_1_test:   1.6736, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   393, Loss_1:   1.1869, Loss_1_test:   1.6765, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   394, Loss_1:   1.1865, Loss_1_test:   1.6734, Loss_2:   1.4954, Loss_2_test:   1.3298\n",
      "Epoch   395, Loss_1:   1.1867, Loss_1_test:   1.6719, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   396, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   397, Loss_1:   1.1867, Loss_1_test:   1.6709, Loss_2:   1.4948, Loss_2_test:   1.3352\n",
      "Epoch   398, Loss_1:   1.1867, Loss_1_test:   1.6713, Loss_2:   1.4946, Loss_2_test:   1.3342\n",
      "Epoch   399, Loss_1:   1.1866, Loss_1_test:   1.6723, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   400, Loss_1:   1.1865, Loss_1_test:   1.6737, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   401, Loss_1:   1.1867, Loss_1_test:   1.6760, Loss_2:   1.4954, Loss_2_test:   1.3365\n",
      "Epoch   402, Loss_1:   1.1866, Loss_1_test:   1.6728, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   403, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4949, Loss_2_test:   1.3329\n",
      "Epoch   404, Loss_1:   1.1868, Loss_1_test:   1.6700, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   405, Loss_1:   1.1868, Loss_1_test:   1.6696, Loss_2:   1.4948, Loss_2_test:   1.3331\n",
      "Epoch   406, Loss_1:   1.1868, Loss_1_test:   1.6699, Loss_2:   1.4946, Loss_2_test:   1.3342\n",
      "Epoch   407, Loss_1:   1.1867, Loss_1_test:   1.6707, Loss_2:   1.4953, Loss_2_test:   1.3364\n",
      "Epoch   408, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   409, Loss_1:   1.1865, Loss_1_test:   1.6739, Loss_2:   1.4947, Loss_2_test:   1.3337\n",
      "Epoch   410, Loss_1:   1.1870, Loss_1_test:   1.6771, Loss_2:   1.4947, Loss_2_test:   1.3337\n",
      "Epoch   411, Loss_1:   1.1865, Loss_1_test:   1.6737, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   412, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4953, Loss_2_test:   1.3363\n",
      "Epoch   413, Loss_1:   1.1867, Loss_1_test:   1.6712, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   414, Loss_1:   1.1867, Loss_1_test:   1.6710, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   415, Loss_1:   1.1867, Loss_1_test:   1.6714, Loss_2:   1.4949, Loss_2_test:   1.3329\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define custom loss function, and optimizer\n",
    "model1 = RNNModel()\n",
    "model2 = RNNModel()\n",
    "\n",
    "squared_hinge_loss = SquaredHingeLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001, amsgrad=True)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001, amsgrad=True)\n",
    "\n",
    "# Training loop\n",
    "record_loss1_test = []\n",
    "record_loss2_test = []\n",
    "min_loss_1_test = float('inf')\n",
    "min_loss_2_test = float('inf')\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs1 = torch.tensor([[0.0]])\n",
    "    outputs2 = torch.tensor([[0.0]])\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        outputs1 += model1(seq)\n",
    "        outputs2 += model2(seq)\n",
    "    \n",
    "    # Compute the custom loss\n",
    "    loss_1 = squared_hinge_loss(outputs1, targets_low_1, targets_high_1)\n",
    "    loss_2 = squared_hinge_loss(outputs2, targets_low_2, targets_high_2)\n",
    "    loss_1_test = squared_hinge_loss(outputs1, targets_low_2, targets_high_2)\n",
    "    loss_2_test = squared_hinge_loss(outputs2, targets_low_1, targets_high_1)\n",
    "\n",
    "    # Print the loss every epochs\n",
    "    if (epoch) % 1 == 0:\n",
    "        print(f'Epoch {epoch:5d}, Loss_1: {loss_1.item():8.4f}, Loss_1_test: {loss_1_test.item():8.4f}, Loss_2: {loss_2.item():8.4f}, Loss_2_test: {loss_2_test.item():8.4f}')\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # record\n",
    "    record_loss1_test.append(loss_1_test.item())\n",
    "    record_loss2_test.append(loss_2_test.item())\n",
    "\n",
    "    # save models\n",
    "    if loss_1_test < min_loss_1_test:\n",
    "        min_loss_1_test = loss_1_test\n",
    "        torch.save(model1.state_dict(), 'saved_models/model1_rnn_relu_best.pth')\n",
    "    \n",
    "    if loss_2_test < min_loss_2_test:\n",
    "        min_loss_2_test = loss_2_test\n",
    "        torch.save(model2.state_dict(), 'saved_models/model2_rnn_relu_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model1\n",
    "model1 = RNNModel()\n",
    "model1.load_state_dict(torch.load('saved_models/model1_rnn_relu_best.pth'))\n",
    "model1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load model2\n",
    "model2 = RNNModel()\n",
    "model2.load_state_dict(torch.load('saved_models/model2_rnn_relu_best.pth'))\n",
    "model2.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas1 = np.zeros(len(seqs))\n",
    "ldas2 = np.zeros(len(seqs))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        ldas1[i] = model1(seq).numpy()[0][0]\n",
    "        ldas2[i] = model2(seq).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs   = gen_data_dict('sequence_label_data/signals.gz')\n",
    "labels = gen_data_dict('sequence_label_data/labels.gz')\n",
    "\n",
    "header = ['sequenceID', 'fold_1_total_labels', 'fold_2_total_labels', 'fold_1_errs', 'fold_2_errs']\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    # generate data\n",
    "    sequence, neg_start_1, neg_end_1, pos_start_1, pos_end_1, neg_start_2, neg_end_2, pos_start_2, pos_end_2 = get_data(i, seqs=seqs, labels=labels)\n",
    "    sequence_length = len(sequence)-1\n",
    "\n",
    "    # vectors of cumulative sums\n",
    "    y, z = get_cumsum(sequence)\n",
    "\n",
    "    # get total labels\n",
    "    fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "    fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "    # run each lambda and record it into csv file\n",
    "    row  = [i, fold1_total_labels, fold2_total_labels]\n",
    "\n",
    "    chpnt_fold1 = opart(10**ldas2[i], sequence)\n",
    "    chpnt_fold2 = opart(10**ldas1[i], sequence)\n",
    "\n",
    "    err_1 = error_count(chpnt_fold1, neg_start_1, neg_end_1, pos_start_1, pos_end_1)\n",
    "    err_2 = error_count(chpnt_fold2, neg_start_2, neg_end_2, pos_start_2, pos_end_2)\n",
    "    \n",
    "    row.append(sum(err_1))\n",
    "    row.append(sum(err_2))\n",
    "\n",
    "    write_to_csv('learning_output/rnn_relu.csv', header, row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
