{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x266712afcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.utility_functions import gen_data_dict, get_data, get_cumsum, error_count, write_to_csv, opart, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "seqs = gen_data_dict('sequence_label_data/signals.gz')\n",
    "\n",
    "# target \n",
    "target_df_1 = pd.read_csv('learning_data/target_lambda_fold1.csv')\n",
    "target_df_2 = pd.read_csv('learning_data/target_lambda_fold2.csv')\n",
    "\n",
    "targets_low_1  = target_df_1.iloc[:, 1:2].to_numpy()\n",
    "targets_high_1 = target_df_1.iloc[:, 2:3].to_numpy()\n",
    "targets_low_2  = target_df_2.iloc[:, 1:2].to_numpy()\n",
    "targets_high_2 = target_df_2.iloc[:, 2:3].to_numpy()\n",
    "\n",
    "targets_low_1  = torch.FloatTensor(targets_low_1)\n",
    "targets_high_1 = torch.FloatTensor(targets_high_1)\n",
    "targets_low_2  = torch.FloatTensor(targets_low_2)\n",
    "targets_high_2 = torch.FloatTensor(targets_high_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=8, num_layers=1, nonlinearity='relu', bias=True, batch_first=False)\n",
    "        self.fc1 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x = self.rnn(x)\n",
    "        x    = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, Loss_1: 9654.5010, Loss_1_test: 9578.5010, Loss_2: 9652.5215, Loss_2_test: 9728.9961\n",
      "Epoch     1, Loss_1: 9225.3545, Loss_1_test: 9152.0947, Loss_2: 9351.2129, Loss_2_test: 9425.7529\n",
      "Epoch     2, Loss_1: 8806.4150, Loss_1_test: 8735.8477, Loss_2: 9052.2520, Loss_2_test: 9124.8672\n",
      "Epoch     3, Loss_1: 8396.0195, Loss_1_test: 8328.1025, Loss_2: 8755.9150, Loss_2_test: 8826.6133\n",
      "Epoch     4, Loss_1: 7995.4492, Loss_1_test: 7930.1367, Loss_2: 8462.1855, Loss_2_test: 8530.9766\n",
      "Epoch     5, Loss_1: 7603.5156, Loss_1_test: 7540.7656, Loss_2: 8171.3125, Loss_2_test: 8238.2061\n",
      "Epoch     6, Loss_1: 7219.9746, Loss_1_test: 7159.7495, Loss_2: 7883.3311, Loss_2_test: 7948.3379\n",
      "Epoch     7, Loss_1: 6845.1875, Loss_1_test: 6787.4473, Loss_2: 7598.5225, Loss_2_test: 7661.6538\n",
      "Epoch     8, Loss_1: 6478.4014, Loss_1_test: 6423.1089, Loss_2: 7317.1768, Loss_2_test: 7378.4468\n",
      "Epoch     9, Loss_1: 6118.4492, Loss_1_test: 6065.5801, Loss_2: 7039.2290, Loss_2_test: 7098.6519\n",
      "Epoch    10, Loss_1: 5765.4189, Loss_1_test: 5714.9448, Loss_2: 6764.8262, Loss_2_test: 6822.4160\n",
      "Epoch    11, Loss_1: 5420.1167, Loss_1_test: 5372.0049, Loss_2: 6493.8286, Loss_2_test: 6549.5962\n",
      "Epoch    12, Loss_1: 5082.1475, Loss_1_test: 5036.3701, Loss_2: 6226.5464, Loss_2_test: 6280.5098\n",
      "Epoch    13, Loss_1: 4751.7129, Loss_1_test: 4708.2397, Loss_2: 5963.0576, Loss_2_test: 6015.2295\n",
      "Epoch    14, Loss_1: 4429.1123, Loss_1_test: 4387.9126, Loss_2: 5703.1870, Loss_2_test: 5753.5811\n",
      "Epoch    15, Loss_1: 4114.6567, Loss_1_test: 4075.6992, Loss_2: 5447.1875, Loss_2_test: 5495.8188\n",
      "Epoch    16, Loss_1: 3808.9233, Loss_1_test: 3772.1716, Loss_2: 5195.2173, Loss_2_test: 5242.1025\n",
      "Epoch    17, Loss_1: 3512.4751, Loss_1_test: 3477.8904, Loss_2: 4947.3906, Loss_2_test: 4992.5459\n",
      "Epoch    18, Loss_1: 3225.8472, Loss_1_test: 3193.3867, Loss_2: 4703.7056, Loss_2_test: 4747.1465\n",
      "Epoch    19, Loss_1: 2948.5579, Loss_1_test: 2918.1848, Loss_2: 4464.1787, Loss_2_test: 4505.9214\n",
      "Epoch    20, Loss_1: 2681.8208, Loss_1_test: 2653.4890, Loss_2: 4229.1792, Loss_2_test: 4269.2417\n",
      "Epoch    21, Loss_1: 2425.8389, Loss_1_test: 2399.5017, Loss_2: 3999.0256, Loss_2_test: 4037.4285\n",
      "Epoch    22, Loss_1: 2180.5642, Loss_1_test: 2156.1765, Loss_2: 3773.8447, Loss_2_test: 3810.6094\n",
      "Epoch    23, Loss_1: 1945.9067, Loss_1_test: 1923.4253, Loss_2: 3553.7490, Loss_2_test: 3588.8953\n",
      "Epoch    24, Loss_1: 1722.7709, Loss_1_test: 1702.1454, Loss_2: 3338.8826, Loss_2_test: 3372.4333\n",
      "Epoch    25, Loss_1: 1511.8518, Loss_1_test: 1493.0284, Loss_2: 3129.4529, Loss_2_test: 3161.4309\n",
      "Epoch    26, Loss_1: 1313.5687, Loss_1_test: 1296.4907, Loss_2: 2925.7336, Loss_2_test: 2956.1641\n",
      "Epoch    27, Loss_1: 1128.3423, Loss_1_test: 1112.9498, Loss_2: 2727.6428, Loss_2_test: 2756.5496\n",
      "Epoch    28, Loss_1: 956.8209, Loss_1_test: 943.0492, Loss_2: 2535.5217, Loss_2_test: 2562.9316\n",
      "Epoch    29, Loss_1: 799.4832, Loss_1_test: 787.2629, Loss_2: 2349.6509, Loss_2_test: 2375.5916\n",
      "Epoch    30, Loss_1: 656.6526, Loss_1_test: 645.9111, Loss_2: 2170.0098, Loss_2_test: 2194.5090\n",
      "Epoch    31, Loss_1: 528.4169, Loss_1_test: 519.0795, Loss_2: 1996.8470, Loss_2_test: 2019.9349\n",
      "Epoch    32, Loss_1: 415.0821, Loss_1_test: 407.0694, Loss_2: 1830.5425, Loss_2_test: 1852.2506\n",
      "Epoch    33, Loss_1: 316.6838, Loss_1_test: 309.9127, Loss_2: 1671.1920, Loss_2_test: 1691.5537\n",
      "Epoch    34, Loss_1: 232.9464, Loss_1_test: 227.3323, Loss_2: 1519.0493, Loss_2_test: 1538.0996\n",
      "Epoch    35, Loss_1: 163.4632, Loss_1_test: 158.9207, Loss_2: 1373.9418, Loss_2_test: 1391.7142\n",
      "Epoch    36, Loss_1: 107.6908, Loss_1_test: 104.1330, Loss_2: 1236.0873, Loss_2_test: 1252.6171\n",
      "Epoch    37, Loss_1:  64.8460, Loss_1_test:  62.1864, Loss_2: 1105.7589, Loss_2_test: 1121.0839\n",
      "Epoch    38, Loss_1:  33.9700, Loss_1_test:  32.1223, Loss_2: 983.0685, Loss_2_test: 997.2279\n",
      "Epoch    39, Loss_1:  13.8727, Loss_1_test:  12.7532, Loss_2: 867.8307, Loss_2_test: 880.8621\n",
      "Epoch    40, Loss_1:   3.3418, Loss_1_test:   2.9027, Loss_2: 760.0673, Loss_2_test: 772.0090\n",
      "Epoch    41, Loss_1:   1.3195, Loss_1_test:   2.0936, Loss_2: 660.1610, Loss_2_test: 671.0552\n",
      "Epoch    42, Loss_1:   4.1558, Loss_1_test:   7.6689, Loss_2: 567.9767, Loss_2_test: 577.8657\n",
      "Epoch    43, Loss_1:   9.3752, Loss_1_test:  16.8179, Loss_2: 483.3128, Loss_2_test: 492.2372\n",
      "Epoch    44, Loss_1:  15.9640, Loss_1_test:  28.2090, Loss_2: 406.0714, Loss_2_test: 414.0718\n",
      "Epoch    45, Loss_1:  23.3473, Loss_1_test:  40.8770, Loss_2: 336.2674, Loss_2_test: 343.3860\n",
      "Epoch    46, Loss_1:  31.0288, Loss_1_test:  53.9969, Loss_2: 273.8223, Loss_2_test: 280.1019\n",
      "Epoch    47, Loss_1:  38.5826, Loss_1_test:  66.8601, Loss_2: 218.6773, Loss_2_test: 224.1627\n",
      "Epoch    48, Loss_1:  45.6593, Loss_1_test:  78.8861, Loss_2: 170.4836, Loss_2_test: 175.2171\n",
      "Epoch    49, Loss_1:  51.9885, Loss_1_test:  89.6260, Loss_2: 129.0259, Loss_2_test: 133.0500\n",
      "Epoch    50, Loss_1:  57.3706, Loss_1_test:  98.7494, Loss_2:  93.9316, Loss_2_test:  97.2863\n",
      "Epoch    51, Loss_1:  61.6767, Loss_1_test: 106.0432, Loss_2:  65.0148, Loss_2_test:  67.7414\n",
      "Epoch    52, Loss_1:  64.8364, Loss_1_test: 111.3925, Loss_2:  42.0269, Loss_2_test:  44.1677\n",
      "Epoch    53, Loss_1:  66.8352, Loss_1_test: 114.7753, Loss_2:  24.5361, Loss_2_test:  26.1318\n",
      "Epoch    54, Loss_1:  67.7058, Loss_1_test: 116.2485, Loss_2:  12.1844, Loss_2_test:  13.2765\n",
      "Epoch    55, Loss_1:  67.5187, Loss_1_test: 115.9318, Loss_2:   4.5276, Loss_2_test:   5.1624\n",
      "Epoch    56, Loss_1:  66.3720, Loss_1_test: 113.9914, Loss_2:   1.6178, Loss_2_test:   1.5735\n",
      "Epoch    57, Loss_1:  64.3841, Loss_1_test: 110.6269, Loss_2:   2.5674, Loss_2_test:   1.5018\n",
      "Epoch    58, Loss_1:  61.6830, Loss_1_test: 106.0538, Loss_2:   6.3029, Loss_2_test:   3.4001\n",
      "Epoch    59, Loss_1:  58.4068, Loss_1_test: 100.5049, Loss_2:  11.4286, Loss_2_test:   6.2892\n",
      "Epoch    60, Loss_1:  54.6883, Loss_1_test:  94.2035, Loss_2:  17.1532, Loss_2_test:   9.5680\n",
      "Epoch    61, Loss_1:  50.6544, Loss_1_test:  87.3633, Loss_2:  22.8191, Loss_2_test:  12.8382\n",
      "Epoch    62, Loss_1:  46.4263, Loss_1_test:  80.1884, Loss_2:  27.8929, Loss_2_test:  15.7804\n",
      "Epoch    63, Loss_1:  42.1146, Loss_1_test:  72.8649, Loss_2:  31.9926, Loss_2_test:  18.1646\n",
      "Epoch    64, Loss_1:  37.8224, Loss_1_test:  65.5669, Loss_2:  34.9114, Loss_2_test:  19.8652\n",
      "Epoch    65, Loss_1:  33.6263, Loss_1_test:  58.4237, Loss_2:  36.5880, Loss_2_test:  20.8430\n",
      "Epoch    66, Loss_1:  29.5910, Loss_1_test:  51.5446, Loss_2:  37.0938, Loss_2_test:  21.1381\n",
      "Epoch    67, Loss_1:  25.7720, Loss_1_test:  45.0237, Loss_2:  36.5666, Loss_2_test:  20.8305\n",
      "Epoch    68, Loss_1:  22.2090, Loss_1_test:  38.9283, Loss_2:  35.1762, Loss_2_test:  20.0195\n",
      "Epoch    69, Loss_1:  18.9269, Loss_1_test:  33.3014, Loss_2:  33.1298, Loss_2_test:  18.8269\n",
      "Epoch    70, Loss_1:  15.9449, Loss_1_test:  28.1761, Loss_2:  30.6485, Loss_2_test:  17.3823\n",
      "Epoch    71, Loss_1:  13.2696, Loss_1_test:  23.5641, Loss_2:  27.9153, Loss_2_test:  15.7934\n",
      "Epoch    72, Loss_1:  10.8992, Loss_1_test:  19.4637, Loss_2:  25.0721, Loss_2_test:  14.1434\n",
      "Epoch    73, Loss_1:   8.8272, Loss_1_test:  15.8644, Loss_2:  22.2017, Loss_2_test:  12.4810\n",
      "Epoch    74, Loss_1:   7.0402, Loss_1_test:  12.7449, Loss_2:  19.3972, Loss_2_test:  10.8609\n",
      "Epoch    75, Loss_1:   5.5213, Loss_1_test:  10.0789, Loss_2:  16.7433, Loss_2_test:   9.3322\n",
      "Epoch    76, Loss_1:   4.2512, Loss_1_test:   7.8379, Loss_2:  14.2828, Loss_2_test:   7.9200\n",
      "Epoch    77, Loss_1:   3.2249, Loss_1_test:   5.9843, Loss_2:  12.0249, Loss_2_test:   6.6291\n",
      "Epoch    78, Loss_1:   2.4299, Loss_1_test:   4.4965, Loss_2:   9.9814, Loss_2_test:   5.4658\n",
      "Epoch    79, Loss_1:   1.8535, Loss_1_test:   3.3412, Loss_2:   8.1792, Loss_2_test:   4.4439\n",
      "Epoch    80, Loss_1:   1.4614, Loss_1_test:   2.4769, Loss_2:   6.6126, Loss_2_test:   3.5709\n",
      "Epoch    81, Loss_1:   1.2608, Loss_1_test:   1.9207, Loss_2:   5.2847, Loss_2_test:   2.8454\n",
      "Epoch    82, Loss_1:   1.1983, Loss_1_test:   1.6085, Loss_2:   4.1820, Loss_2_test:   2.2651\n",
      "Epoch    83, Loss_1:   1.2766, Loss_1_test:   1.5130, Loss_2:   3.2866, Loss_2_test:   1.8285\n",
      "Epoch    84, Loss_1:   1.4461, Loss_1_test:   1.5499, Loss_2:   2.5802, Loss_2_test:   1.5075\n",
      "Epoch    85, Loss_1:   1.6835, Loss_1_test:   1.6792, Loss_2:   2.0829, Loss_2_test:   1.3157\n",
      "Epoch    86, Loss_1:   1.9291, Loss_1_test:   1.8223, Loss_2:   1.7165, Loss_2_test:   1.1981\n",
      "Epoch    87, Loss_1:   2.2144, Loss_1_test:   2.0318, Loss_2:   1.5715, Loss_2_test:   1.2146\n",
      "Epoch    88, Loss_1:   2.4636, Loss_1_test:   2.2162, Loss_2:   1.5079, Loss_2_test:   1.2875\n",
      "Epoch    89, Loss_1:   2.6614, Loss_1_test:   2.3628, Loss_2:   1.5384, Loss_2_test:   1.4238\n",
      "Epoch    90, Loss_1:   2.7999, Loss_1_test:   2.4656, Loss_2:   1.6371, Loss_2_test:   1.6085\n",
      "Epoch    91, Loss_1:   2.8760, Loss_1_test:   2.5220, Loss_2:   1.7468, Loss_2_test:   1.8010\n",
      "Epoch    92, Loss_1:   2.8910, Loss_1_test:   2.5332, Loss_2:   1.8767, Loss_2_test:   2.0042\n",
      "Epoch    93, Loss_1:   2.8494, Loss_1_test:   2.5023, Loss_2:   2.0361, Loss_2_test:   2.2202\n",
      "Epoch    94, Loss_1:   2.7586, Loss_1_test:   2.4349, Loss_2:   2.1771, Loss_2_test:   2.4108\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define custom loss function, and optimizer\n",
    "model1 = RNNModel()\n",
    "model2 = RNNModel()\n",
    "\n",
    "squared_hinge_loss = SquaredHingeLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "record_loss1_test = []\n",
    "record_loss2_test = []\n",
    "min_loss_1_test = float('inf')\n",
    "min_loss_2_test = float('inf')\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs1 = torch.tensor([[0.0]])\n",
    "    outputs2 = torch.tensor([[0.0]])\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        outputs1 += model1(seq)\n",
    "        outputs2 += model2(seq)\n",
    "    \n",
    "    # Compute the custom loss\n",
    "    loss_1 = squared_hinge_loss(outputs1, targets_low_1, targets_high_1)\n",
    "    loss_2 = squared_hinge_loss(outputs2, targets_low_2, targets_high_2)\n",
    "    loss_1_test = squared_hinge_loss(outputs1, targets_low_2, targets_high_2)\n",
    "    loss_2_test = squared_hinge_loss(outputs2, targets_low_1, targets_high_1)\n",
    "\n",
    "    # Print the loss every epochs\n",
    "    if (epoch) % 1 == 0:\n",
    "        print(f'Epoch {epoch:5d}, Loss_1: {loss_1.item():8.4f}, Loss_1_test: {loss_1_test.item():8.4f}, Loss_2: {loss_2.item():8.4f}, Loss_2_test: {loss_2_test.item():8.4f}')\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # record\n",
    "    record_loss1_test.append(loss_1_test.item())\n",
    "    record_loss2_test.append(loss_2_test.item())\n",
    "\n",
    "    # save models\n",
    "    if loss_1_test < min_loss_1_test:\n",
    "        min_loss_1_test = loss_1_test\n",
    "        torch.save(model1.state_dict(), 'saved_models/model1_rnn_relu_best.pth')\n",
    "    \n",
    "    if loss_2_test < min_loss_2_test:\n",
    "        min_loss_2_test = loss_2_test\n",
    "        torch.save(model2.state_dict(), 'saved_models/model2_rnn_relu_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model1\n",
    "model1 = RNNModel()\n",
    "model1.load_state_dict(torch.load('saved_models/model1_rnn_relu_best.pth'))\n",
    "model1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load model2\n",
    "model2 = RNNModel()\n",
    "model2.load_state_dict(torch.load('saved_models/model2_rnn_relu_best.pth'))\n",
    "model2.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas1 = np.zeros(len(seqs))\n",
    "ldas2 = np.zeros(len(seqs))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        ldas1[i] = model1(seq).numpy()[0][0]\n",
    "        ldas2[i] = model2(seq).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs   = gen_data_dict('sequence_label_data/signals.gz')\n",
    "labels = gen_data_dict('sequence_label_data/labels.gz')\n",
    "\n",
    "header = ['sequenceID', 'fold_1_total_labels', 'fold_2_total_labels', 'fold_1_errs', 'fold_2_errs']\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    # generate data\n",
    "    sequence, neg_start_1, neg_end_1, pos_start_1, pos_end_1, neg_start_2, neg_end_2, pos_start_2, pos_end_2 = get_data(i, seqs=seqs, labels=labels)\n",
    "    sequence_length = len(sequence)-1\n",
    "\n",
    "    # vectors of cumulative sums\n",
    "    y, z = get_cumsum(sequence)\n",
    "\n",
    "    # get total labels\n",
    "    fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "    fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "    # run each lambda and record it into csv file\n",
    "    row  = [i, fold1_total_labels, fold2_total_labels]\n",
    "\n",
    "    chpnt_fold1 = opart(10**ldas2[i], sequence)\n",
    "    chpnt_fold2 = opart(10**ldas1[i], sequence)\n",
    "\n",
    "    err_1 = error_count(chpnt_fold1, neg_start_1, neg_end_1, pos_start_1, pos_end_1)\n",
    "    err_2 = error_count(chpnt_fold2, neg_start_2, neg_end_2, pos_start_2, pos_end_2)\n",
    "    \n",
    "    row.append(sum(err_1))\n",
    "    row.append(sum(err_2))\n",
    "\n",
    "    write_to_csv('learning_output/rnn_relu.csv', header, row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
