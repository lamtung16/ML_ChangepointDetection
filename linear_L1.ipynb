{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utility_functions import SquaredHingeLoss, get_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L1 regularization term\n",
    "def l1_regularization(model, lambda_l1):\n",
    "    l1_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l1_loss += torch.norm(param, p=1)\n",
    "    return lambda_l1 * l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter non-complete columns\n",
    "def filter_numeric_columns(df):\n",
    "    numeric_columns = ['sequenceID']\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in ['int64', 'float64']:\n",
    "            if df[column].notna().all():\n",
    "                if not df[column].isin([np.inf, -np.inf]).any():\n",
    "                    numeric_columns.append(column)\n",
    "    numeric_columns = numeric_columns\n",
    "    return df[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fold dfs\n",
    "def get_fold_dfs(fold, fold_df, inputs_df, outputs_df, evaluation_df):\n",
    "    train_inputs_df = inputs_df[inputs_df['sequenceID'].isin(fold_df[fold_df['fold'] != fold]['sequenceID'])]\n",
    "    train_outputs_df = outputs_df[outputs_df['sequenceID'].isin(fold_df[fold_df['fold'] != fold]['sequenceID'])]\n",
    "    train_eval_df = evaluation_df[evaluation_df['sequenceID'].isin(fold_df[fold_df['fold'] != fold]['sequenceID'])]\n",
    "    test_inputs_df = inputs_df[inputs_df['sequenceID'].isin(fold_df[fold_df['fold'] == fold]['sequenceID'])]\n",
    "    test_eval_df = evaluation_df[evaluation_df['sequenceID'].isin(fold_df[fold_df['fold'] == fold]['sequenceID'])]\n",
    "    return filter_numeric_columns(train_inputs_df), train_outputs_df, train_eval_df, filter_numeric_columns(test_inputs_df), test_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['detailed', 'systematic', 'epigenomic']:\n",
    "    # Paths setup\n",
    "    fold_path = f'training_data/{dataset}/folds.csv'\n",
    "    inputs_path = f'training_data/{dataset}/inputs_old.csv'\n",
    "    outputs_path = f'training_data/{dataset}/outputs.csv'\n",
    "    evaluation_path = f'training_data/{dataset}/evaluation.csv'\n",
    "    acc_rate_path = f'acc_rate/{dataset}.csv'\n",
    "    output_df_path = f'record_dataframe/{dataset}/'\n",
    "\n",
    "    # Read dataframes\n",
    "    fold_df = pd.read_csv(fold_path)\n",
    "    inputs_df = pd.read_csv(inputs_path)\n",
    "    outputs_df = pd.read_csv(outputs_path)\n",
    "    evaluation_df = pd.read_csv(evaluation_path)\n",
    "\n",
    "    # Number of folds\n",
    "    n_folds = fold_df['fold'].nunique()\n",
    "\n",
    "    # Define candidate lambda_l1 values\n",
    "    candidate_lambda_l1 = [0, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20]\n",
    "\n",
    "    # Initialize variables to store best lambda_l1 and corresponding best accuracy\n",
    "    best_lambda_l1 = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Iterate over each candidate lambda_l1\n",
    "    for lambda_l1 in candidate_lambda_l1:\n",
    "        total_acc = 0\n",
    "        for fold in range(1, n_folds + 1):\n",
    "            train_inputs_df, train_outputs_df, train_eval_df, test_inputs_df, test_eval_df = get_fold_dfs(fold, fold_df, inputs_df, outputs_df, evaluation_df)\n",
    "\n",
    "            # Convert data to tensors\n",
    "            inputs = torch.Tensor(train_inputs_df.drop(columns=['sequenceID']).to_numpy())\n",
    "            test_inputs = torch.Tensor(test_inputs_df.drop(columns=['sequenceID']).to_numpy())\n",
    "            targets_low = torch.Tensor(train_outputs_df['min.log.lambda'].to_numpy().reshape(-1, 1))\n",
    "            targets_high = torch.Tensor(train_outputs_df['max.log.lambda'].to_numpy().reshape(-1, 1))\n",
    "            outputs = torch.cat((targets_low, targets_high), dim=1)\n",
    "\n",
    "            # Hyperparameters\n",
    "            lr = 0.00001\n",
    "            n_iters = 10000000\n",
    "\n",
    "            # Initialize the model\n",
    "            model = LinearModel(inputs.shape[1])\n",
    "\n",
    "            # Define loss function and optimizer\n",
    "            criterion = SquaredHingeLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Training loop\n",
    "            best_loss = float('inf')\n",
    "            patience = 100000\n",
    "            wait = 0\n",
    "            for epoch in range(n_iters):\n",
    "                model.train()\n",
    "                loss = criterion(model(inputs), outputs)\n",
    "                loss += l1_regularization(model, lambda_l1)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                    if wait >= patience:\n",
    "                        print(\"Early stopping after {} epochs without improvement.\".format(patience))\n",
    "                        break\n",
    "\n",
    "            # Calculate accuracy\n",
    "            with torch.no_grad():\n",
    "                lldas = model(test_inputs).numpy().reshape(-1)\n",
    "\n",
    "            lldas_df = pd.DataFrame(list(zip(test_inputs_df['sequenceID'], lldas)), columns=['sequenceID', 'llda'])\n",
    "            acc = get_acc(test_eval_df, lldas_df)\n",
    "            total_acc += acc\n",
    "            print(dataset, fold, acc)\n",
    "\n",
    "        # Calculate average accuracy across folds\n",
    "        avg_acc = total_acc / n_folds\n",
    "\n",
    "        # Check if this lambda_l1 gives better accuracy\n",
    "        if avg_acc > best_accuracy:\n",
    "            best_accuracy = avg_acc\n",
    "            best_lambda_l1 = lambda_l1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
