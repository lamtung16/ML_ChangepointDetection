{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1da205bb250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.OPART import gen_data_dict, get_data, get_cumsum, L, trace_back, error_count, write_to_csv, opart\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "margin = 1\n",
    "class SquaredHingeLoss(nn.Module):\n",
    "    def forward(self, predicted, low, high):\n",
    "        low  = low + margin\n",
    "        high = high - margin\n",
    "        loss = torch.relu(low - predicted) + torch.relu(predicted - high)\n",
    "        return torch.sum(loss**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    mean = data.mean(dim=0)\n",
    "    std  = data.std(dim=0)\n",
    "    norm_data = (data - mean) / std\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data = torch.FloatTensor(pd.read_csv('learning_data/seq_features.csv').iloc[:, 1:].to_numpy())\n",
    "data = normalize_data(data)\n",
    "\n",
    "target_df_1 = pd.read_csv('learning_data/target_lambda_fold1.csv')\n",
    "target_df_2 = pd.read_csv('learning_data/target_lambda_fold2.csv')\n",
    "\n",
    "targets_low_1  = target_df_1.iloc[:, 1:2].to_numpy()\n",
    "targets_high_1 = target_df_1.iloc[:, 2:3].to_numpy()\n",
    "targets_low_2  = target_df_2.iloc[:, 1:2].to_numpy()\n",
    "targets_high_2 = target_df_2.iloc[:, 2:3].to_numpy()\n",
    "\n",
    "targets_low_1  = torch.FloatTensor(targets_low_1)\n",
    "targets_high_1 = torch.FloatTensor(targets_high_1)\n",
    "targets_low_2  = torch.FloatTensor(targets_low_2)\n",
    "targets_high_2 = torch.FloatTensor(targets_high_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df   = pd.read_csv('learning_data/seq_features.csv')\n",
    "# target_df = pd.read_csv('learning_data/target_lambda_bothfold.csv')\n",
    "\n",
    "# n_train = 300\n",
    "\n",
    "# # train data\n",
    "# data = torch.FloatTensor(data_df.iloc[:n_train, 1:].to_numpy())\n",
    "# data = normalize_data(data)\n",
    "# targets_1 = torch.FloatTensor(target_df.iloc[:n_train, 1:2].to_numpy())\n",
    "# targets_2 = torch.FloatTensor(target_df.iloc[:n_train, 2:3].to_numpy())\n",
    "\n",
    "# # test_data\n",
    "# data_test = torch.FloatTensor(data_df.iloc[n_train:, 1:].to_numpy())\n",
    "# data_test = normalize_data(data_test)\n",
    "# targets_test_1 = torch.FloatTensor(target_df.iloc[n_train:, 1:2].to_numpy())\n",
    "# targets_test_2 = torch.FloatTensor(target_df.iloc[n_train:, 2:3].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep learning model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, Loss_1: 501.9193, Loss_1_test: 630.4261, Loss_2: 663.4351, Loss_2_test: 606.8508\n",
      "Epoch    10, Loss_1: 486.9156, Loss_1_test: 622.2831, Loss_2: 632.6459, Loss_2_test: 567.3534\n",
      "Epoch    20, Loss_1: 478.1483, Loss_1_test: 620.2513, Loss_2: 611.2495, Loss_2_test: 540.7096\n",
      "Epoch    30, Loss_1: 470.1064, Loss_1_test: 619.2408, Loss_2: 596.6531, Loss_2_test: 523.7631\n",
      "Epoch    40, Loss_1: 462.6455, Loss_1_test: 618.7621, Loss_2: 585.9767, Loss_2_test: 513.3187\n",
      "Epoch    50, Loss_1: 458.6603, Loss_1_test: 616.9952, Loss_2: 576.5859, Loss_2_test: 504.2643\n",
      "Epoch    60, Loss_1: 453.4476, Loss_1_test: 610.9558, Loss_2: 567.7049, Loss_2_test: 495.6997\n",
      "Epoch    70, Loss_1: 448.6669, Loss_1_test: 604.7373, Loss_2: 560.1041, Loss_2_test: 487.8163\n",
      "Epoch    80, Loss_1: 444.1629, Loss_1_test: 597.9248, Loss_2: 555.1877, Loss_2_test: 482.4869\n",
      "Epoch    90, Loss_1: 439.6819, Loss_1_test: 590.9053, Loss_2: 550.1139, Loss_2_test: 479.4402\n",
      "Epoch   100, Loss_1: 435.3570, Loss_1_test: 583.9104, Loss_2: 544.6987, Loss_2_test: 476.6091\n",
      "Epoch   110, Loss_1: 431.5131, Loss_1_test: 578.4581, Loss_2: 539.0895, Loss_2_test: 473.1537\n",
      "Epoch   120, Loss_1: 427.9749, Loss_1_test: 572.9703, Loss_2: 532.9821, Loss_2_test: 469.1441\n",
      "Epoch   130, Loss_1: 424.7632, Loss_1_test: 566.6540, Loss_2: 526.6531, Loss_2_test: 465.8346\n",
      "Epoch   140, Loss_1: 422.0918, Loss_1_test: 559.0018, Loss_2: 520.2392, Loss_2_test: 463.3444\n",
      "Epoch   150, Loss_1: 419.6884, Loss_1_test: 550.7271, Loss_2: 513.9418, Loss_2_test: 462.2324\n",
      "Epoch   160, Loss_1: 417.9204, Loss_1_test: 546.2817, Loss_2: 508.3191, Loss_2_test: 461.8600\n",
      "Epoch   170, Loss_1: 416.8873, Loss_1_test: 546.3896, Loss_2: 503.3066, Loss_2_test: 461.6682\n",
      "Epoch   180, Loss_1: 416.2088, Loss_1_test: 547.8776, Loss_2: 499.4406, Loss_2_test: 462.4614\n",
      "Epoch   190, Loss_1: 415.6367, Loss_1_test: 548.5554, Loss_2: 496.6183, Loss_2_test: 462.8031\n",
      "Epoch   200, Loss_1: 415.1110, Loss_1_test: 549.1693, Loss_2: 494.8587, Loss_2_test: 459.4766\n",
      "Epoch   210, Loss_1: 414.6186, Loss_1_test: 549.5627, Loss_2: 493.8611, Loss_2_test: 457.1354\n",
      "Epoch   220, Loss_1: 414.1396, Loss_1_test: 549.3121, Loss_2: 493.1050, Loss_2_test: 456.5004\n",
      "Epoch   230, Loss_1: 413.7032, Loss_1_test: 549.4954, Loss_2: 492.5383, Loss_2_test: 455.5934\n",
      "Epoch   240, Loss_1: 413.2860, Loss_1_test: 550.2999, Loss_2: 492.0452, Loss_2_test: 454.7902\n",
      "Epoch   250, Loss_1: 412.8535, Loss_1_test: 550.4722, Loss_2: 491.6439, Loss_2_test: 454.6208\n",
      "Epoch   260, Loss_1: 412.4886, Loss_1_test: 550.6918, Loss_2: 491.3600, Loss_2_test: 454.6134\n",
      "Epoch   270, Loss_1: 412.1574, Loss_1_test: 551.6660, Loss_2: 491.1394, Loss_2_test: 454.6306\n",
      "Epoch   280, Loss_1: 411.8668, Loss_1_test: 552.3975, Loss_2: 490.9431, Loss_2_test: 454.6264\n",
      "Epoch   290, Loss_1: 411.6189, Loss_1_test: 552.9648, Loss_2: 490.7502, Loss_2_test: 454.6674\n",
      "Epoch   300, Loss_1: 411.3697, Loss_1_test: 553.1337, Loss_2: 490.5659, Loss_2_test: 454.7269\n",
      "Epoch   310, Loss_1: 411.1378, Loss_1_test: 553.3947, Loss_2: 490.4026, Loss_2_test: 454.6929\n",
      "Epoch   320, Loss_1: 410.8787, Loss_1_test: 553.6379, Loss_2: 490.2483, Loss_2_test: 454.7151\n",
      "Epoch   330, Loss_1: 410.6389, Loss_1_test: 553.3954, Loss_2: 490.0964, Loss_2_test: 454.5227\n",
      "Epoch   340, Loss_1: 410.3484, Loss_1_test: 553.2424, Loss_2: 489.9479, Loss_2_test: 454.3645\n",
      "Epoch   350, Loss_1: 410.0651, Loss_1_test: 553.1917, Loss_2: 489.8023, Loss_2_test: 454.2029\n",
      "Epoch   360, Loss_1: 409.7674, Loss_1_test: 553.0037, Loss_2: 489.6635, Loss_2_test: 454.0827\n",
      "Epoch   370, Loss_1: 409.5219, Loss_1_test: 553.3841, Loss_2: 489.5117, Loss_2_test: 454.0159\n",
      "Epoch   380, Loss_1: 409.2803, Loss_1_test: 554.4484, Loss_2: 489.3491, Loss_2_test: 453.8877\n",
      "Epoch   390, Loss_1: 409.0493, Loss_1_test: 554.9683, Loss_2: 489.1741, Loss_2_test: 453.7382\n",
      "Epoch   400, Loss_1: 408.7993, Loss_1_test: 555.5104, Loss_2: 488.9777, Loss_2_test: 453.6332\n",
      "Epoch   410, Loss_1: 408.5685, Loss_1_test: 555.4348, Loss_2: 488.7649, Loss_2_test: 453.7795\n",
      "Epoch   420, Loss_1: 408.3700, Loss_1_test: 555.6606, Loss_2: 488.5663, Loss_2_test: 453.5865\n",
      "Epoch   430, Loss_1: 408.1770, Loss_1_test: 556.0189, Loss_2: 488.3676, Loss_2_test: 453.4423\n",
      "Epoch   440, Loss_1: 408.0341, Loss_1_test: 555.9720, Loss_2: 488.1616, Loss_2_test: 453.2936\n",
      "Epoch   450, Loss_1: 407.8860, Loss_1_test: 556.8275, Loss_2: 487.9469, Loss_2_test: 453.2731\n",
      "Epoch   460, Loss_1: 407.7191, Loss_1_test: 557.3232, Loss_2: 487.7291, Loss_2_test: 453.0240\n",
      "Epoch   470, Loss_1: 407.5767, Loss_1_test: 557.5814, Loss_2: 487.5146, Loss_2_test: 453.1750\n",
      "Epoch   480, Loss_1: 407.4051, Loss_1_test: 558.3093, Loss_2: 487.3250, Loss_2_test: 452.7358\n",
      "Epoch   490, Loss_1: 407.2622, Loss_1_test: 558.6978, Loss_2: 487.1167, Loss_2_test: 452.6641\n",
      "Epoch   500, Loss_1: 407.1293, Loss_1_test: 559.1238, Loss_2: 486.9306, Loss_2_test: 452.4023\n",
      "Epoch   510, Loss_1: 406.9756, Loss_1_test: 559.7330, Loss_2: 486.7377, Loss_2_test: 452.2418\n",
      "Epoch   520, Loss_1: 406.8299, Loss_1_test: 560.4395, Loss_2: 486.5612, Loss_2_test: 452.0429\n",
      "Epoch   530, Loss_1: 406.6896, Loss_1_test: 560.8619, Loss_2: 486.3786, Loss_2_test: 452.2242\n",
      "Epoch   540, Loss_1: 406.5969, Loss_1_test: 560.8357, Loss_2: 486.1857, Loss_2_test: 452.3405\n",
      "Epoch   550, Loss_1: 406.4880, Loss_1_test: 561.5905, Loss_2: 485.9924, Loss_2_test: 452.5348\n",
      "Epoch   560, Loss_1: 406.3889, Loss_1_test: 561.7694, Loss_2: 485.7862, Loss_2_test: 452.7516\n",
      "Epoch   570, Loss_1: 406.2960, Loss_1_test: 561.9587, Loss_2: 485.5802, Loss_2_test: 452.8491\n",
      "Epoch   580, Loss_1: 406.2077, Loss_1_test: 562.2935, Loss_2: 485.3677, Loss_2_test: 452.8842\n",
      "Epoch   590, Loss_1: 406.1158, Loss_1_test: 562.2944, Loss_2: 485.1401, Loss_2_test: 453.0634\n",
      "Epoch   600, Loss_1: 406.0293, Loss_1_test: 562.4451, Loss_2: 484.9096, Loss_2_test: 453.0711\n",
      "Epoch   610, Loss_1: 405.9344, Loss_1_test: 562.4069, Loss_2: 484.6711, Loss_2_test: 453.2677\n",
      "Epoch   620, Loss_1: 405.8672, Loss_1_test: 562.5145, Loss_2: 484.4139, Loss_2_test: 453.3299\n",
      "Epoch   630, Loss_1: 405.7738, Loss_1_test: 562.8911, Loss_2: 484.1605, Loss_2_test: 453.5257\n",
      "Epoch   640, Loss_1: 405.7189, Loss_1_test: 563.2515, Loss_2: 483.8781, Loss_2_test: 453.7720\n",
      "Epoch   650, Loss_1: 405.6137, Loss_1_test: 563.3876, Loss_2: 483.6324, Loss_2_test: 454.1895\n",
      "Epoch   660, Loss_1: 405.5526, Loss_1_test: 563.7593, Loss_2: 483.3813, Loss_2_test: 454.1872\n",
      "Epoch   670, Loss_1: 405.5028, Loss_1_test: 563.0676, Loss_2: 483.1262, Loss_2_test: 454.3111\n",
      "Epoch   680, Loss_1: 405.3781, Loss_1_test: 562.6129, Loss_2: 482.8334, Loss_2_test: 454.3077\n",
      "Epoch   690, Loss_1: 405.2895, Loss_1_test: 563.0465, Loss_2: 482.5286, Loss_2_test: 454.4211\n",
      "Epoch   700, Loss_1: 405.2031, Loss_1_test: 562.9469, Loss_2: 482.2580, Loss_2_test: 454.5608\n",
      "Epoch   710, Loss_1: 405.1012, Loss_1_test: 562.7504, Loss_2: 481.9984, Loss_2_test: 454.6635\n",
      "Epoch   720, Loss_1: 404.9782, Loss_1_test: 563.4269, Loss_2: 481.8002, Loss_2_test: 455.0489\n",
      "Epoch   730, Loss_1: 404.9201, Loss_1_test: 563.2283, Loss_2: 481.5978, Loss_2_test: 455.1458\n",
      "Epoch   740, Loss_1: 404.7559, Loss_1_test: 563.6470, Loss_2: 481.4107, Loss_2_test: 455.4802\n",
      "Epoch   750, Loss_1: 404.7260, Loss_1_test: 563.1471, Loss_2: 481.2212, Loss_2_test: 455.6908\n",
      "Epoch   760, Loss_1: 404.5789, Loss_1_test: 562.7684, Loss_2: 481.0402, Loss_2_test: 455.9602\n",
      "Epoch   770, Loss_1: 404.5200, Loss_1_test: 563.5751, Loss_2: 480.8650, Loss_2_test: 456.0164\n",
      "Epoch   780, Loss_1: 404.4900, Loss_1_test: 563.1456, Loss_2: 480.7156, Loss_2_test: 456.3320\n",
      "Epoch   790, Loss_1: 404.3383, Loss_1_test: 562.8688, Loss_2: 480.5754, Loss_2_test: 456.3766\n",
      "Epoch   800, Loss_1: 404.2487, Loss_1_test: 562.7209, Loss_2: 480.4370, Loss_2_test: 456.3753\n",
      "Epoch   810, Loss_1: 404.1603, Loss_1_test: 562.5308, Loss_2: 480.3037, Loss_2_test: 456.5731\n",
      "Epoch   820, Loss_1: 404.0732, Loss_1_test: 562.5894, Loss_2: 480.1606, Loss_2_test: 456.5775\n",
      "Epoch   830, Loss_1: 403.9935, Loss_1_test: 562.4020, Loss_2: 480.0188, Loss_2_test: 456.7546\n",
      "Epoch   840, Loss_1: 403.9445, Loss_1_test: 562.2765, Loss_2: 479.8857, Loss_2_test: 456.8047\n",
      "Epoch   850, Loss_1: 403.9297, Loss_1_test: 562.8434, Loss_2: 479.7531, Loss_2_test: 456.7950\n",
      "Epoch   860, Loss_1: 403.7778, Loss_1_test: 562.7592, Loss_2: 479.6380, Loss_2_test: 456.8741\n",
      "Epoch   870, Loss_1: 403.7158, Loss_1_test: 563.0156, Loss_2: 479.4828, Loss_2_test: 457.1406\n",
      "Epoch   880, Loss_1: 403.6322, Loss_1_test: 563.1022, Loss_2: 479.3310, Loss_2_test: 456.8996\n",
      "Epoch   890, Loss_1: 403.6378, Loss_1_test: 562.6802, Loss_2: 479.1619, Loss_2_test: 457.0497\n",
      "Epoch   900, Loss_1: 403.4822, Loss_1_test: 563.0308, Loss_2: 479.0106, Loss_2_test: 457.2752\n",
      "Epoch   910, Loss_1: 403.4437, Loss_1_test: 562.3145, Loss_2: 478.8189, Loss_2_test: 456.9613\n",
      "Epoch   920, Loss_1: 403.3439, Loss_1_test: 562.7015, Loss_2: 478.6103, Loss_2_test: 457.1968\n",
      "Epoch   930, Loss_1: 403.2321, Loss_1_test: 562.8971, Loss_2: 478.4399, Loss_2_test: 457.3381\n",
      "Epoch   940, Loss_1: 403.1718, Loss_1_test: 562.9564, Loss_2: 478.2787, Loss_2_test: 457.6624\n",
      "Epoch   950, Loss_1: 403.0634, Loss_1_test: 562.7580, Loss_2: 478.1265, Loss_2_test: 457.8085\n",
      "Epoch   960, Loss_1: 402.9777, Loss_1_test: 562.2852, Loss_2: 477.9532, Loss_2_test: 457.9552\n",
      "Epoch   970, Loss_1: 402.9337, Loss_1_test: 562.5599, Loss_2: 477.7825, Loss_2_test: 457.9409\n",
      "Epoch   980, Loss_1: 402.8353, Loss_1_test: 562.8394, Loss_2: 477.6359, Loss_2_test: 458.0776\n",
      "Epoch   990, Loss_1: 402.7821, Loss_1_test: 562.4074, Loss_2: 477.4894, Loss_2_test: 458.2603\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define custom loss function, and optimizer\n",
    "model1 = MyModel(input_size = data.shape[1])\n",
    "model2 = MyModel(input_size = data.shape[1])\n",
    "\n",
    "squared_hinge_loss = SquaredHingeLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "record_loss1_test = []\n",
    "record_loss2_test = []\n",
    "min_loss_1_test = float('inf')\n",
    "min_loss_2_test = float('inf')\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs1 = model1(data)\n",
    "    outputs2 = model2(data)\n",
    "    \n",
    "    # Compute the custom loss\n",
    "    loss_1 = squared_hinge_loss(outputs1, targets_low_1, targets_high_1)\n",
    "    loss_2 = squared_hinge_loss(outputs2, targets_low_2, targets_high_2)\n",
    "    loss_1_test = squared_hinge_loss(outputs1, targets_low_2, targets_high_2)\n",
    "    loss_2_test = squared_hinge_loss(outputs2, targets_low_1, targets_high_1)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # record\n",
    "    record_loss1_test.append(loss_1_test.item())\n",
    "    record_loss2_test.append(loss_2_test.item())\n",
    "\n",
    "    # save models\n",
    "    if loss_1_test < min_loss_1_test:\n",
    "        min_loss_1_test = loss_1_test\n",
    "        torch.save(model1.state_dict(), 'saved_models/model1_best.pth')\n",
    "    \n",
    "    if loss_2_test < min_loss_2_test:\n",
    "        min_loss_2_test = loss_2_test\n",
    "        torch.save(model2.state_dict(), 'saved_models/model2_best.pth')\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch {epoch:5d}, Loss_1: {loss_1.item():8.4f}, Loss_1_test: {loss_1_test.item():8.4f}, Loss_2: {loss_2.item():8.4f}, Loss_2_test: {loss_2_test.item():8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=15, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model1\n",
    "model1 = MyModel(input_size=data.shape[1])\n",
    "model1.load_state_dict(torch.load('saved_models/model1_best.pth'))\n",
    "model1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load model2\n",
    "model2 = MyModel(input_size=data.shape[1])\n",
    "model2.load_state_dict(torch.load('saved_models/model2_best.pth'))\n",
    "model2.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for fold 2:  545.6544799804688\n",
      "loss for fold 1:  451.9737854003906\n"
     ]
    }
   ],
   "source": [
    "print(\"loss for fold 2: \",squared_hinge_loss(model1(data), targets_low_2, targets_high_2).item())\n",
    "print(\"loss for fold 1: \",squared_hinge_loss(model2(data), targets_low_1, targets_high_1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ldas1 = model1(data).numpy().reshape(-1)\n",
    "    ldas2 = model2(data).numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs   = gen_data_dict('sequence_label_data/signals.gz')\n",
    "labels = gen_data_dict('sequence_label_data/labels.gz')\n",
    "\n",
    "header = ['sequenceID', 'fold_1_total_labels', 'fold_2_total_labels', 'fold_1_errs', 'fold_2_errs']\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    # generate data\n",
    "    sequence, neg_start_1, neg_end_1, pos_start_1, pos_end_1, neg_start_2, neg_end_2, pos_start_2, pos_end_2 = get_data(i, seqs=seqs, labels=labels)\n",
    "    sequence_length = len(sequence)-1\n",
    "\n",
    "    # vectors of cumulative sums\n",
    "    y, z = get_cumsum(sequence)\n",
    "\n",
    "    # get total labels\n",
    "    fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "    fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "    # run each lambda and record it into csv file\n",
    "    row  = [i, fold1_total_labels, fold2_total_labels]\n",
    "\n",
    "    chpnt_fold1 = opart(10**ldas2[i], sequence, y, z)\n",
    "    chpnt_fold2 = opart(10**ldas1[i], sequence, y, z)\n",
    "\n",
    "    err_1 = error_count(chpnt_fold1, neg_start_1, neg_end_1, pos_start_1, pos_end_1)\n",
    "    err_2 = error_count(chpnt_fold2, neg_start_2, neg_end_2, pos_start_2, pos_end_2)\n",
    "    \n",
    "    row.append(sum(err_1))\n",
    "    row.append(sum(err_2))\n",
    "\n",
    "    write_to_csv('learning_output/deep.csv', header, row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
