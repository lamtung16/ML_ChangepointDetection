{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1967f0ffcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.utility_functions import gen_data_dict, get_data, get_cumsum, error_count, write_to_csv, opart, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "seqs = gen_data_dict('sequence_label_data/signals.gz')\n",
    "\n",
    "# target \n",
    "target_df_1 = pd.read_csv('learning_data/target_lambda_fold1.csv')\n",
    "target_df_2 = pd.read_csv('learning_data/target_lambda_fold2.csv')\n",
    "\n",
    "targets_low_1  = target_df_1.iloc[:, 1:2].to_numpy()\n",
    "targets_high_1 = target_df_1.iloc[:, 2:3].to_numpy()\n",
    "targets_low_2  = target_df_2.iloc[:, 1:2].to_numpy()\n",
    "targets_high_2 = target_df_2.iloc[:, 2:3].to_numpy()\n",
    "\n",
    "targets_low_1  = torch.FloatTensor(targets_low_1)\n",
    "targets_high_1 = torch.FloatTensor(targets_high_1)\n",
    "targets_low_2  = torch.FloatTensor(targets_low_2)\n",
    "targets_high_2 = torch.FloatTensor(targets_high_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=8, num_layers=1, nonlinearity='tanh', bias=True, batch_first=False)\n",
    "        self.fc1 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x = self.rnn(x)\n",
    "        x    = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, Loss_1: 10147.3105, Loss_1_test: 10068.1797, Loss_2: 20591.3125, Loss_2_test: 20734.7539\n",
      "Epoch     1, Loss_1: 9631.8486, Loss_1_test: 9555.9932, Loss_2: 19731.6758, Loss_2_test: 19870.0117\n",
      "Epoch     2, Loss_1: 9125.3174, Loss_1_test: 9052.7002, Loss_2: 18895.6426, Loss_2_test: 19028.9922\n",
      "Epoch     3, Loss_1: 8628.2305, Loss_1_test: 8558.8115, Loss_2: 18083.2793, Loss_2_test: 18211.7676\n",
      "Epoch     4, Loss_1: 8141.0557, Loss_1_test: 8074.7939, Loss_2: 17294.4766, Loss_2_test: 17418.2266\n",
      "Epoch     5, Loss_1: 7664.3027, Loss_1_test: 7601.1533, Loss_2: 16528.9668, Loss_2_test: 16648.0957\n",
      "Epoch     6, Loss_1: 7198.4478, Loss_1_test: 7138.3643, Loss_2: 15786.1826, Loss_2_test: 15900.8096\n",
      "Epoch     7, Loss_1: 6743.9429, Loss_1_test: 6686.8765, Loss_2: 15065.3438, Loss_2_test: 15175.5801\n",
      "Epoch     8, Loss_1: 6301.2671, Loss_1_test: 6247.1646, Loss_2: 14365.6816, Loss_2_test: 14471.6396\n",
      "Epoch     9, Loss_1: 5870.8501, Loss_1_test: 5819.6587, Loss_2: 13686.4307, Loss_2_test: 13788.2129\n",
      "Epoch    10, Loss_1: 5453.1499, Loss_1_test: 5404.8115, Loss_2: 13027.0752, Loss_2_test: 13124.7812\n",
      "Epoch    11, Loss_1: 5048.5825, Loss_1_test: 5003.0381, Loss_2: 12387.1758, Loss_2_test: 12480.9092\n",
      "Epoch    12, Loss_1: 4657.5703, Loss_1_test: 4614.7583, Loss_2: 11766.4512, Loss_2_test: 11856.3076\n",
      "Epoch    13, Loss_1: 4280.5166, Loss_1_test: 4240.3735, Loss_2: 11164.6504, Loss_2_test: 11250.7285\n",
      "Epoch    14, Loss_1: 3917.8191, Loss_1_test: 3880.2778, Loss_2: 10581.5312, Loss_2_test: 10663.9238\n",
      "Epoch    15, Loss_1: 3569.8513, Loss_1_test: 3534.8450, Loss_2: 10016.8398, Loss_2_test: 10095.6436\n",
      "Epoch    16, Loss_1: 3236.9854, Loss_1_test: 3204.4419, Loss_2: 9470.3213, Loss_2_test: 9545.6270\n",
      "Epoch    17, Loss_1: 2919.5759, Loss_1_test: 2889.4231, Loss_2: 8941.7275, Loss_2_test: 9013.6279\n",
      "Epoch    18, Loss_1: 2617.9519, Loss_1_test: 2590.1147, Loss_2: 8430.7715, Loss_2_test: 8499.3574\n",
      "Epoch    19, Loss_1: 2332.4370, Loss_1_test: 2306.8374, Loss_2: 7937.1792, Loss_2_test: 8002.5386\n",
      "Epoch    20, Loss_1: 2063.3108, Loss_1_test: 2039.8699, Loss_2: 7460.7153, Loss_2_test: 7522.9370\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define custom loss function, and optimizer\n",
    "model1 = RNNModel()\n",
    "model2 = RNNModel()\n",
    "\n",
    "squared_hinge_loss = SquaredHingeLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "record_loss1_test = []\n",
    "record_loss2_test = []\n",
    "min_loss_1_test = float('inf')\n",
    "min_loss_2_test = float('inf')\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs1 = torch.tensor([[0.0]])\n",
    "    outputs2 = torch.tensor([[0.0]])\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        outputs1 += model1(seq)\n",
    "        outputs2 += model2(seq)\n",
    "    \n",
    "    # Compute the custom loss\n",
    "    loss_1 = squared_hinge_loss(outputs1, targets_low_1, targets_high_1)\n",
    "    loss_2 = squared_hinge_loss(outputs2, targets_low_2, targets_high_2)\n",
    "    loss_1_test = squared_hinge_loss(outputs1, targets_low_2, targets_high_2)\n",
    "    loss_2_test = squared_hinge_loss(outputs2, targets_low_1, targets_high_1)\n",
    "    \n",
    "    # Print the loss every epochs\n",
    "    if (epoch) % 1 == 0:\n",
    "        print(f'Epoch {epoch:5d}, Loss_1: {loss_1.item():8.4f}, Loss_1_test: {loss_1_test.item():8.4f}, Loss_2: {loss_2.item():8.4f}, Loss_2_test: {loss_2_test.item():8.4f}')\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # record\n",
    "    record_loss1_test.append(loss_1_test.item())\n",
    "    record_loss2_test.append(loss_2_test.item())\n",
    "\n",
    "    # save models\n",
    "    if loss_1_test < min_loss_1_test:\n",
    "        min_loss_1_test = loss_1_test\n",
    "        torch.save(model1.state_dict(), 'saved_models/model1_rnn_tanh_best.pth')\n",
    "    \n",
    "    if loss_2_test < min_loss_2_test:\n",
    "        min_loss_2_test = loss_2_test\n",
    "        torch.save(model2.state_dict(), 'saved_models/model2_rnn_tanh_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model1\n",
    "model1 = RNNModel()\n",
    "model1.load_state_dict(torch.load('saved_models/model1_rnn_tanh_best.pth'))\n",
    "model1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load model2\n",
    "model2 = RNNModel()\n",
    "model2.load_state_dict(torch.load('saved_models/model2_rnn_tanh_best.pth'))\n",
    "model2.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas1 = np.zeros(len(seqs))\n",
    "ldas2 = np.zeros(len(seqs))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        ldas1[i] = model1(seq).numpy()[0][0]\n",
    "        ldas2[i] = model2(seq).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs   = gen_data_dict('sequence_label_data/signals.gz')\n",
    "labels = gen_data_dict('sequence_label_data/labels.gz')\n",
    "\n",
    "header = ['sequenceID', 'fold_1_total_labels', 'fold_2_total_labels', 'fold_1_errs', 'fold_2_errs']\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    # generate data\n",
    "    sequence, neg_start_1, neg_end_1, pos_start_1, pos_end_1, neg_start_2, neg_end_2, pos_start_2, pos_end_2 = get_data(i, seqs=seqs, labels=labels)\n",
    "    sequence_length = len(sequence)-1\n",
    "\n",
    "    # vectors of cumulative sums\n",
    "    y, z = get_cumsum(sequence)\n",
    "\n",
    "    # get total labels\n",
    "    fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "    fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "    # run each lambda and record it into csv file\n",
    "    row  = [i, fold1_total_labels, fold2_total_labels]\n",
    "\n",
    "    chpnt_fold1 = opart(10**ldas2[i], sequence)\n",
    "    chpnt_fold2 = opart(10**ldas1[i], sequence)\n",
    "\n",
    "    err_1 = error_count(chpnt_fold1, neg_start_1, neg_end_1, pos_start_1, pos_end_1)\n",
    "    err_2 = error_count(chpnt_fold2, neg_start_2, neg_end_2, pos_start_2, pos_end_2)\n",
    "    \n",
    "    row.append(sum(err_1))\n",
    "    row.append(sum(err_2))\n",
    "\n",
    "    write_to_csv('learning_output/rnn_tanh.csv', header, row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
