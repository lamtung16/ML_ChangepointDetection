{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1967f0ffcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.utility_functions import gen_data_dict, get_data, get_cumsum, error_count, write_to_csv, opart, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "seqs = gen_data_dict('sequence_label_data/signals.gz')\n",
    "\n",
    "# target \n",
    "target_df_1 = pd.read_csv('learning_data/target_lambda_fold1.csv')\n",
    "target_df_2 = pd.read_csv('learning_data/target_lambda_fold2.csv')\n",
    "\n",
    "targets_low_1  = target_df_1.iloc[:, 1:2].to_numpy()\n",
    "targets_high_1 = target_df_1.iloc[:, 2:3].to_numpy()\n",
    "targets_low_2  = target_df_2.iloc[:, 1:2].to_numpy()\n",
    "targets_high_2 = target_df_2.iloc[:, 2:3].to_numpy()\n",
    "\n",
    "targets_low_1  = torch.FloatTensor(targets_low_1)\n",
    "targets_high_1 = torch.FloatTensor(targets_high_1)\n",
    "targets_low_2  = torch.FloatTensor(targets_low_2)\n",
    "targets_high_2 = torch.FloatTensor(targets_high_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=8, num_layers=1, nonlinearity='tanh', bias=True, batch_first=False)\n",
    "        self.fc1 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x = self.rnn(x)\n",
    "        x    = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, Loss_1: 10147.3105, Loss_1_test: 10068.1797, Loss_2: 20591.3125, Loss_2_test: 20734.7539\n",
      "Epoch     1, Loss_1: 9631.8486, Loss_1_test: 9555.9932, Loss_2: 19731.6758, Loss_2_test: 19870.0117\n",
      "Epoch     2, Loss_1: 9125.3174, Loss_1_test: 9052.7002, Loss_2: 18895.6426, Loss_2_test: 19028.9922\n",
      "Epoch     3, Loss_1: 8628.2305, Loss_1_test: 8558.8115, Loss_2: 18083.2793, Loss_2_test: 18211.7676\n",
      "Epoch     4, Loss_1: 8141.0557, Loss_1_test: 8074.7939, Loss_2: 17294.4766, Loss_2_test: 17418.2266\n",
      "Epoch     5, Loss_1: 7664.3027, Loss_1_test: 7601.1533, Loss_2: 16528.9668, Loss_2_test: 16648.0957\n",
      "Epoch     6, Loss_1: 7198.4478, Loss_1_test: 7138.3643, Loss_2: 15786.1826, Loss_2_test: 15900.8096\n",
      "Epoch     7, Loss_1: 6743.9429, Loss_1_test: 6686.8765, Loss_2: 15065.3438, Loss_2_test: 15175.5801\n",
      "Epoch     8, Loss_1: 6301.2671, Loss_1_test: 6247.1646, Loss_2: 14365.6816, Loss_2_test: 14471.6396\n",
      "Epoch     9, Loss_1: 5870.8501, Loss_1_test: 5819.6587, Loss_2: 13686.4307, Loss_2_test: 13788.2129\n",
      "Epoch    10, Loss_1: 5453.1499, Loss_1_test: 5404.8115, Loss_2: 13027.0752, Loss_2_test: 13124.7812\n",
      "Epoch    11, Loss_1: 5048.5825, Loss_1_test: 5003.0381, Loss_2: 12387.1758, Loss_2_test: 12480.9092\n",
      "Epoch    12, Loss_1: 4657.5703, Loss_1_test: 4614.7583, Loss_2: 11766.4512, Loss_2_test: 11856.3076\n",
      "Epoch    13, Loss_1: 4280.5166, Loss_1_test: 4240.3735, Loss_2: 11164.6504, Loss_2_test: 11250.7285\n",
      "Epoch    14, Loss_1: 3917.8191, Loss_1_test: 3880.2778, Loss_2: 10581.5312, Loss_2_test: 10663.9238\n",
      "Epoch    15, Loss_1: 3569.8513, Loss_1_test: 3534.8450, Loss_2: 10016.8398, Loss_2_test: 10095.6436\n",
      "Epoch    16, Loss_1: 3236.9854, Loss_1_test: 3204.4419, Loss_2: 9470.3213, Loss_2_test: 9545.6270\n",
      "Epoch    17, Loss_1: 2919.5759, Loss_1_test: 2889.4231, Loss_2: 8941.7275, Loss_2_test: 9013.6279\n",
      "Epoch    18, Loss_1: 2617.9519, Loss_1_test: 2590.1147, Loss_2: 8430.7715, Loss_2_test: 8499.3574\n",
      "Epoch    19, Loss_1: 2332.4370, Loss_1_test: 2306.8374, Loss_2: 7937.1792, Loss_2_test: 8002.5386\n",
      "Epoch    20, Loss_1: 2063.3108, Loss_1_test: 2039.8699, Loss_2: 7460.7153, Loss_2_test: 7522.9370\n",
      "Epoch    21, Loss_1: 1810.8511, Loss_1_test: 1789.4873, Loss_2: 7001.0894, Loss_2_test: 7060.2583\n",
      "Epoch    22, Loss_1: 1575.2810, Loss_1_test: 1555.9104, Loss_2: 6558.0640, Loss_2_test: 6614.2646\n",
      "Epoch    23, Loss_1: 1356.7916, Loss_1_test: 1339.3284, Loss_2: 6131.4116, Loss_2_test: 6184.7290\n",
      "Epoch    24, Loss_1: 1155.5245, Loss_1_test: 1139.8810, Loss_2: 5720.9248, Loss_2_test: 5771.4409\n",
      "Epoch    25, Loss_1: 971.5583, Loss_1_test: 957.6448, Loss_2: 5326.4067, Loss_2_test: 5374.2026\n",
      "Epoch    26, Loss_1: 804.9019, Loss_1_test: 792.6270, Loss_2: 4947.6973, Loss_2_test: 4992.8540\n",
      "Epoch    27, Loss_1: 655.4702, Loss_1_test: 644.7413, Loss_2: 4584.6294, Loss_2_test: 4627.2280\n",
      "Epoch    28, Loss_1: 523.0893, Loss_1_test: 513.8123, Loss_2: 4237.0923, Loss_2_test: 4277.2119\n",
      "Epoch    29, Loss_1: 407.4600, Loss_1_test: 399.5400, Loss_2: 3904.9570, Loss_2_test: 3942.6772\n",
      "Epoch    30, Loss_1: 308.1566, Loss_1_test: 301.4984, Loss_2: 3588.1191, Loss_2_test: 3623.5193\n",
      "Epoch    31, Loss_1: 224.6121, Loss_1_test: 219.1201, Loss_2: 3286.4863, Loss_2_test: 3319.6453\n",
      "Epoch    32, Loss_1: 156.1041, Loss_1_test: 151.6836, Loss_2: 2999.9561, Loss_2_test: 3030.9529\n",
      "Epoch    33, Loss_1: 101.7496, Loss_1_test:  98.3068, Loss_2: 2728.4280, Loss_2_test: 2757.3411\n",
      "Epoch    34, Loss_1:  60.5060, Loss_1_test:  57.9489, Loss_2: 2471.7971, Loss_2_test: 2498.7061\n",
      "Epoch    35, Loss_1:  31.1748, Loss_1_test:  29.4137, Loss_2: 2229.9404, Loss_2_test: 2254.9229\n",
      "Epoch    36, Loss_1:  12.4162, Loss_1_test:  11.3646, Loss_2: 2002.7227, Loss_2_test: 2025.8584\n",
      "Epoch    37, Loss_1:   2.8886, Loss_1_test:   2.5314, Loss_2: 1789.9773, Loss_2_test: 1811.3452\n",
      "Epoch    38, Loss_1:   1.3857, Loss_1_test:   2.2770, Loss_2: 1591.5154, Loss_2_test: 1611.1936\n",
      "Epoch    39, Loss_1:   4.2992, Loss_1_test:   7.9229, Loss_2: 1407.1218, Loss_2_test: 1425.1888\n",
      "Epoch    40, Loss_1:   9.3920, Loss_1_test:  16.8472, Loss_2: 1236.5382, Loss_2_test: 1253.0720\n",
      "Epoch    41, Loss_1:  15.7426, Loss_1_test:  27.8278, Loss_2: 1079.4712, Loss_2_test: 1094.5492\n",
      "Epoch    42, Loss_1:  22.8162, Loss_1_test:  39.9681, Loss_2: 935.5898, Loss_2_test: 949.2888\n",
      "Epoch    43, Loss_1:  30.1547, Loss_1_test:  52.5062, Loss_2: 804.5189, Loss_2_test: 816.9147\n",
      "Epoch    44, Loss_1:  37.3746, Loss_1_test:  64.8051, Loss_2: 685.8409, Loss_2_test: 697.0082\n",
      "Epoch    45, Loss_1:  44.1652, Loss_1_test:  76.3488, Loss_2: 579.0983, Loss_2_test: 589.1108\n",
      "Epoch    46, Loss_1:  50.2848, Loss_1_test:  86.7364, Loss_2: 483.7897, Loss_2_test: 492.7196\n",
      "Epoch    47, Loss_1:  55.5561, Loss_1_test:  95.6745, Loss_2: 399.3742, Loss_2_test: 407.2921\n",
      "Epoch    48, Loss_1:  59.8615, Loss_1_test: 102.9691, Loss_2: 325.2723, Loss_2_test: 332.2470\n",
      "Epoch    49, Loss_1:  63.1361, Loss_1_test: 108.5142, Loss_2: 260.8727, Loss_2_test: 266.9710\n",
      "Epoch    50, Loss_1:  65.3613, Loss_1_test: 112.2809, Loss_2: 205.5339, Loss_2_test: 210.8204\n",
      "Epoch    51, Loss_1:  66.5579, Loss_1_test: 114.3061, Loss_2: 158.5904, Loss_2_test: 163.1275\n",
      "Epoch    52, Loss_1:  66.7782, Loss_1_test: 114.6789, Loss_2: 119.3591, Loss_2_test: 123.2064\n",
      "Epoch    53, Loss_1:  66.0999, Loss_1_test: 113.5310, Loss_2:  87.1446, Loss_2_test:  90.3596\n",
      "Epoch    54, Loss_1:  64.6185, Loss_1_test: 111.0236, Loss_2:  61.2485, Loss_2_test:  63.8857\n",
      "Epoch    55, Loss_1:  62.4413, Loss_1_test: 107.3378, Loss_2:  40.9752, Loss_2_test:  43.0865\n",
      "Epoch    56, Loss_1:  59.6824, Loss_1_test: 102.6657, Loss_2:  25.6405, Loss_2_test:  27.2748\n",
      "Epoch    57, Loss_1:  56.4572, Loss_1_test:  97.2016, Loss_2:  14.5785, Loss_2_test:  15.7821\n",
      "Epoch    58, Loss_1:  52.8792, Loss_1_test:  91.1365, Loss_2:   7.1498, Loss_2_test:   7.9660\n",
      "Epoch    59, Loss_1:  49.0558, Loss_1_test:  84.6512, Loss_2:   2.8624, Loss_2_test:   3.2956\n",
      "Epoch    60, Loss_1:  45.0862, Loss_1_test:  77.9130, Loss_2:   1.5233, Loss_2_test:   1.3937\n",
      "Epoch    61, Loss_1:  41.0601, Loss_1_test:  71.0727, Loss_2:   2.2139, Loss_2_test:   1.3625\n",
      "Epoch    62, Loss_1:  37.0564, Loss_1_test:  64.2635, Loss_2:   4.3804, Loss_2_test:   2.3690\n",
      "Epoch    63, Loss_1:  33.1423, Loss_1_test:  57.5991, Loss_2:   7.3158, Loss_2_test:   3.9597\n",
      "Epoch    64, Loss_1:  29.3736, Loss_1_test:  51.1736, Loss_2:  10.6331, Loss_2_test:   5.8363\n",
      "Epoch    65, Loss_1:  25.7948, Loss_1_test:  45.0626, Loss_2:  14.0785, Loss_2_test:   7.8029\n",
      "Epoch    66, Loss_1:  22.4398, Loss_1_test:  39.3235, Loss_2:  17.4588, Loss_2_test:   9.7439\n",
      "Epoch    67, Loss_1:  19.3325, Loss_1_test:  33.9976, Loss_2:  20.6282, Loss_2_test:  11.5715\n",
      "Epoch    68, Loss_1:  16.4883, Loss_1_test:  29.1112, Loss_2:  23.4777, Loss_2_test:  13.2195\n",
      "Epoch    69, Loss_1:  13.9146, Loss_1_test:  24.6775, Loss_2:  25.9302, Loss_2_test:  14.6410\n",
      "Epoch    70, Loss_1:  11.6120, Loss_1_test:  20.6984, Loss_2:  27.9364, Loss_2_test:  15.8057\n",
      "Epoch    71, Loss_1:   9.5757, Loss_1_test:  17.1665, Loss_2:  29.4713, Loss_2_test:  16.6977\n",
      "Epoch    72, Loss_1:   7.7960, Loss_1_test:  14.0664, Loss_2:  30.5298, Loss_2_test:  17.3133\n",
      "Epoch    73, Loss_1:   6.2598, Loss_1_test:  11.3770, Loss_2:  31.1235, Loss_2_test:  17.6588\n",
      "Epoch    74, Loss_1:   4.9514, Loss_1_test:   9.0755, Loss_2:  31.2770, Loss_2_test:  17.7481\n",
      "Epoch    75, Loss_1:   3.8586, Loss_1_test:   7.1332, Loss_2:  31.0250, Loss_2_test:  17.6014\n",
      "Epoch    76, Loss_1:   2.9703, Loss_1_test:   5.5201, Loss_2:  30.4095, Loss_2_test:  17.2433\n",
      "Epoch    77, Loss_1:   2.2869, Loss_1_test:   4.2236, Loss_2:  29.4771, Loss_2_test:  16.7011\n",
      "Epoch    78, Loss_1:   1.7927, Loss_1_test:   3.2084, Loss_2:  28.2769, Loss_2_test:  16.0035\n",
      "Epoch    79, Loss_1:   1.4471, Loss_1_test:   2.4398, Loss_2:  26.8587, Loss_2_test:  15.1799\n",
      "Epoch    80, Loss_1:   1.2649, Loss_1_test:   1.9332, Loss_2:  25.2714, Loss_2_test:  14.2589\n",
      "Epoch    81, Loss_1:   1.1935, Loss_1_test:   1.6257, Loss_2:  23.5616, Loss_2_test:  13.2681\n",
      "Epoch    82, Loss_1:   1.2544, Loss_1_test:   1.5266, Loss_2:  21.7730, Loss_2_test:  12.2331\n",
      "Epoch    83, Loss_1:   1.3736, Loss_1_test:   1.5133, Loss_2:  19.9454, Loss_2_test:  11.1772\n",
      "Epoch    84, Loss_1:   1.5722, Loss_1_test:   1.6171, Loss_2:  18.1143, Loss_2_test:  10.1213\n",
      "Epoch    85, Loss_1:   1.7792, Loss_1_test:   1.7341, Loss_2:  16.3107, Loss_2_test:   9.0835\n",
      "Epoch    86, Loss_1:   1.9882, Loss_1_test:   1.8648, Loss_2:  14.5606, Loss_2_test:   8.0791\n",
      "Epoch    87, Loss_1:   2.2023, Loss_1_test:   2.0229, Loss_2:  12.8858, Loss_2_test:   7.1206\n",
      "Epoch    88, Loss_1:   2.3756, Loss_1_test:   2.1511, Loss_2:  11.3036, Loss_2_test:   6.2180\n",
      "Epoch    89, Loss_1:   2.5009, Loss_1_test:   2.2438, Loss_2:   9.8279, Loss_2_test:   5.3785\n",
      "Epoch    90, Loss_1:   2.5752, Loss_1_test:   2.2989, Loss_2:   8.4690, Loss_2_test:   4.6078\n",
      "Epoch    91, Loss_1:   2.5988, Loss_1_test:   2.3164, Loss_2:   7.2310, Loss_2_test:   3.9127\n",
      "Epoch    92, Loss_1:   2.5752, Loss_1_test:   2.2989, Loss_2:   6.1175, Loss_2_test:   3.2981\n",
      "Epoch    93, Loss_1:   2.5097, Loss_1_test:   2.2504, Loss_2:   5.1327, Loss_2_test:   2.7650\n",
      "Epoch    94, Loss_1:   2.4092, Loss_1_test:   2.1759, Loss_2:   4.2777, Loss_2_test:   2.3152\n",
      "Epoch    95, Loss_1:   2.2813, Loss_1_test:   2.0813, Loss_2:   3.5456, Loss_2_test:   1.9473\n",
      "Epoch    96, Loss_1:   2.1343, Loss_1_test:   1.9726, Loss_2:   2.9367, Loss_2_test:   1.6689\n",
      "Epoch    97, Loss_1:   1.9759, Loss_1_test:   1.8558, Loss_2:   2.4337, Loss_2_test:   1.4447\n",
      "Epoch    98, Loss_1:   1.8356, Loss_1_test:   1.7669, Loss_2:   2.0683, Loss_2_test:   1.3106\n",
      "Epoch    99, Loss_1:   1.7079, Loss_1_test:   1.6931, Loss_2:   1.7771, Loss_2_test:   1.2158\n",
      "Epoch   100, Loss_1:   1.5886, Loss_1_test:   1.6261, Loss_2:   1.6154, Loss_2_test:   1.1962\n",
      "Epoch   101, Loss_1:   1.4799, Loss_1_test:   1.5675, Loss_2:   1.5442, Loss_2_test:   1.2347\n",
      "Epoch   102, Loss_1:   1.3832, Loss_1_test:   1.5180, Loss_2:   1.5051, Loss_2_test:   1.2946\n",
      "Epoch   103, Loss_1:   1.3126, Loss_1_test:   1.4993, Loss_2:   1.5210, Loss_2_test:   1.3891\n",
      "Epoch   104, Loss_1:   1.2707, Loss_1_test:   1.5161, Loss_2:   1.5864, Loss_2_test:   1.5156\n",
      "Epoch   105, Loss_1:   1.2388, Loss_1_test:   1.5401, Loss_2:   1.6565, Loss_2_test:   1.6434\n",
      "Epoch   106, Loss_1:   1.2156, Loss_1_test:   1.5697, Loss_2:   1.7272, Loss_2_test:   1.7673\n",
      "Epoch   107, Loss_1:   1.1999, Loss_1_test:   1.6036, Loss_2:   1.7953, Loss_2_test:   1.8837\n",
      "Epoch   108, Loss_1:   1.1905, Loss_1_test:   1.6402, Loss_2:   1.8747, Loss_2_test:   2.0016\n",
      "Epoch   109, Loss_1:   1.1884, Loss_1_test:   1.6820, Loss_2:   1.9556, Loss_2_test:   2.1112\n",
      "Epoch   110, Loss_1:   1.2088, Loss_1_test:   1.7533, Loss_2:   2.0209, Loss_2_test:   2.1996\n",
      "Epoch   111, Loss_1:   1.2277, Loss_1_test:   1.8163, Loss_2:   2.0699, Loss_2_test:   2.2658\n",
      "Epoch   112, Loss_1:   1.2445, Loss_1_test:   1.8699, Loss_2:   2.1027, Loss_2_test:   2.3102\n",
      "Epoch   113, Loss_1:   1.2586, Loss_1_test:   1.9136, Loss_2:   2.1199, Loss_2_test:   2.3335\n",
      "Epoch   114, Loss_1:   1.2696, Loss_1_test:   1.9473, Loss_2:   2.1226, Loss_2_test:   2.3371\n",
      "Epoch   115, Loss_1:   1.2775, Loss_1_test:   1.9710, Loss_2:   2.1119, Loss_2_test:   2.3227\n",
      "Epoch   116, Loss_1:   1.2822, Loss_1_test:   1.9852, Loss_2:   2.0894, Loss_2_test:   2.2922\n",
      "Epoch   117, Loss_1:   1.2840, Loss_1_test:   1.9904, Loss_2:   2.0566, Loss_2_test:   2.2479\n",
      "Epoch   118, Loss_1:   1.2829, Loss_1_test:   1.9873, Loss_2:   2.0151, Loss_2_test:   2.1918\n",
      "Epoch   119, Loss_1:   1.2794, Loss_1_test:   1.9767, Loss_2:   1.9667, Loss_2_test:   2.1262\n",
      "Epoch   120, Loss_1:   1.2737, Loss_1_test:   1.9596, Loss_2:   1.9129, Loss_2_test:   2.0533\n",
      "Epoch   121, Loss_1:   1.2661, Loss_1_test:   1.9368, Loss_2:   1.8551, Loss_2_test:   1.9750\n",
      "Epoch   122, Loss_1:   1.2571, Loss_1_test:   1.9093, Loss_2:   1.8055, Loss_2_test:   1.9009\n",
      "Epoch   123, Loss_1:   1.2471, Loss_1_test:   1.8779, Loss_2:   1.7671, Loss_2_test:   1.8357\n",
      "Epoch   124, Loss_1:   1.2362, Loss_1_test:   1.8436, Loss_2:   1.7305, Loss_2_test:   1.7730\n",
      "Epoch   125, Loss_1:   1.2249, Loss_1_test:   1.8072, Loss_2:   1.6959, Loss_2_test:   1.7129\n",
      "Epoch   126, Loss_1:   1.2135, Loss_1_test:   1.7694, Loss_2:   1.6635, Loss_2_test:   1.6557\n",
      "Epoch   127, Loss_1:   1.2022, Loss_1_test:   1.7308, Loss_2:   1.6333, Loss_2_test:   1.6017\n",
      "Epoch   128, Loss_1:   1.1912, Loss_1_test:   1.6922, Loss_2:   1.6055, Loss_2_test:   1.5509\n",
      "Epoch   129, Loss_1:   1.1873, Loss_1_test:   1.6645, Loss_2:   1.5799, Loss_2_test:   1.5034\n",
      "Epoch   130, Loss_1:   1.1892, Loss_1_test:   1.6486, Loss_2:   1.5566, Loss_2_test:   1.4591\n",
      "Epoch   131, Loss_1:   1.1915, Loss_1_test:   1.6353, Loss_2:   1.5356, Loss_2_test:   1.4182\n",
      "Epoch   132, Loss_1:   1.1939, Loss_1_test:   1.6243, Loss_2:   1.5167, Loss_2_test:   1.3805\n",
      "Epoch   133, Loss_1:   1.1962, Loss_1_test:   1.6155, Loss_2:   1.4998, Loss_2_test:   1.3458\n",
      "Epoch   134, Loss_1:   1.1983, Loss_1_test:   1.6085, Loss_2:   1.4971, Loss_2_test:   1.3218\n",
      "Epoch   135, Loss_1:   1.2001, Loss_1_test:   1.6031, Loss_2:   1.5014, Loss_2_test:   1.3053\n",
      "Epoch   136, Loss_1:   1.2015, Loss_1_test:   1.5992, Loss_2:   1.5062, Loss_2_test:   1.2916\n",
      "Epoch   137, Loss_1:   1.2025, Loss_1_test:   1.5966, Loss_2:   1.5111, Loss_2_test:   1.2805\n",
      "Epoch   138, Loss_1:   1.2031, Loss_1_test:   1.5952, Loss_2:   1.5157, Loss_2_test:   1.2714\n",
      "Epoch   139, Loss_1:   1.2032, Loss_1_test:   1.5948, Loss_2:   1.5200, Loss_2_test:   1.2641\n",
      "Epoch   140, Loss_1:   1.2030, Loss_1_test:   1.5954, Loss_2:   1.5238, Loss_2_test:   1.2583\n",
      "Epoch   141, Loss_1:   1.2024, Loss_1_test:   1.5968, Loss_2:   1.5270, Loss_2_test:   1.2539\n",
      "Epoch   142, Loss_1:   1.2016, Loss_1_test:   1.5990, Loss_2:   1.5296, Loss_2_test:   1.2505\n",
      "Epoch   143, Loss_1:   1.2005, Loss_1_test:   1.6019, Loss_2:   1.5315, Loss_2_test:   1.2482\n",
      "Epoch   144, Loss_1:   1.1993, Loss_1_test:   1.6055, Loss_2:   1.5328, Loss_2_test:   1.2467\n",
      "Epoch   145, Loss_1:   1.1979, Loss_1_test:   1.6097, Loss_2:   1.5334, Loss_2_test:   1.2460\n",
      "Epoch   146, Loss_1:   1.1965, Loss_1_test:   1.6143, Loss_2:   1.5335, Loss_2_test:   1.2459\n",
      "Epoch   147, Loss_1:   1.1951, Loss_1_test:   1.6194, Loss_2:   1.5330, Loss_2_test:   1.2465\n",
      "Epoch   148, Loss_1:   1.1937, Loss_1_test:   1.6249, Loss_2:   1.5320, Loss_2_test:   1.2476\n",
      "Epoch   149, Loss_1:   1.1924, Loss_1_test:   1.6307, Loss_2:   1.5307, Loss_2_test:   1.2492\n",
      "Epoch   150, Loss_1:   1.1912, Loss_1_test:   1.6368, Loss_2:   1.5290, Loss_2_test:   1.2514\n",
      "Epoch   151, Loss_1:   1.1901, Loss_1_test:   1.6430, Loss_2:   1.5270, Loss_2_test:   1.2539\n",
      "Epoch   152, Loss_1:   1.1891, Loss_1_test:   1.6493, Loss_2:   1.5248, Loss_2_test:   1.2569\n",
      "Epoch   153, Loss_1:   1.1882, Loss_1_test:   1.6557, Loss_2:   1.5225, Loss_2_test:   1.2603\n",
      "Epoch   154, Loss_1:   1.1875, Loss_1_test:   1.6620, Loss_2:   1.5201, Loss_2_test:   1.2641\n",
      "Epoch   155, Loss_1:   1.1869, Loss_1_test:   1.6682, Loss_2:   1.5176, Loss_2_test:   1.2681\n",
      "Epoch   156, Loss_1:   1.1865, Loss_1_test:   1.6743, Loss_2:   1.5151, Loss_2_test:   1.2725\n",
      "Epoch   157, Loss_1:   1.1893, Loss_1_test:   1.6854, Loss_2:   1.5127, Loss_2_test:   1.2771\n",
      "Epoch   158, Loss_1:   1.1909, Loss_1_test:   1.6912, Loss_2:   1.5104, Loss_2_test:   1.2819\n",
      "Epoch   159, Loss_1:   1.1910, Loss_1_test:   1.6914, Loss_2:   1.5081, Loss_2_test:   1.2870\n",
      "Epoch   160, Loss_1:   1.1896, Loss_1_test:   1.6865, Loss_2:   1.5060, Loss_2_test:   1.2922\n",
      "Epoch   161, Loss_1:   1.1870, Loss_1_test:   1.6771, Loss_2:   1.5040, Loss_2_test:   1.2975\n",
      "Epoch   162, Loss_1:   1.1868, Loss_1_test:   1.6695, Loss_2:   1.5022, Loss_2_test:   1.3030\n",
      "Epoch   163, Loss_1:   1.1873, Loss_1_test:   1.6640, Loss_2:   1.5005, Loss_2_test:   1.3085\n",
      "Epoch   164, Loss_1:   1.1878, Loss_1_test:   1.6595, Loss_2:   1.4990, Loss_2_test:   1.3140\n",
      "Epoch   165, Loss_1:   1.1882, Loss_1_test:   1.6559, Loss_2:   1.4976, Loss_2_test:   1.3195\n",
      "Epoch   166, Loss_1:   1.1886, Loss_1_test:   1.6532, Loss_2:   1.4964, Loss_2_test:   1.3250\n",
      "Epoch   167, Loss_1:   1.1888, Loss_1_test:   1.6513, Loss_2:   1.4953, Loss_2_test:   1.3304\n",
      "Epoch   168, Loss_1:   1.1890, Loss_1_test:   1.6501, Loss_2:   1.4953, Loss_2_test:   1.3363\n",
      "Epoch   169, Loss_1:   1.1890, Loss_1_test:   1.6496, Loss_2:   1.4979, Loss_2_test:   1.3417\n",
      "Epoch   170, Loss_1:   1.1890, Loss_1_test:   1.6497, Loss_2:   1.4991, Loss_2_test:   1.3444\n",
      "Epoch   171, Loss_1:   1.1889, Loss_1_test:   1.6503, Loss_2:   1.4992, Loss_2_test:   1.3444\n",
      "Epoch   172, Loss_1:   1.1888, Loss_1_test:   1.6515, Loss_2:   1.4980, Loss_2_test:   1.3421\n",
      "Epoch   173, Loss_1:   1.1886, Loss_1_test:   1.6530, Loss_2:   1.4959, Loss_2_test:   1.3377\n",
      "Epoch   174, Loss_1:   1.1883, Loss_1_test:   1.6550, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   175, Loss_1:   1.1881, Loss_1_test:   1.6573, Loss_2:   1.4955, Loss_2_test:   1.3295\n",
      "Epoch   176, Loss_1:   1.1878, Loss_1_test:   1.6598, Loss_2:   1.4960, Loss_2_test:   1.3270\n",
      "Epoch   177, Loss_1:   1.1875, Loss_1_test:   1.6626, Loss_2:   1.4964, Loss_2_test:   1.3251\n",
      "Epoch   178, Loss_1:   1.1872, Loss_1_test:   1.6656, Loss_2:   1.4966, Loss_2_test:   1.3238\n",
      "Epoch   179, Loss_1:   1.1869, Loss_1_test:   1.6687, Loss_2:   1.4968, Loss_2_test:   1.3230\n",
      "Epoch   180, Loss_1:   1.1866, Loss_1_test:   1.6719, Loss_2:   1.4969, Loss_2_test:   1.3227\n",
      "Epoch   181, Loss_1:   1.1866, Loss_1_test:   1.6755, Loss_2:   1.4968, Loss_2_test:   1.3228\n",
      "Epoch   182, Loss_1:   1.1868, Loss_1_test:   1.6762, Loss_2:   1.4967, Loss_2_test:   1.3233\n",
      "Epoch   183, Loss_1:   1.1865, Loss_1_test:   1.6733, Loss_2:   1.4966, Loss_2_test:   1.3241\n",
      "Epoch   184, Loss_1:   1.1867, Loss_1_test:   1.6716, Loss_2:   1.4963, Loss_2_test:   1.3253\n",
      "Epoch   185, Loss_1:   1.1868, Loss_1_test:   1.6705, Loss_2:   1.4960, Loss_2_test:   1.3267\n",
      "Epoch   186, Loss_1:   1.1868, Loss_1_test:   1.6697, Loss_2:   1.4957, Loss_2_test:   1.3284\n",
      "Epoch   187, Loss_1:   1.1868, Loss_1_test:   1.6695, Loss_2:   1.4953, Loss_2_test:   1.3303\n",
      "Epoch   188, Loss_1:   1.1868, Loss_1_test:   1.6696, Loss_2:   1.4950, Loss_2_test:   1.3323\n",
      "Epoch   189, Loss_1:   1.1868, Loss_1_test:   1.6701, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   190, Loss_1:   1.1867, Loss_1_test:   1.6709, Loss_2:   1.4962, Loss_2_test:   1.3382\n",
      "Epoch   191, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4967, Loss_2_test:   1.3392\n",
      "Epoch   192, Loss_1:   1.1865, Loss_1_test:   1.6733, Loss_2:   1.4959, Loss_2_test:   1.3376\n",
      "Epoch   193, Loss_1:   1.1864, Loss_1_test:   1.6749, Loss_2:   1.4947, Loss_2_test:   1.3342\n",
      "Epoch   194, Loss_1:   1.1874, Loss_1_test:   1.6783, Loss_2:   1.4950, Loss_2_test:   1.3323\n",
      "Epoch   195, Loss_1:   1.1867, Loss_1_test:   1.6760, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   196, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4953, Loss_2_test:   1.3302\n",
      "Epoch   197, Loss_1:   1.1869, Loss_1_test:   1.6688, Loss_2:   1.4954, Loss_2_test:   1.3299\n",
      "Epoch   198, Loss_1:   1.1871, Loss_1_test:   1.6665, Loss_2:   1.4954, Loss_2_test:   1.3300\n",
      "Epoch   199, Loss_1:   1.1872, Loss_1_test:   1.6649, Loss_2:   1.4953, Loss_2_test:   1.3304\n",
      "Epoch   200, Loss_1:   1.1873, Loss_1_test:   1.6639, Loss_2:   1.4952, Loss_2_test:   1.3311\n",
      "Epoch   201, Loss_1:   1.1874, Loss_1_test:   1.6634, Loss_2:   1.4950, Loss_2_test:   1.3322\n",
      "Epoch   202, Loss_1:   1.1874, Loss_1_test:   1.6634, Loss_2:   1.4948, Loss_2_test:   1.3335\n",
      "Epoch   203, Loss_1:   1.1873, Loss_1_test:   1.6639, Loss_2:   1.4948, Loss_2_test:   1.3352\n",
      "Epoch   204, Loss_1:   1.1872, Loss_1_test:   1.6648, Loss_2:   1.4946, Loss_2_test:   1.3349\n",
      "Epoch   205, Loss_1:   1.1871, Loss_1_test:   1.6661, Loss_2:   1.4948, Loss_2_test:   1.3332\n",
      "Epoch   206, Loss_1:   1.1870, Loss_1_test:   1.6676, Loss_2:   1.4950, Loss_2_test:   1.3320\n",
      "Epoch   207, Loss_1:   1.1868, Loss_1_test:   1.6695, Loss_2:   1.4952, Loss_2_test:   1.3313\n",
      "Epoch   208, Loss_1:   1.1867, Loss_1_test:   1.6715, Loss_2:   1.4952, Loss_2_test:   1.3310\n",
      "Epoch   209, Loss_1:   1.1865, Loss_1_test:   1.6738, Loss_2:   1.4952, Loss_2_test:   1.3312\n",
      "Epoch   210, Loss_1:   1.1871, Loss_1_test:   1.6774, Loss_2:   1.4951, Loss_2_test:   1.3317\n",
      "Epoch   211, Loss_1:   1.1867, Loss_1_test:   1.6760, Loss_2:   1.4949, Loss_2_test:   1.3325\n",
      "Epoch   212, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4947, Loss_2_test:   1.3336\n",
      "Epoch   213, Loss_1:   1.1868, Loss_1_test:   1.6694, Loss_2:   1.4948, Loss_2_test:   1.3352\n",
      "Epoch   214, Loss_1:   1.1870, Loss_1_test:   1.6674, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   215, Loss_1:   1.1871, Loss_1_test:   1.6660, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   216, Loss_1:   1.1872, Loss_1_test:   1.6653, Loss_2:   1.4948, Loss_2_test:   1.3353\n",
      "Epoch   217, Loss_1:   1.1872, Loss_1_test:   1.6650, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   218, Loss_1:   1.1872, Loss_1_test:   1.6652, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   219, Loss_1:   1.1871, Loss_1_test:   1.6659, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   220, Loss_1:   1.1871, Loss_1_test:   1.6669, Loss_2:   1.4949, Loss_2_test:   1.3328\n",
      "Epoch   221, Loss_1:   1.1869, Loss_1_test:   1.6683, Loss_2:   1.4948, Loss_2_test:   1.3332\n",
      "Epoch   222, Loss_1:   1.1868, Loss_1_test:   1.6700, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   223, Loss_1:   1.1867, Loss_1_test:   1.6719, Loss_2:   1.4948, Loss_2_test:   1.3353\n",
      "Epoch   224, Loss_1:   1.1865, Loss_1_test:   1.6740, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   225, Loss_1:   1.1872, Loss_1_test:   1.6777, Loss_2:   1.4946, Loss_2_test:   1.3342\n",
      "Epoch   226, Loss_1:   1.1867, Loss_1_test:   1.6759, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   227, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4946, Loss_2_test:   1.3349\n",
      "Epoch   228, Loss_1:   1.1869, Loss_1_test:   1.6688, Loss_2:   1.4947, Loss_2_test:   1.3337\n",
      "Epoch   229, Loss_1:   1.1871, Loss_1_test:   1.6667, Loss_2:   1.4949, Loss_2_test:   1.3330\n",
      "Epoch   230, Loss_1:   1.1872, Loss_1_test:   1.6651, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   231, Loss_1:   1.1873, Loss_1_test:   1.6643, Loss_2:   1.4949, Loss_2_test:   1.3329\n",
      "Epoch   232, Loss_1:   1.1873, Loss_1_test:   1.6639, Loss_2:   1.4948, Loss_2_test:   1.3334\n",
      "Epoch   233, Loss_1:   1.1873, Loss_1_test:   1.6641, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   234, Loss_1:   1.1872, Loss_1_test:   1.6648, Loss_2:   1.4951, Loss_2_test:   1.3358\n",
      "Epoch   235, Loss_1:   1.1871, Loss_1_test:   1.6659, Loss_2:   1.4946, Loss_2_test:   1.3348\n",
      "Epoch   236, Loss_1:   1.1870, Loss_1_test:   1.6673, Loss_2:   1.4949, Loss_2_test:   1.3325\n",
      "Epoch   237, Loss_1:   1.1869, Loss_1_test:   1.6691, Loss_2:   1.4952, Loss_2_test:   1.3309\n",
      "Epoch   238, Loss_1:   1.1867, Loss_1_test:   1.6711, Loss_2:   1.4954, Loss_2_test:   1.3298\n",
      "Epoch   239, Loss_1:   1.1865, Loss_1_test:   1.6734, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   240, Loss_1:   1.1869, Loss_1_test:   1.6767, Loss_2:   1.4956, Loss_2_test:   1.3291\n",
      "Epoch   241, Loss_1:   1.1864, Loss_1_test:   1.6750, Loss_2:   1.4955, Loss_2_test:   1.3295\n",
      "Epoch   242, Loss_1:   1.1867, Loss_1_test:   1.6712, Loss_2:   1.4953, Loss_2_test:   1.3302\n",
      "Epoch   243, Loss_1:   1.1869, Loss_1_test:   1.6682, Loss_2:   1.4952, Loss_2_test:   1.3313\n",
      "Epoch   244, Loss_1:   1.1871, Loss_1_test:   1.6660, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   245, Loss_1:   1.1873, Loss_1_test:   1.6645, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   246, Loss_1:   1.1874, Loss_1_test:   1.6637, Loss_2:   1.4957, Loss_2_test:   1.3371\n",
      "Epoch   247, Loss_1:   1.1874, Loss_1_test:   1.6634, Loss_2:   1.4956, Loss_2_test:   1.3370\n",
      "Epoch   248, Loss_1:   1.1874, Loss_1_test:   1.6637, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   249, Loss_1:   1.1873, Loss_1_test:   1.6644, Loss_2:   1.4948, Loss_2_test:   1.3331\n",
      "Epoch   250, Loss_1:   1.1872, Loss_1_test:   1.6656, Loss_2:   1.4949, Loss_2_test:   1.3324\n",
      "Epoch   251, Loss_1:   1.1870, Loss_1_test:   1.6672, Loss_2:   1.4950, Loss_2_test:   1.3322\n",
      "Epoch   252, Loss_1:   1.1869, Loss_1_test:   1.6691, Loss_2:   1.4950, Loss_2_test:   1.3324\n",
      "Epoch   253, Loss_1:   1.1867, Loss_1_test:   1.6712, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   254, Loss_1:   1.1865, Loss_1_test:   1.6736, Loss_2:   1.4947, Loss_2_test:   1.3339\n",
      "Epoch   255, Loss_1:   1.1871, Loss_1_test:   1.6773, Loss_2:   1.4949, Loss_2_test:   1.3355\n",
      "Epoch   256, Loss_1:   1.1866, Loss_1_test:   1.6757, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   257, Loss_1:   1.1867, Loss_1_test:   1.6714, Loss_2:   1.4946, Loss_2_test:   1.3344\n",
      "Epoch   258, Loss_1:   1.1869, Loss_1_test:   1.6684, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   259, Loss_1:   1.1871, Loss_1_test:   1.6661, Loss_2:   1.4949, Loss_2_test:   1.3355\n",
      "Epoch   260, Loss_1:   1.1873, Loss_1_test:   1.6646, Loss_2:   1.4947, Loss_2_test:   1.3340\n",
      "Epoch   261, Loss_1:   1.1874, Loss_1_test:   1.6637, Loss_2:   1.4948, Loss_2_test:   1.3332\n",
      "Epoch   262, Loss_1:   1.1874, Loss_1_test:   1.6634, Loss_2:   1.4949, Loss_2_test:   1.3330\n",
      "Epoch   263, Loss_1:   1.1874, Loss_1_test:   1.6637, Loss_2:   1.4948, Loss_2_test:   1.3332\n",
      "Epoch   264, Loss_1:   1.1873, Loss_1_test:   1.6645, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   265, Loss_1:   1.1872, Loss_1_test:   1.6657, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   266, Loss_1:   1.1870, Loss_1_test:   1.6674, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   267, Loss_1:   1.1869, Loss_1_test:   1.6693, Loss_2:   1.4948, Loss_2_test:   1.3333\n",
      "Epoch   268, Loss_1:   1.1867, Loss_1_test:   1.6715, Loss_2:   1.4948, Loss_2_test:   1.3333\n",
      "Epoch   269, Loss_1:   1.1865, Loss_1_test:   1.6740, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   270, Loss_1:   1.1874, Loss_1_test:   1.6783, Loss_2:   1.4946, Loss_2_test:   1.3346\n",
      "Epoch   271, Loss_1:   1.1869, Loss_1_test:   1.6765, Loss_2:   1.4953, Loss_2_test:   1.3363\n",
      "Epoch   272, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4947, Loss_2_test:   1.3350\n",
      "Epoch   273, Loss_1:   1.1869, Loss_1_test:   1.6685, Loss_2:   1.4950, Loss_2_test:   1.3323\n",
      "Epoch   274, Loss_1:   1.1871, Loss_1_test:   1.6662, Loss_2:   1.4953, Loss_2_test:   1.3304\n",
      "Epoch   275, Loss_1:   1.1873, Loss_1_test:   1.6645, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   276, Loss_1:   1.1874, Loss_1_test:   1.6636, Loss_2:   1.4957, Loss_2_test:   1.3285\n",
      "Epoch   277, Loss_1:   1.1874, Loss_1_test:   1.6633, Loss_2:   1.4957, Loss_2_test:   1.3283\n",
      "Epoch   278, Loss_1:   1.1874, Loss_1_test:   1.6636, Loss_2:   1.4956, Loss_2_test:   1.3287\n",
      "Epoch   279, Loss_1:   1.1873, Loss_1_test:   1.6644, Loss_2:   1.4955, Loss_2_test:   1.3294\n",
      "Epoch   280, Loss_1:   1.1872, Loss_1_test:   1.6657, Loss_2:   1.4953, Loss_2_test:   1.3306\n",
      "Epoch   281, Loss_1:   1.1870, Loss_1_test:   1.6673, Loss_2:   1.4950, Loss_2_test:   1.3321\n",
      "Epoch   282, Loss_1:   1.1868, Loss_1_test:   1.6694, Loss_2:   1.4947, Loss_2_test:   1.3339\n",
      "Epoch   283, Loss_1:   1.1867, Loss_1_test:   1.6716, Loss_2:   1.4954, Loss_2_test:   1.3366\n",
      "Epoch   284, Loss_1:   1.1865, Loss_1_test:   1.6742, Loss_2:   1.4954, Loss_2_test:   1.3365\n",
      "Epoch   285, Loss_1:   1.1875, Loss_1_test:   1.6788, Loss_2:   1.4947, Loss_2_test:   1.3339\n",
      "Epoch   286, Loss_1:   1.1870, Loss_1_test:   1.6769, Loss_2:   1.4949, Loss_2_test:   1.3326\n",
      "Epoch   287, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4951, Loss_2_test:   1.3318\n",
      "Epoch   288, Loss_1:   1.1869, Loss_1_test:   1.6685, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   289, Loss_1:   1.1871, Loss_1_test:   1.6660, Loss_2:   1.4951, Loss_2_test:   1.3318\n",
      "Epoch   290, Loss_1:   1.1873, Loss_1_test:   1.6644, Loss_2:   1.4949, Loss_2_test:   1.3325\n",
      "Epoch   291, Loss_1:   1.1874, Loss_1_test:   1.6634, Loss_2:   1.4948, Loss_2_test:   1.3335\n",
      "Epoch   292, Loss_1:   1.1874, Loss_1_test:   1.6631, Loss_2:   1.4947, Loss_2_test:   1.3350\n",
      "Epoch   293, Loss_1:   1.1874, Loss_1_test:   1.6634, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   294, Loss_1:   1.1873, Loss_1_test:   1.6643, Loss_2:   1.4947, Loss_2_test:   1.3341\n",
      "Epoch   295, Loss_1:   1.1872, Loss_1_test:   1.6656, Loss_2:   1.4946, Loss_2_test:   1.3343\n",
      "Epoch   296, Loss_1:   1.1870, Loss_1_test:   1.6673, Loss_2:   1.4948, Loss_2_test:   1.3352\n",
      "Epoch   297, Loss_1:   1.1868, Loss_1_test:   1.6694, Loss_2:   1.4947, Loss_2_test:   1.3337\n",
      "Epoch   298, Loss_1:   1.1867, Loss_1_test:   1.6717, Loss_2:   1.4949, Loss_2_test:   1.3330\n",
      "Epoch   299, Loss_1:   1.1865, Loss_1_test:   1.6743, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   300, Loss_1:   1.1876, Loss_1_test:   1.6793, Loss_2:   1.4949, Loss_2_test:   1.3330\n",
      "Epoch   301, Loss_1:   1.1871, Loss_1_test:   1.6774, Loss_2:   1.4947, Loss_2_test:   1.3336\n",
      "Epoch   302, Loss_1:   1.1867, Loss_1_test:   1.6719, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   303, Loss_1:   1.1869, Loss_1_test:   1.6685, Loss_2:   1.4955, Loss_2_test:   1.3368\n",
      "Epoch   304, Loss_1:   1.1871, Loss_1_test:   1.6660, Loss_2:   1.4950, Loss_2_test:   1.3357\n",
      "Epoch   305, Loss_1:   1.1873, Loss_1_test:   1.6642, Loss_2:   1.4949, Loss_2_test:   1.3327\n",
      "Epoch   306, Loss_1:   1.1874, Loss_1_test:   1.6633, Loss_2:   1.4952, Loss_2_test:   1.3308\n",
      "Epoch   307, Loss_1:   1.1874, Loss_1_test:   1.6630, Loss_2:   1.4955, Loss_2_test:   1.3296\n",
      "Epoch   308, Loss_1:   1.1874, Loss_1_test:   1.6633, Loss_2:   1.4956, Loss_2_test:   1.3290\n",
      "Epoch   309, Loss_1:   1.1873, Loss_1_test:   1.6641, Loss_2:   1.4956, Loss_2_test:   1.3289\n",
      "Epoch   310, Loss_1:   1.1872, Loss_1_test:   1.6655, Loss_2:   1.4955, Loss_2_test:   1.3293\n",
      "Epoch   311, Loss_1:   1.1870, Loss_1_test:   1.6673, Loss_2:   1.4954, Loss_2_test:   1.3302\n",
      "Epoch   312, Loss_1:   1.1868, Loss_1_test:   1.6694, Loss_2:   1.4951, Loss_2_test:   1.3314\n",
      "Epoch   313, Loss_1:   1.1867, Loss_1_test:   1.6718, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   314, Loss_1:   1.1865, Loss_1_test:   1.6745, Loss_2:   1.4947, Loss_2_test:   1.3351\n",
      "Epoch   315, Loss_1:   1.1878, Loss_1_test:   1.6798, Loss_2:   1.4946, Loss_2_test:   1.3347\n",
      "Epoch   316, Loss_1:   1.1872, Loss_1_test:   1.6779, Loss_2:   1.4946, Loss_2_test:   1.3349\n",
      "Epoch   317, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4948, Loss_2_test:   1.3330\n",
      "Epoch   318, Loss_1:   1.1869, Loss_1_test:   1.6685, Loss_2:   1.4950, Loss_2_test:   1.3319\n",
      "Epoch   319, Loss_1:   1.1871, Loss_1_test:   1.6659, Loss_2:   1.4952, Loss_2_test:   1.3313\n",
      "Epoch   320, Loss_1:   1.1873, Loss_1_test:   1.6641, Loss_2:   1.4952, Loss_2_test:   1.3312\n",
      "Epoch   321, Loss_1:   1.1874, Loss_1_test:   1.6631, Loss_2:   1.4951, Loss_2_test:   1.3317\n",
      "Epoch   322, Loss_1:   1.1874, Loss_1_test:   1.6628, Loss_2:   1.4949, Loss_2_test:   1.3325\n",
      "Epoch   323, Loss_1:   1.1874, Loss_1_test:   1.6631, Loss_2:   1.4947, Loss_2_test:   1.3338\n",
      "Epoch   324, Loss_1:   1.1873, Loss_1_test:   1.6640, Loss_2:   1.4950, Loss_2_test:   1.3358\n",
      "Epoch   325, Loss_1:   1.1872, Loss_1_test:   1.6654, Loss_2:   1.4946, Loss_2_test:   1.3348\n",
      "Epoch   326, Loss_1:   1.1870, Loss_1_test:   1.6673, Loss_2:   1.4950, Loss_2_test:   1.3322\n",
      "Epoch   327, Loss_1:   1.1868, Loss_1_test:   1.6695, Loss_2:   1.4953, Loss_2_test:   1.3304\n",
      "Epoch   328, Loss_1:   1.1866, Loss_1_test:   1.6720, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   329, Loss_1:   1.1865, Loss_1_test:   1.6747, Loss_2:   1.4956, Loss_2_test:   1.3287\n",
      "Epoch   330, Loss_1:   1.1880, Loss_1_test:   1.6805, Loss_2:   1.4956, Loss_2_test:   1.3287\n",
      "Epoch   331, Loss_1:   1.1874, Loss_1_test:   1.6784, Loss_2:   1.4955, Loss_2_test:   1.3292\n",
      "Epoch   332, Loss_1:   1.1866, Loss_1_test:   1.6721, Loss_2:   1.4954, Loss_2_test:   1.3302\n",
      "Epoch   333, Loss_1:   1.1869, Loss_1_test:   1.6685, Loss_2:   1.4951, Loss_2_test:   1.3316\n",
      "Epoch   334, Loss_1:   1.1872, Loss_1_test:   1.6659, Loss_2:   1.4948, Loss_2_test:   1.3333\n",
      "Epoch   335, Loss_1:   1.1873, Loss_1_test:   1.6640, Loss_2:   1.4950, Loss_2_test:   1.3358\n",
      "Epoch   336, Loss_1:   1.1874, Loss_1_test:   1.6630, Loss_2:   1.4949, Loss_2_test:   1.3354\n",
      "Epoch   337, Loss_1:   1.1875, Loss_1_test:   1.6626, Loss_2:   1.4949, Loss_2_test:   1.3329\n",
      "Epoch   338, Loss_1:   1.1874, Loss_1_test:   1.6630, Loss_2:   1.4952, Loss_2_test:   1.3313\n",
      "Epoch   339, Loss_1:   1.1873, Loss_1_test:   1.6639, Loss_2:   1.4953, Loss_2_test:   1.3304\n",
      "Epoch   340, Loss_1:   1.1872, Loss_1_test:   1.6653, Loss_2:   1.4954, Loss_2_test:   1.3301\n",
      "Epoch   341, Loss_1:   1.1870, Loss_1_test:   1.6672, Loss_2:   1.4953, Loss_2_test:   1.3303\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define custom loss function, and optimizer\n",
    "model1 = RNNModel()\n",
    "model2 = RNNModel()\n",
    "\n",
    "squared_hinge_loss = SquaredHingeLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "record_loss1_test = []\n",
    "record_loss2_test = []\n",
    "min_loss_1_test = float('inf')\n",
    "min_loss_2_test = float('inf')\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs1 = torch.tensor([[0.0]])\n",
    "    outputs2 = torch.tensor([[0.0]])\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        outputs1 += model1(seq)\n",
    "        outputs2 += model2(seq)\n",
    "    \n",
    "    # Compute the custom loss\n",
    "    loss_1 = squared_hinge_loss(outputs1, targets_low_1, targets_high_1)\n",
    "    loss_2 = squared_hinge_loss(outputs2, targets_low_2, targets_high_2)\n",
    "    loss_1_test = squared_hinge_loss(outputs1, targets_low_2, targets_high_2)\n",
    "    loss_2_test = squared_hinge_loss(outputs2, targets_low_1, targets_high_1)\n",
    "    \n",
    "    # Print the loss every epochs\n",
    "    if (epoch) % 1 == 0:\n",
    "        print(f'Epoch {epoch:5d}, Loss_1: {loss_1.item():8.4f}, Loss_1_test: {loss_1_test.item():8.4f}, Loss_2: {loss_2.item():8.4f}, Loss_2_test: {loss_2_test.item():8.4f}')\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # record\n",
    "    record_loss1_test.append(loss_1_test.item())\n",
    "    record_loss2_test.append(loss_2_test.item())\n",
    "\n",
    "    # save models\n",
    "    if loss_1_test < min_loss_1_test:\n",
    "        min_loss_1_test = loss_1_test\n",
    "        torch.save(model1.state_dict(), 'saved_models/model1_rnn_tanh_best.pth')\n",
    "    \n",
    "    if loss_2_test < min_loss_2_test:\n",
    "        min_loss_2_test = loss_2_test\n",
    "        torch.save(model2.state_dict(), 'saved_models/model2_rnn_tanh_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model1\n",
    "model1 = RNNModel()\n",
    "model1.load_state_dict(torch.load('saved_models/model1_rnn_tanh_best.pth'))\n",
    "model1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load model2\n",
    "model2 = RNNModel()\n",
    "model2.load_state_dict(torch.load('saved_models/model2_rnn_tanh_best.pth'))\n",
    "model2.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas1 = np.zeros(len(seqs))\n",
    "ldas2 = np.zeros(len(seqs))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        ldas1[i] = model1(seq).numpy()[0][0]\n",
    "        ldas2[i] = model2(seq).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs   = gen_data_dict('sequence_label_data/signals.gz')\n",
    "labels = gen_data_dict('sequence_label_data/labels.gz')\n",
    "\n",
    "header = ['sequenceID', 'fold_1_total_labels', 'fold_2_total_labels', 'fold_1_errs', 'fold_2_errs']\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    # generate data\n",
    "    sequence, neg_start_1, neg_end_1, pos_start_1, pos_end_1, neg_start_2, neg_end_2, pos_start_2, pos_end_2 = get_data(i, seqs=seqs, labels=labels)\n",
    "    sequence_length = len(sequence)-1\n",
    "\n",
    "    # vectors of cumulative sums\n",
    "    y, z = get_cumsum(sequence)\n",
    "\n",
    "    # get total labels\n",
    "    fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "    fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "    # run each lambda and record it into csv file\n",
    "    row  = [i, fold1_total_labels, fold2_total_labels]\n",
    "\n",
    "    chpnt_fold1 = opart(10**ldas2[i], sequence)\n",
    "    chpnt_fold2 = opart(10**ldas1[i], sequence)\n",
    "\n",
    "    err_1 = error_count(chpnt_fold1, neg_start_1, neg_end_1, pos_start_1, pos_end_1)\n",
    "    err_2 = error_count(chpnt_fold2, neg_start_2, neg_end_2, pos_start_2, pos_end_2)\n",
    "    \n",
    "    row.append(sum(err_1))\n",
    "    row.append(sum(err_2))\n",
    "\n",
    "    write_to_csv('learning_output/rnn_tanh.csv', header, row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
