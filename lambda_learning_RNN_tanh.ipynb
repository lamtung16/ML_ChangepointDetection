{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1967f0ffcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.utility_functions import gen_data_dict, get_data, get_cumsum, error_count, write_to_csv, opart, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "seqs = gen_data_dict('sequence_label_data/signals.gz')\n",
    "\n",
    "# target \n",
    "target_df_1 = pd.read_csv('learning_data/target_lambda_fold1.csv')\n",
    "target_df_2 = pd.read_csv('learning_data/target_lambda_fold2.csv')\n",
    "\n",
    "targets_low_1  = target_df_1.iloc[:, 1:2].to_numpy()\n",
    "targets_high_1 = target_df_1.iloc[:, 2:3].to_numpy()\n",
    "targets_low_2  = target_df_2.iloc[:, 1:2].to_numpy()\n",
    "targets_high_2 = target_df_2.iloc[:, 2:3].to_numpy()\n",
    "\n",
    "targets_low_1  = torch.FloatTensor(targets_low_1)\n",
    "targets_high_1 = torch.FloatTensor(targets_high_1)\n",
    "targets_low_2  = torch.FloatTensor(targets_low_2)\n",
    "targets_high_2 = torch.FloatTensor(targets_high_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=8, num_layers=1, nonlinearity='tanh', bias=True, batch_first=False)\n",
    "        self.fc1 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x = self.rnn(x)\n",
    "        x    = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, Loss_1: 10147.3105, Loss_1_test: 10068.1797, Loss_2: 20591.3125, Loss_2_test: 20734.7539\n",
      "Epoch     1, Loss_1: 9631.8486, Loss_1_test: 9555.9932, Loss_2: 19731.6758, Loss_2_test: 19870.0117\n",
      "Epoch     2, Loss_1: 9125.3174, Loss_1_test: 9052.7002, Loss_2: 18895.6426, Loss_2_test: 19028.9922\n",
      "Epoch     3, Loss_1: 8628.2305, Loss_1_test: 8558.8115, Loss_2: 18083.2793, Loss_2_test: 18211.7676\n",
      "Epoch     4, Loss_1: 8141.0557, Loss_1_test: 8074.7939, Loss_2: 17294.4766, Loss_2_test: 17418.2266\n",
      "Epoch     5, Loss_1: 7664.3027, Loss_1_test: 7601.1533, Loss_2: 16528.9668, Loss_2_test: 16648.0957\n",
      "Epoch     6, Loss_1: 7198.4478, Loss_1_test: 7138.3643, Loss_2: 15786.1826, Loss_2_test: 15900.8096\n",
      "Epoch     7, Loss_1: 6743.9429, Loss_1_test: 6686.8765, Loss_2: 15065.3438, Loss_2_test: 15175.5801\n",
      "Epoch     8, Loss_1: 6301.2671, Loss_1_test: 6247.1646, Loss_2: 14365.6816, Loss_2_test: 14471.6396\n",
      "Epoch     9, Loss_1: 5870.8501, Loss_1_test: 5819.6587, Loss_2: 13686.4307, Loss_2_test: 13788.2129\n",
      "Epoch    10, Loss_1: 5453.1499, Loss_1_test: 5404.8115, Loss_2: 13027.0752, Loss_2_test: 13124.7812\n",
      "Epoch    11, Loss_1: 5048.5825, Loss_1_test: 5003.0381, Loss_2: 12387.1758, Loss_2_test: 12480.9092\n",
      "Epoch    12, Loss_1: 4657.5703, Loss_1_test: 4614.7583, Loss_2: 11766.4512, Loss_2_test: 11856.3076\n",
      "Epoch    13, Loss_1: 4280.5166, Loss_1_test: 4240.3735, Loss_2: 11164.6504, Loss_2_test: 11250.7285\n",
      "Epoch    14, Loss_1: 3917.8191, Loss_1_test: 3880.2778, Loss_2: 10581.5312, Loss_2_test: 10663.9238\n",
      "Epoch    15, Loss_1: 3569.8513, Loss_1_test: 3534.8450, Loss_2: 10016.8398, Loss_2_test: 10095.6436\n",
      "Epoch    16, Loss_1: 3236.9854, Loss_1_test: 3204.4419, Loss_2: 9470.3213, Loss_2_test: 9545.6270\n",
      "Epoch    17, Loss_1: 2919.5759, Loss_1_test: 2889.4231, Loss_2: 8941.7275, Loss_2_test: 9013.6279\n",
      "Epoch    18, Loss_1: 2617.9519, Loss_1_test: 2590.1147, Loss_2: 8430.7715, Loss_2_test: 8499.3574\n",
      "Epoch    19, Loss_1: 2332.4370, Loss_1_test: 2306.8374, Loss_2: 7937.1792, Loss_2_test: 8002.5386\n",
      "Epoch    20, Loss_1: 2063.3108, Loss_1_test: 2039.8699, Loss_2: 7460.7153, Loss_2_test: 7522.9370\n",
      "Epoch    21, Loss_1: 1810.8511, Loss_1_test: 1789.4873, Loss_2: 7001.0894, Loss_2_test: 7060.2583\n",
      "Epoch    22, Loss_1: 1575.2810, Loss_1_test: 1555.9104, Loss_2: 6558.0640, Loss_2_test: 6614.2646\n",
      "Epoch    23, Loss_1: 1356.7916, Loss_1_test: 1339.3284, Loss_2: 6131.4116, Loss_2_test: 6184.7290\n",
      "Epoch    24, Loss_1: 1155.5245, Loss_1_test: 1139.8810, Loss_2: 5720.9248, Loss_2_test: 5771.4409\n",
      "Epoch    25, Loss_1: 971.5583, Loss_1_test: 957.6448, Loss_2: 5326.4067, Loss_2_test: 5374.2026\n",
      "Epoch    26, Loss_1: 804.9019, Loss_1_test: 792.6270, Loss_2: 4947.6973, Loss_2_test: 4992.8540\n",
      "Epoch    27, Loss_1: 655.4702, Loss_1_test: 644.7413, Loss_2: 4584.6294, Loss_2_test: 4627.2280\n",
      "Epoch    28, Loss_1: 523.0893, Loss_1_test: 513.8123, Loss_2: 4237.0923, Loss_2_test: 4277.2119\n",
      "Epoch    29, Loss_1: 407.4600, Loss_1_test: 399.5400, Loss_2: 3904.9570, Loss_2_test: 3942.6772\n",
      "Epoch    30, Loss_1: 308.1566, Loss_1_test: 301.4984, Loss_2: 3588.1191, Loss_2_test: 3623.5193\n",
      "Epoch    31, Loss_1: 224.6121, Loss_1_test: 219.1201, Loss_2: 3286.4863, Loss_2_test: 3319.6453\n",
      "Epoch    32, Loss_1: 156.1041, Loss_1_test: 151.6836, Loss_2: 2999.9561, Loss_2_test: 3030.9529\n",
      "Epoch    33, Loss_1: 101.7496, Loss_1_test:  98.3068, Loss_2: 2728.4280, Loss_2_test: 2757.3411\n",
      "Epoch    34, Loss_1:  60.5060, Loss_1_test:  57.9489, Loss_2: 2471.7971, Loss_2_test: 2498.7061\n",
      "Epoch    35, Loss_1:  31.1748, Loss_1_test:  29.4137, Loss_2: 2229.9404, Loss_2_test: 2254.9229\n",
      "Epoch    36, Loss_1:  12.4162, Loss_1_test:  11.3646, Loss_2: 2002.7227, Loss_2_test: 2025.8584\n",
      "Epoch    37, Loss_1:   2.8886, Loss_1_test:   2.5314, Loss_2: 1789.9773, Loss_2_test: 1811.3452\n",
      "Epoch    38, Loss_1:   1.3857, Loss_1_test:   2.2770, Loss_2: 1591.5154, Loss_2_test: 1611.1936\n",
      "Epoch    39, Loss_1:   4.2992, Loss_1_test:   7.9229, Loss_2: 1407.1218, Loss_2_test: 1425.1888\n",
      "Epoch    40, Loss_1:   9.3920, Loss_1_test:  16.8472, Loss_2: 1236.5382, Loss_2_test: 1253.0720\n",
      "Epoch    41, Loss_1:  15.7426, Loss_1_test:  27.8278, Loss_2: 1079.4712, Loss_2_test: 1094.5492\n",
      "Epoch    42, Loss_1:  22.8162, Loss_1_test:  39.9681, Loss_2: 935.5898, Loss_2_test: 949.2888\n",
      "Epoch    43, Loss_1:  30.1547, Loss_1_test:  52.5062, Loss_2: 804.5189, Loss_2_test: 816.9147\n",
      "Epoch    44, Loss_1:  37.3746, Loss_1_test:  64.8051, Loss_2: 685.8409, Loss_2_test: 697.0082\n",
      "Epoch    45, Loss_1:  44.1652, Loss_1_test:  76.3488, Loss_2: 579.0983, Loss_2_test: 589.1108\n",
      "Epoch    46, Loss_1:  50.2848, Loss_1_test:  86.7364, Loss_2: 483.7897, Loss_2_test: 492.7196\n",
      "Epoch    47, Loss_1:  55.5561, Loss_1_test:  95.6745, Loss_2: 399.3742, Loss_2_test: 407.2921\n",
      "Epoch    48, Loss_1:  59.8615, Loss_1_test: 102.9691, Loss_2: 325.2723, Loss_2_test: 332.2470\n",
      "Epoch    49, Loss_1:  63.1361, Loss_1_test: 108.5142, Loss_2: 260.8727, Loss_2_test: 266.9710\n",
      "Epoch    50, Loss_1:  65.3613, Loss_1_test: 112.2809, Loss_2: 205.5339, Loss_2_test: 210.8204\n",
      "Epoch    51, Loss_1:  66.5579, Loss_1_test: 114.3061, Loss_2: 158.5904, Loss_2_test: 163.1275\n",
      "Epoch    52, Loss_1:  66.7782, Loss_1_test: 114.6789, Loss_2: 119.3591, Loss_2_test: 123.2064\n",
      "Epoch    53, Loss_1:  66.0999, Loss_1_test: 113.5310, Loss_2:  87.1446, Loss_2_test:  90.3596\n",
      "Epoch    54, Loss_1:  64.6185, Loss_1_test: 111.0236, Loss_2:  61.2485, Loss_2_test:  63.8857\n",
      "Epoch    55, Loss_1:  62.4413, Loss_1_test: 107.3378, Loss_2:  40.9752, Loss_2_test:  43.0865\n",
      "Epoch    56, Loss_1:  59.6824, Loss_1_test: 102.6657, Loss_2:  25.6405, Loss_2_test:  27.2748\n",
      "Epoch    57, Loss_1:  56.4572, Loss_1_test:  97.2016, Loss_2:  14.5785, Loss_2_test:  15.7821\n",
      "Epoch    58, Loss_1:  52.8792, Loss_1_test:  91.1365, Loss_2:   7.1498, Loss_2_test:   7.9660\n",
      "Epoch    59, Loss_1:  49.0558, Loss_1_test:  84.6512, Loss_2:   2.8624, Loss_2_test:   3.2956\n",
      "Epoch    60, Loss_1:  45.0862, Loss_1_test:  77.9130, Loss_2:   1.5233, Loss_2_test:   1.3937\n",
      "Epoch    61, Loss_1:  41.0601, Loss_1_test:  71.0727, Loss_2:   2.2139, Loss_2_test:   1.3625\n",
      "Epoch    62, Loss_1:  37.0564, Loss_1_test:  64.2635, Loss_2:   4.3804, Loss_2_test:   2.3690\n",
      "Epoch    63, Loss_1:  33.1423, Loss_1_test:  57.5991, Loss_2:   7.3158, Loss_2_test:   3.9597\n",
      "Epoch    64, Loss_1:  29.3736, Loss_1_test:  51.1736, Loss_2:  10.6331, Loss_2_test:   5.8363\n",
      "Epoch    65, Loss_1:  25.7948, Loss_1_test:  45.0626, Loss_2:  14.0785, Loss_2_test:   7.8029\n",
      "Epoch    66, Loss_1:  22.4398, Loss_1_test:  39.3235, Loss_2:  17.4588, Loss_2_test:   9.7439\n",
      "Epoch    67, Loss_1:  19.3325, Loss_1_test:  33.9976, Loss_2:  20.6282, Loss_2_test:  11.5715\n",
      "Epoch    68, Loss_1:  16.4883, Loss_1_test:  29.1112, Loss_2:  23.4777, Loss_2_test:  13.2195\n",
      "Epoch    69, Loss_1:  13.9146, Loss_1_test:  24.6775, Loss_2:  25.9302, Loss_2_test:  14.6410\n",
      "Epoch    70, Loss_1:  11.6120, Loss_1_test:  20.6984, Loss_2:  27.9364, Loss_2_test:  15.8057\n",
      "Epoch    71, Loss_1:   9.5757, Loss_1_test:  17.1665, Loss_2:  29.4713, Loss_2_test:  16.6977\n",
      "Epoch    72, Loss_1:   7.7960, Loss_1_test:  14.0664, Loss_2:  30.5298, Loss_2_test:  17.3133\n",
      "Epoch    73, Loss_1:   6.2598, Loss_1_test:  11.3770, Loss_2:  31.1235, Loss_2_test:  17.6588\n",
      "Epoch    74, Loss_1:   4.9514, Loss_1_test:   9.0755, Loss_2:  31.2770, Loss_2_test:  17.7481\n",
      "Epoch    75, Loss_1:   3.8586, Loss_1_test:   7.1332, Loss_2:  31.0250, Loss_2_test:  17.6014\n",
      "Epoch    76, Loss_1:   2.9703, Loss_1_test:   5.5201, Loss_2:  30.4095, Loss_2_test:  17.2433\n",
      "Epoch    77, Loss_1:   2.2869, Loss_1_test:   4.2236, Loss_2:  29.4771, Loss_2_test:  16.7011\n",
      "Epoch    78, Loss_1:   1.7927, Loss_1_test:   3.2084, Loss_2:  28.2769, Loss_2_test:  16.0035\n",
      "Epoch    79, Loss_1:   1.4471, Loss_1_test:   2.4398, Loss_2:  26.8587, Loss_2_test:  15.1799\n",
      "Epoch    80, Loss_1:   1.2649, Loss_1_test:   1.9332, Loss_2:  25.2714, Loss_2_test:  14.2589\n",
      "Epoch    81, Loss_1:   1.1935, Loss_1_test:   1.6257, Loss_2:  23.5616, Loss_2_test:  13.2681\n",
      "Epoch    82, Loss_1:   1.2544, Loss_1_test:   1.5266, Loss_2:  21.7730, Loss_2_test:  12.2331\n",
      "Epoch    83, Loss_1:   1.3736, Loss_1_test:   1.5133, Loss_2:  19.9454, Loss_2_test:  11.1772\n",
      "Epoch    84, Loss_1:   1.5722, Loss_1_test:   1.6171, Loss_2:  18.1143, Loss_2_test:  10.1213\n",
      "Epoch    85, Loss_1:   1.7792, Loss_1_test:   1.7341, Loss_2:  16.3107, Loss_2_test:   9.0835\n",
      "Epoch    86, Loss_1:   1.9882, Loss_1_test:   1.8648, Loss_2:  14.5606, Loss_2_test:   8.0791\n",
      "Epoch    87, Loss_1:   2.2023, Loss_1_test:   2.0229, Loss_2:  12.8858, Loss_2_test:   7.1206\n",
      "Epoch    88, Loss_1:   2.3756, Loss_1_test:   2.1511, Loss_2:  11.3036, Loss_2_test:   6.2180\n",
      "Epoch    89, Loss_1:   2.5009, Loss_1_test:   2.2438, Loss_2:   9.8279, Loss_2_test:   5.3785\n",
      "Epoch    90, Loss_1:   2.5752, Loss_1_test:   2.2989, Loss_2:   8.4690, Loss_2_test:   4.6078\n",
      "Epoch    91, Loss_1:   2.5988, Loss_1_test:   2.3164, Loss_2:   7.2310, Loss_2_test:   3.9127\n",
      "Epoch    92, Loss_1:   2.5752, Loss_1_test:   2.2989, Loss_2:   6.1175, Loss_2_test:   3.2981\n",
      "Epoch    93, Loss_1:   2.5097, Loss_1_test:   2.2504, Loss_2:   5.1327, Loss_2_test:   2.7650\n",
      "Epoch    94, Loss_1:   2.4092, Loss_1_test:   2.1759, Loss_2:   4.2777, Loss_2_test:   2.3152\n",
      "Epoch    95, Loss_1:   2.2813, Loss_1_test:   2.0813, Loss_2:   3.5456, Loss_2_test:   1.9473\n",
      "Epoch    96, Loss_1:   2.1343, Loss_1_test:   1.9726, Loss_2:   2.9367, Loss_2_test:   1.6689\n",
      "Epoch    97, Loss_1:   1.9759, Loss_1_test:   1.8558, Loss_2:   2.4337, Loss_2_test:   1.4447\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, define custom loss function, and optimizer\n",
    "model1 = RNNModel()\n",
    "model2 = RNNModel()\n",
    "\n",
    "squared_hinge_loss = SquaredHingeLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "record_loss1_test = []\n",
    "record_loss2_test = []\n",
    "min_loss_1_test = float('inf')\n",
    "min_loss_2_test = float('inf')\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs1 = torch.tensor([[0.0]])\n",
    "    outputs2 = torch.tensor([[0.0]])\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        outputs1 += model1(seq)\n",
    "        outputs2 += model2(seq)\n",
    "    \n",
    "    # Compute the custom loss\n",
    "    loss_1 = squared_hinge_loss(outputs1, targets_low_1, targets_high_1)\n",
    "    loss_2 = squared_hinge_loss(outputs2, targets_low_2, targets_high_2)\n",
    "    loss_1_test = squared_hinge_loss(outputs1, targets_low_2, targets_high_2)\n",
    "    loss_2_test = squared_hinge_loss(outputs2, targets_low_1, targets_high_1)\n",
    "    \n",
    "    # Print the loss every epochs\n",
    "    if (epoch) % 1 == 0:\n",
    "        print(f'Epoch {epoch:5d}, Loss_1: {loss_1.item():8.4f}, Loss_1_test: {loss_1_test.item():8.4f}, Loss_2: {loss_2.item():8.4f}, Loss_2_test: {loss_2_test.item():8.4f}')\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    # record\n",
    "    record_loss1_test.append(loss_1_test.item())\n",
    "    record_loss2_test.append(loss_2_test.item())\n",
    "\n",
    "    # save models\n",
    "    if loss_1_test < min_loss_1_test:\n",
    "        min_loss_1_test = loss_1_test\n",
    "        torch.save(model1.state_dict(), 'saved_models/model1_rnn_tanh_best.pth')\n",
    "    \n",
    "    if loss_2_test < min_loss_2_test:\n",
    "        min_loss_2_test = loss_2_test\n",
    "        torch.save(model2.state_dict(), 'saved_models/model2_rnn_tanh_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model1\n",
    "model1 = RNNModel()\n",
    "model1.load_state_dict(torch.load('saved_models/model1_rnn_tanh_best.pth'))\n",
    "model1.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load model2\n",
    "model2 = RNNModel()\n",
    "model2.load_state_dict(torch.load('saved_models/model2_rnn_tanh_best.pth'))\n",
    "model2.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas1 = np.zeros(len(seqs))\n",
    "ldas2 = np.zeros(len(seqs))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(seqs)):\n",
    "        seq = torch.tensor(seqs[i][1]['logratio'].to_numpy(), dtype=torch.float32).reshape(-1,1)\n",
    "        ldas1[i] = model1(seq).numpy()[0][0]\n",
    "        ldas2[i] = model2(seq).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs   = gen_data_dict('sequence_label_data/signals.gz')\n",
    "labels = gen_data_dict('sequence_label_data/labels.gz')\n",
    "\n",
    "header = ['sequenceID', 'fold_1_total_labels', 'fold_2_total_labels', 'fold_1_errs', 'fold_2_errs']\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    # generate data\n",
    "    sequence, neg_start_1, neg_end_1, pos_start_1, pos_end_1, neg_start_2, neg_end_2, pos_start_2, pos_end_2 = get_data(i, seqs=seqs, labels=labels)\n",
    "    sequence_length = len(sequence)-1\n",
    "\n",
    "    # vectors of cumulative sums\n",
    "    y, z = get_cumsum(sequence)\n",
    "\n",
    "    # get total labels\n",
    "    fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "    fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "    # run each lambda and record it into csv file\n",
    "    row  = [i, fold1_total_labels, fold2_total_labels]\n",
    "\n",
    "    chpnt_fold1 = opart(10**ldas2[i], sequence)\n",
    "    chpnt_fold2 = opart(10**ldas1[i], sequence)\n",
    "\n",
    "    err_1 = error_count(chpnt_fold1, neg_start_1, neg_end_1, pos_start_1, pos_end_1)\n",
    "    err_2 = error_count(chpnt_fold2, neg_start_2, neg_end_2, pos_start_2, pos_end_2)\n",
    "    \n",
    "    row.append(sum(err_1))\n",
    "    row.append(sum(err_2))\n",
    "\n",
    "    write_to_csv('learning_output/rnn_tanh.csv', header, row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
