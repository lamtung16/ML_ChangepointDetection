{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13efedbb2d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from utility_functions import gen_data_dict, get_data, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen feature for training\n",
    "chosen_feature = ['std_deviation', 'count', 'sum_diff', 'range_value', 'abs_skewness']\n",
    "\n",
    "# hyper para in cv\n",
    "cv_batch_size = 4\n",
    "cv_n_folds    = 2\n",
    "cv_n_ites     = 200\n",
    "\n",
    "# batch size in training\n",
    "train_batch_size = 1\n",
    "\n",
    "# learning rate in cv and training\n",
    "lr = 0.001\n",
    "\n",
    "# hyper parameters to try\n",
    "n_hiddens_values  = [1, 2, 3]\n",
    "layer_size_values = [4, 8, 16]\n",
    "\n",
    "configs = [{'n_hiddens': 0, 'layer_size': 0}]\n",
    "configs += [{'n_hiddens': n, 'layer_size': s} for n in n_hiddens_values for s in layer_size_values]\n",
    "\n",
    "# getting dataframe of error count\n",
    "err_fold1_df = pd.read_csv('../1_training_data/errors_fold1_base_10.csv')\n",
    "err_fold2_df = pd.read_csv('../1_training_data/errors_fold2_base_10.csv')\n",
    "\n",
    "# getting sequences and labels data\n",
    "seqs   = gen_data_dict('../0_sequences_labels/signals.gz')\n",
    "labels = gen_data_dict('../0_sequences_labels/labels.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 5\n",
      "Shape of features tensor: [413, 5]\n"
     ]
    }
   ],
   "source": [
    "# Chose features\n",
    "features = pd.read_csv('../1_training_data/seq_features.csv').iloc[:, 1:][chosen_feature].to_numpy()\n",
    "\n",
    "# normalize them\n",
    "mean = np.mean(features, axis=0)\n",
    "std_dev = np.std(features, axis=0)\n",
    "features = (features-mean)/std_dev\n",
    "\n",
    "# convert to torch tensor\n",
    "features = torch.Tensor(features)\n",
    "\n",
    "# verify feature input size\n",
    "feature_input_size = len(chosen_feature)\n",
    "print(\"Number of features:\", feature_input_size)\n",
    "\n",
    "# verify the shape\n",
    "print(\"Shape of features tensor:\", list(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target intervals\n",
    "target_df_1 = pd.read_csv('../1_training_data/target_lambda_fold1_base_10.csv')\n",
    "target_df_2 = pd.read_csv('../1_training_data/target_lambda_fold2_base_10.csv')\n",
    "\n",
    "targets_low_1  = torch.Tensor(target_df_1.iloc[:, 1:2].to_numpy())\n",
    "targets_high_1 = torch.Tensor(target_df_1.iloc[:, 2:3].to_numpy())\n",
    "targets_low_2  = torch.Tensor(target_df_2.iloc[:, 1:2].to_numpy())\n",
    "targets_high_2 = torch.Tensor(target_df_2.iloc[:, 2:3].to_numpy())\n",
    "\n",
    "target_fold1 = torch.cat((targets_low_1, targets_high_1), dim=1)\n",
    "target_fold2 = torch.cat((targets_low_2, targets_high_2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architect\n",
    "class DLModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, hidden_size):\n",
    "        super(DLModel, self).__init__()\n",
    "        self.input_size    = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size   = hidden_size\n",
    "\n",
    "        if(self.hidden_layers == 0):\n",
    "            self.linear_model = nn.Linear(input_size, 1)                                                        # Define linear model\n",
    "        else:\n",
    "            self.input_layer = nn.Linear(input_size, hidden_size)                                               # Define input layer\n",
    "            self.hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers-1)])  # Define hidden layers\n",
    "            self.output_layer = nn.Linear(hidden_size, 1)                                                       # Define output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if(self.hidden_layers == 0):\n",
    "            return self.linear_model(x)\n",
    "        else:\n",
    "            x = torch.relu(self.input_layer(x))\n",
    "            for layer in self.hidden:\n",
    "                x = torch.relu(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_learn(n_splits, X, y, n_hiddens, layer_size, lr, n_ite):\n",
    "    \n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    # loss function\n",
    "    loss_func = SquaredHingeLoss(margin=1)\n",
    "\n",
    "    # learn best ite\n",
    "    total_train_losses = np.zeros(n_ite)\n",
    "    total_val_losses   = np.zeros(n_ite)\n",
    "    for train_index, val_index in kf.split(X):\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset    = TensorDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=cv_batch_size, shuffle=True)\n",
    "\n",
    "        # Define your model\n",
    "        model = DLModel(feature_input_size, n_hiddens, layer_size)\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "        # Training loop for the specified number of iterations\n",
    "        train_losses = []\n",
    "        val_losses   = []\n",
    "        for i in range(n_ite):\n",
    "            # training\n",
    "            train_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_func(model(inputs), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # validating\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = loss_func(model(X_val), y_val)\n",
    "\n",
    "            # add train_loss and val_loss into arrays\n",
    "            train_losses.append(train_loss/len(dataloader))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        total_train_losses += train_losses\n",
    "        total_val_losses += val_losses\n",
    "\n",
    "    best_no_ite = np.argmin(total_val_losses)\n",
    "    return best_no_ite + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "def train_model(X, y, n_hiddens, layer_size, lr, n_ites):\n",
    "    model = DLModel(feature_input_size, n_hiddens, layer_size)\n",
    "    loss_func = SquaredHingeLoss(margin=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset    = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(n_ites):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_stat(ldas1, ldas2, err_fold1_df, err_fold2_df, seqs, labels):\n",
    "    header = ['sequenceID', 'lda_fold1', 'lda_fold2', 'fold_1_total_labels', 'fold_2_total_labels', 'fold1_err', 'fold2_err']\n",
    "    rows = []\n",
    "    for i in range(len(seqs)):\n",
    "        # get total labels\n",
    "        _, neg_start_1, _, pos_start_1, _, neg_start_2, _, pos_start_2, _ = get_data(i, seqs, labels)\n",
    "        fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "        fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "        # round lambda\n",
    "        ldas1 = [round(num*2)/2 for num in ldas1]\n",
    "        ldas2 = [round(num*2)/2 for num in ldas2]\n",
    "\n",
    "        # get err\n",
    "        fold1_err = err_fold1_df.iloc[i][str(ldas1[i])]\n",
    "        fold2_err = err_fold2_df.iloc[i][str(ldas2[i])]\n",
    "        \n",
    "        # add row to rows\n",
    "        row = [seqs[i][0], ldas1[i], ldas2[i], fold1_total_labels, fold2_total_labels, fold1_err, fold2_err]\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(full_X, X1, X2, y1, y2, config, err_fold1_df, err_fold2_df, seqs, labels):\n",
    "    n_hiddens  = config['n_hiddens']\n",
    "    layer_size = config['layer_size']\n",
    "\n",
    "    best_no_ite_1 = cv_learn(cv_n_folds, X1, y1, n_hiddens, layer_size, lr, cv_n_ites)\n",
    "    best_no_ite_2 = cv_learn(cv_n_folds, X2, y2, n_hiddens, layer_size, lr, cv_n_ites)\n",
    "\n",
    "    model1 = train_model(X1, y1, n_hiddens, layer_size, lr, best_no_ite_1)\n",
    "    model2 = train_model(X2, y2, n_hiddens, layer_size, lr, best_no_ite_2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ldas1 = model1(full_X).numpy().reshape(-1)\n",
    "        ldas2 = model2(full_X).numpy().reshape(-1)\n",
    "\n",
    "    df = get_df_stat(ldas1, ldas2, err_fold1_df, err_fold2_df, seqs, labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:  2.1min remaining:  8.5min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  4.6min finished\n"
     ]
    }
   ],
   "source": [
    "# train all models parallel and get stats dfs\n",
    "torch.manual_seed(0)\n",
    "dfs = Parallel(n_jobs=len(configs), verbose=1)(delayed(try_model)(features, features, features, target_fold1, target_fold2,\n",
    "                                                        configs[i], err_fold1_df, err_fold2_df, seqs, labels) for i in range(len(configs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_hiddens  layer_size  fold1_test  fold2_test\n",
      "0          0           0   78.457447   81.538462\n",
      "1          1           4   78.856383   79.807692\n",
      "2          1           8   78.590426   79.615385\n",
      "3          1          16   79.388298   81.153846\n",
      "4          2           4   78.457447   80.192308\n",
      "5          2           8   78.856383   79.615385\n",
      "6          2          16   79.388298   80.769231\n",
      "7          3           4   79.255319   79.807692\n",
      "8          3           8   79.787234   77.884615\n",
      "9          3          16   81.515957   78.269231\n"
     ]
    }
   ],
   "source": [
    "# printing results\n",
    "results = []\n",
    "for i in range(len(dfs)):\n",
    "    df = dfs[i]\n",
    "    \n",
    "    total_label_fold1 = df['fold_1_total_labels'].sum()\n",
    "    total_label_fold2 = df['fold_2_total_labels'].sum()\n",
    "    err1 = df['fold1_err'].sum()\n",
    "    err2 = df['fold2_err'].sum()\n",
    "    rate1 = (total_label_fold1 - err1)/total_label_fold1\n",
    "    rate2 = (total_label_fold2 - err2)/total_label_fold2\n",
    "\n",
    "    results.append({\n",
    "        'n_hiddens': configs[i]['n_hiddens'],\n",
    "        'layer_size': configs[i]['layer_size'],\n",
    "        'fold1_test': rate1 * 100,\n",
    "        'fold2_test': rate2 * 100\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fold1_test: 79.26\n",
      "Average fold2_test: 79.87\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of fold1_test and fold2_test\n",
    "fold1_test_avg = df_results['fold1_test'].mean()\n",
    "fold2_test_avg = df_results['fold2_test'].mean()\n",
    "\n",
    "print(\"Average fold1_test: %.2f\" % fold1_test_avg)\n",
    "print(\"Average fold2_test: %.2f\" % fold2_test_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
