{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e1ed503d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ipynb.fs.full.utility_functions import gen_data_dict, get_data, SquaredHingeLoss\n",
    "\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen feature for training\n",
    "chosen_feature = ['std_deviation', 'count', 'sum_diff', 'range_value', 'abs_skewness']\n",
    "\n",
    "# hyper para in cv\n",
    "cv_batch_size = 8\n",
    "cv_n_folds    = 2\n",
    "cv_n_ites     = 100\n",
    "\n",
    "# batch size in training\n",
    "train_batch_size = 1\n",
    "\n",
    "# learning rate in cv and training\n",
    "lr = 0.001\n",
    "\n",
    "# hyper parameters\n",
    "n_hiddens_values  = [1, 2, 3]\n",
    "layer_size_values = [4, 8, 16]\n",
    "\n",
    "configs = [{'n_hiddens': 0, 'layer_size': 0}]\n",
    "configs += [{'n_hiddens': n, 'layer_size': s} for n in n_hiddens_values for s in layer_size_values]\n",
    "\n",
    "# getting dataframe of error count\n",
    "err_fold1_df = pd.read_csv('1_training_data/errors_fold1_base_10.csv')\n",
    "err_fold2_df = pd.read_csv('1_training_data/errors_fold2_base_10.csv')\n",
    "\n",
    "# getting sequences and labels data\n",
    "seqs   = gen_data_dict('0_sequences_labels/signals.gz')\n",
    "labels = gen_data_dict('0_sequences_labels/labels.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Features and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Chose features\n",
    "X = pd.read_csv('1_training_data/seq_features.csv').iloc[:, 1:][chosen_feature].to_numpy()\n",
    "\n",
    "# verify feature input size\n",
    "feature_input_size = len(chosen_feature)\n",
    "print(feature_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features tensor: torch.Size([413, 5])\n"
     ]
    }
   ],
   "source": [
    "# normalize them\n",
    "mean = np.mean(X, axis=0)\n",
    "std_dev = np.std(X, axis=0)\n",
    "X = (X-mean)/std_dev\n",
    "\n",
    "# convert to torch tensor\n",
    "features = torch.Tensor(X)\n",
    "\n",
    "# verify the shape\n",
    "print(\"Shape of features tensor:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Target Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([413, 2]) torch.Size([413, 2])\n"
     ]
    }
   ],
   "source": [
    "# data (targets)\n",
    "target_df_1 = pd.read_csv('1_training_data/target_lambda_fold1_base_10.csv')\n",
    "target_df_2 = pd.read_csv('1_training_data/target_lambda_fold2_base_10.csv')\n",
    "\n",
    "targets_low_1  = torch.Tensor(target_df_1.iloc[:, 1:2].to_numpy())\n",
    "targets_high_1 = torch.Tensor(target_df_1.iloc[:, 2:3].to_numpy())\n",
    "targets_low_2  = torch.Tensor(target_df_2.iloc[:, 1:2].to_numpy())\n",
    "targets_high_2 = torch.Tensor(target_df_2.iloc[:, 2:3].to_numpy())\n",
    "\n",
    "target_fold1 = torch.cat((targets_low_1, targets_high_1), dim=1)\n",
    "target_fold2 = torch.cat((targets_low_2, targets_high_2), dim=1)\n",
    "\n",
    "# verify shape\n",
    "print(target_fold1.shape, target_fold2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, hidden_size):\n",
    "        super(DLModel, self).__init__()\n",
    "        self.input_size    = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size   = hidden_size\n",
    "\n",
    "        if(self.hidden_layers == 0):\n",
    "            self.linear_model = nn.Linear(input_size, 1)                                                        # Define linear model\n",
    "        else:\n",
    "            self.input_layer = nn.Linear(input_size, hidden_size)                                               # Define input layer\n",
    "            self.hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers-1)])  # Define hidden layers\n",
    "            self.output_layer = nn.Linear(hidden_size, 1)                                                       # Define output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if(self.hidden_layers == 0):\n",
    "            return self.linear_model(x)\n",
    "        else:\n",
    "            x = torch.relu(self.input_layer(x))\n",
    "            for layer in self.hidden:\n",
    "                x = torch.relu(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify model architect\n",
    "# print(\"Linear model:\\n\", DLModel(5, 0, 4))\n",
    "# print(\"\\nMLP model:\\n\",  DLModel(5, 2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot train loss and val loss\n",
    "def plot_loss(train_loss, val_loss, best_ite, train_set_name, val_set_name):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,   'r', label='Validation loss')\n",
    "    \n",
    "    # Mark the minimum validation loss point\n",
    "    if(best_ite != None):\n",
    "        plt.plot(best_ite, val_loss[best_ite], 'g*', markersize=10, label=f'Min Val epoch: {best_ite: 3d}')\n",
    "\n",
    "    plt.title('Train ' + train_set_name + \" Validate \" + val_set_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_learn(n_splits, X, y, n_hiddens, layer_size, show_plot, lr, n_ite):\n",
    "    \n",
    "    # Define the number of folds for cross-validation\n",
    "    kf = KFold(n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    # loss function\n",
    "    loss_func = SquaredHingeLoss(margin=1)\n",
    "\n",
    "    # learn best ite\n",
    "    total_train_losses = np.zeros(n_ite)\n",
    "    total_val_losses   = np.zeros(n_ite)\n",
    "    for train_index, val_index in kf.split(X):\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset    = TensorDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=cv_batch_size, shuffle=True)\n",
    "\n",
    "        # Define your model\n",
    "        model = DLModel(feature_input_size, n_hiddens, layer_size)\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "        # Training loop for the specified number of iterations\n",
    "        train_losses = []\n",
    "        val_losses   = []\n",
    "        for i in range(n_ite):\n",
    "            # training\n",
    "            train_loss = 0\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_func(model(inputs), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # validating\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = loss_func(model(X_val), y_val)\n",
    "\n",
    "            # add train_loss and val_loss into arrays\n",
    "            train_losses.append(train_loss/len(dataloader))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        total_train_losses += train_losses\n",
    "        total_val_losses += val_losses\n",
    "\n",
    "    best_no_ite = np.argmin(total_val_losses)\n",
    "    if(show_plot == True):\n",
    "        plot_loss(total_train_losses/n_splits, total_val_losses/n_splits, best_no_ite, 'subtrain', 'val')\n",
    "    \n",
    "    return best_no_ite + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test cv_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing cv_learn fold 1\n",
    "# torch.manual_seed(0)\n",
    "# best_no_ite_fold1 = cv_learn(2, features, target_fold1, 2, 8, True, 0.001, 100)\n",
    "# print(\"Best number of iteration:\", best_no_ite_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing cv_learn fold 2\n",
    "# torch.manual_seed(1)\n",
    "# best_no_ite_fold2 = cv_learn(2, features, target_fold2, 2, 8, True, 0.001, 100)\n",
    "# print(\"Best number of iteration:\", best_no_ite_fold2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing training one fold and validating the other fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_loss_plot(X_train, X_test, y_train, y_test, train_set_name, test_set_name, hidden_layers, hidden_size, lr, n_ite):\n",
    "#     # loss function\n",
    "#     loss_func = SquaredHingeLoss(margin=1)\n",
    "\n",
    "#     # Define your model and optimizer\n",
    "#     model = DLModel(feature_input_size, hidden_layers, hidden_size)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     # Create DataLoader\n",
    "#     dataset    = TensorDataset(X_train, y_train)\n",
    "#     dataloader = DataLoader(dataset, batch_size=60, shuffle=True)\n",
    "    \n",
    "#     train_losses = []\n",
    "#     val_losses   = []\n",
    "#     for i in range(n_ite):\n",
    "#         # Training\n",
    "#         train_loss = 0\n",
    "#         for inputs, labels in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = loss_func(model(inputs), labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#         # Evaluating\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_loss = loss_func(model(X_test), y_test)\n",
    "\n",
    "#         # append train_loss and val_loss\n",
    "#         train_losses.append(train_loss/len(dataloader))\n",
    "#         val_losses.append(val_loss.item())\n",
    "    \n",
    "#     plot_loss(train_losses, val_losses, np.argmin(val_losses), train_set_name, test_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# train_test_loss_plot(features, features, target_fold1, target_fold2, 'Fold 1', 'Fold 2', 2, 8, 0.001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# train_test_loss_plot(features, features, target_fold2, target_fold1, 'Fold 2', 'Fold 1', 2, 8, 0.001, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Acc rate for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, n_hiddens, layer_size, lr, n_ites):\n",
    "    model = DLModel(feature_input_size, n_hiddens, layer_size)\n",
    "    loss_func = SquaredHingeLoss(margin=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset    = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for _ in range(n_ites):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_stat(ldas1, ldas2, err_fold1_df, err_fold2_df, seqs, labels):\n",
    "    header = ['sequenceID', 'lda_fold1', 'lda_fold2', 'fold_1_total_labels', 'fold_2_total_labels', 'fold1_err', 'fold2_err']\n",
    "    rows = []\n",
    "    for i in range(len(seqs)):\n",
    "        # get total labels\n",
    "        _, neg_start_1, _, pos_start_1, _, neg_start_2, _, pos_start_2, _ = get_data(i, seqs, labels)\n",
    "        fold1_total_labels = len(neg_start_1) + len(pos_start_1)\n",
    "        fold2_total_labels = len(neg_start_2) + len(pos_start_2)\n",
    "\n",
    "        # round lambda\n",
    "        ldas1 = [round(num*2)/2 for num in ldas1]\n",
    "        ldas2 = [round(num*2)/2 for num in ldas2]\n",
    "\n",
    "        # get err\n",
    "        fold1_err = err_fold1_df.iloc[i][str(ldas1[i])]\n",
    "        fold2_err = err_fold2_df.iloc[i][str(ldas2[i])]\n",
    "        \n",
    "        # add row to rows\n",
    "        row = [seqs[i][0], ldas1[i], ldas2[i], fold1_total_labels, fold2_total_labels, fold1_err, fold2_err]\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(full_X, X1, X2, y1, y2, config, err_fold1_df, err_fold2_df, seqs, labels):\n",
    "    n_hiddens  = config['n_hiddens']\n",
    "    layer_size = config['layer_size']\n",
    "\n",
    "    # best_no_ite_1 = cv_learn(cv_n_folds, X1, y1, n_hiddens, layer_size, False, lr, cv_n_ites)\n",
    "    # best_no_ite_2 = cv_learn(cv_n_folds, X2, y2, n_hiddens, layer_size, False, lr, cv_n_ites)\n",
    "\n",
    "    # model1 = train_model(X1, y1, n_hiddens, layer_size, lr, best_no_ite_1)\n",
    "    # model2 = train_model(X2, y2, n_hiddens, layer_size, lr, best_no_ite_2)\n",
    "\n",
    "    model1 = train_model(X1, y1, n_hiddens, layer_size, lr, 500)\n",
    "    model2 = train_model(X2, y2, n_hiddens, layer_size, lr, 500)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ldas1 = model1(full_X).numpy().reshape(-1)\n",
    "        ldas2 = model2(full_X).numpy().reshape(-1)\n",
    "\n",
    "    df = get_df_stat(ldas1, ldas2, err_fold1_df, err_fold2_df, seqs, labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:  5.1min remaining: 20.3min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  6.7min finished\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dfs = Parallel(n_jobs=len(configs), verbose=1)(delayed(try_model)(features, features, features, target_fold1, target_fold2,\n",
    "                                                        configs[i], err_fold1_df, err_fold2_df, seqs, labels) for i in range(len(configs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_hiddens  layer_size  fold1_test  fold2_test\n",
      "0          0           0   78.723404   80.576923\n",
      "1          1           4   78.856383   81.153846\n",
      "2          1           8   78.457447   81.730769\n",
      "3          1          16   79.654255   82.692308\n",
      "4          2           4   79.521277   81.538462\n",
      "5          2           8   79.388298   81.923077\n",
      "6          2          16   79.122340   87.500000\n",
      "7          3           4   79.654255   80.961538\n",
      "8          3           8   79.654255   87.115385\n",
      "9          3          16   80.718085   83.269231\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(dfs)):\n",
    "    df = dfs[i]\n",
    "    \n",
    "    total_label_fold1 = df['fold_1_total_labels'].sum()\n",
    "    total_label_fold2 = df['fold_2_total_labels'].sum()\n",
    "    err1 = df['fold1_err'].sum()\n",
    "    err2 = df['fold2_err'].sum()\n",
    "    rate1 = (total_label_fold1 - err1)/total_label_fold1\n",
    "    rate2 = (total_label_fold2 - err2)/total_label_fold2\n",
    "\n",
    "    results.append({\n",
    "        'n_hiddens': configs[i]['n_hiddens'],\n",
    "        'layer_size': configs[i]['layer_size'],\n",
    "        'fold1_test': rate1 * 100,\n",
    "        'fold2_test': rate2 * 100\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fold1_test: 79.37\n",
      "Average fold2_test: 82.85\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average of fold1_test and fold2_test\n",
    "fold1_test_avg = df_results['fold1_test'].mean()\n",
    "fold2_test_avg = df_results['fold2_test'].mean()\n",
    "\n",
    "print(\"Average fold1_test: %.2f\" % fold1_test_avg)\n",
    "print(\"Average fold2_test: %.2f\" % fold2_test_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
